value,labels
Page 1:,Page 1:
A Comprehensive Overview of Large Language Models,A Comprehensive Overview of Large Language Models
"Humza Naveeda, Asad Ullah Khanb,∗, Shi Qiuc,∗, Muhammad Saqibd,e,∗, Saeed Anwarf,g, Muhammad Usmanf,g, Naveed Akhtarh,j,","Humza Naveeda, Asad Ullah Khanb,∗, Shi Qiuc,∗, Muhammad Saqibd,e,∗, Saeed Anwarf,g, Muhammad Usmanf,g, Naveed Akhtarh,j,"
"aThe University of Sydney, Sydney, Australia","aThe University of Sydney, Sydney, Australia"
"bUniversity of Engineering and Technology (UET), Lahore, Pakistan","bUniversity of Engineering and Technology (UET), Lahore, Pakistan"
"cThe Chinese University of Hong Kong (CUHK), HKSAR, China","cThe Chinese University of Hong Kong (CUHK), HKSAR, China"
"dUniversity of Technology Sydney (UTS), Sydney, Australia","dUniversity of Technology Sydney (UTS), Sydney, Australia"
"eCommonwealth Scientific and Industrial Research Organisation (CSIRO), Sydney, Australia","eCommonwealth Scientific and Industrial Research Organisation (CSIRO), Sydney, Australia"
"fKing Fahd University of Petroleum and Minerals (KFUPM), Dhahran, Saudi Arabia","fKing Fahd University of Petroleum and Minerals (KFUPM), Dhahran, Saudi Arabia"
"gSDAIA-KFUPM Joint Research Center for Artificial Intelligence (JRCAI), Dhahran, Saudi Arabia","gSDAIA-KFUPM Joint Research Center for Artificial Intelligence (JRCAI), Dhahran, Saudi Arabia"
"hThe University of Melbourne (UoM), Melbourne, Australia","hThe University of Melbourne (UoM), Melbourne, Australia"
"iAustralian National University (ANU), Canberra, Australia","iAustralian National University (ANU), Canberra, Australia"
"jThe University of Western Australia (UWA), Perth, Australia","jThe University of Western Australia (UWA), Perth, Australia"
Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and,Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and
beyond.,beyond.
This success of LLMs has led to a large influx of research contributions in this direction.,This success of LLMs has led to a large influx of research contributions in this direction.
These works encompass diverse,These works encompass diverse
"topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,","topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,"
"robotics, datasets, benchmarking, efficiency, and more.","robotics, datasets, benchmarking, efficiency, and more."
With the rapid development of techniques and regular breakthroughs in,With the rapid development of techniques and regular breakthroughs in
"LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction.","LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction."
Considering,Considering
"the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise","the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise"
yet comprehensive overview of the recent developments in this field.,yet comprehensive overview of the recent developments in this field.
This article provides an overview of the literature on a broad,This article provides an overview of the literature on a broad
range of LLM-related concepts.,range of LLM-related concepts.
Our self-contained comprehensive overview of LLMs discusses relevant background concepts,Our self-contained comprehensive overview of LLMs discusses relevant background concepts
along with covering the advanced topics at the frontier of research in LLMs.,along with covering the advanced topics at the frontier of research in LLMs.
This review article is intended to provide not only a,This review article is intended to provide not only a
"systematic survey but also a quick, comprehensive reference for the researchers and practitioners to draw insights from extensive,","systematic survey but also a quick, comprehensive reference for the researchers and practitioners to draw insights from extensive,"
informative summaries of the existing works to advance the LLM research.,informative summaries of the existing works to advance the LLM research.
"Large Language Models, LLMs, chatGPT, Augmented LLMs, Multimodal LLMs, LLM training, LLM Benchmarking","Large Language Models, LLMs, chatGPT, Augmented LLMs, Multimodal LLMs, LLM training, LLM Benchmarking"
Language plays a fundamental role in facilitating commu-,Language plays a fundamental role in facilitating commu-
nication and self-expression for humans and their interaction,nication and self-expression for humans and their interaction
with machines.,with machines.
The need for generalized models stems from,The need for generalized models stems from
the growing demand for machines to handle complex language,the growing demand for machines to handle complex language
"tasks, including translation, summarization, information re-","tasks, including translation, summarization, information re-"
"trieval, conversational interactions, etc.","trieval, conversational interactions, etc."
"Recently, significant","Recently, significant"
"breakthroughs have been witnessed in language models, pri-","breakthroughs have been witnessed in language models, pri-"
"marily attributed to transformers [1], increased computational","marily attributed to transformers [1], increased computational"
"capabilities, and the availability of large-scale training data.","capabilities, and the availability of large-scale training data."
These developments have brought about a revolutionary trans-,These developments have brought about a revolutionary trans-
formation by enabling the creation of LLMs that can approxi-,formation by enabling the creation of LLMs that can approxi-
"mate human-level performance on various tasks [2, 3].","mate human-level performance on various tasks [2, 3]."
Large,Large
"Email addresses: humza_naveed@yahoo.com (Humza Naveed),","Email addresses: humza_naveed@yahoo.com (Humza Naveed),"
"aukhanee@gmail.com (Asad Ullah Khan), shiqiu@cse.cuhk.edu.hk (Shi","aukhanee@gmail.com (Asad Ullah Khan), shiqiu@cse.cuhk.edu.hk (Shi"
Figure 1: The trend of papers released over the years containing keywords,Figure 1: The trend of papers released over the years containing keywords
"""Large Language Model"", ""Large Language Model + Fine-Tuning"", and ""Large","""Large Language Model"", ""Large Language Model + Fine-Tuning"", and ""Large"
arXiv:2307.06435v10  [cs.CL]  17 Oct 2024,arXiv:2307.06435v10  [cs.CL]  17 Oct 2024
Page 2:,Page 2:
"Figure 2: Chronological display of LLM releases: blue cards represent ‘pre-trained’ models, while orange cards correspond to ‘instruction-tuned’ models.","Figure 2: Chronological display of LLM releases: blue cards represent ‘pre-trained’ models, while orange cards correspond to ‘instruction-tuned’ models."
Models,Models
"on the upper half signify open-source availability, whereas those on the bottom are closed-source.","on the upper half signify open-source availability, whereas those on the bottom are closed-source."
The chart illustrates the increasing trend towards instruction-tuned,The chart illustrates the increasing trend towards instruction-tuned
"and open-source models, highlighting the evolving landscape and trends in natural language processing research.","and open-source models, highlighting the evolving landscape and trends in natural language processing research."
Language Models (LLMs) have emerged as cutting-edge arti-,Language Models (LLMs) have emerged as cutting-edge arti-
ficial intelligence systems that can process and generate text,ficial intelligence systems that can process and generate text
with coherent communication [4] and generalize to multiple,with coherent communication [4] and generalize to multiple
The historical progress in natural language processing (NLP),The historical progress in natural language processing (NLP)
evolved from statistical to neural language modeling and then,evolved from statistical to neural language modeling and then
from pre-trained language models (PLMs) to LLMs.,from pre-trained language models (PLMs) to LLMs.
conventional language modeling (LM) trains task-specific mod-,conventional language modeling (LM) trains task-specific mod-
"els in supervised settings, PLMs are trained in a self-supervised","els in supervised settings, PLMs are trained in a self-supervised"
"setting on a large corpus of text [7, 8, 9] with the aim of learning","setting on a large corpus of text [7, 8, 9] with the aim of learning"
a generic representation that is shareable among various NLP,a generic representation that is shareable among various NLP
tasks.,tasks.
"After fine-tuning for downstream tasks, PLMs surpass","After fine-tuning for downstream tasks, PLMs surpass"
the performance gains of traditional language modeling (LM).,the performance gains of traditional language modeling (LM).
"The larger PLMs bring more performance gains, which has led","The larger PLMs bring more performance gains, which has led"
to the transitioning of PLMs to LLMs by significantly increas-,to the transitioning of PLMs to LLMs by significantly increas-
ing model parameters (tens to hundreds of billions) [10] and,ing model parameters (tens to hundreds of billions) [10] and
"training dataset (many GBs and TBs) [10, 11].","training dataset (many GBs and TBs) [10, 11]."
Following this,Following this
"development, numerous LLMs have been proposed in the lit-","development, numerous LLMs have been proposed in the lit-"
"erature [10, 11, 12, 6, 13, 14, 15].","erature [10, 11, 12, 6, 13, 14, 15]."
An increasing trend in the,An increasing trend in the
number of released LLMs and names of a few significant LLMs,number of released LLMs and names of a few significant LLMs
"proposed over the years are shown in Fig 1 and Fig 2, respec-","proposed over the years are shown in Fig 1 and Fig 2, respec-"
"The early work on LLMs, such as T5 [10] and mT5 [11] em-","The early work on LLMs, such as T5 [10] and mT5 [11] em-"
ployed transfer learning until GPT-3 [6] showed LLMs are,ployed transfer learning until GPT-3 [6] showed LLMs are
zero-shot transferable to downstream tasks without fine-tuning.,zero-shot transferable to downstream tasks without fine-tuning.
LLMs accurately respond to task queries when prompted with,LLMs accurately respond to task queries when prompted with
task descriptions and examples.,task descriptions and examples.
"However, pre-trained LLMs","However, pre-trained LLMs"
fail to follow user intent and perform worse in zero-shot set-,fail to follow user intent and perform worse in zero-shot set-
Fine-tuning them with task instruc-,Fine-tuning them with task instruc-
"tions data [16, 17, 18, 19] and aligning with human prefer-","tions data [16, 17, 18, 19] and aligning with human prefer-"
"ences [20, 21] enhances generalization to unseen tasks, im-","ences [20, 21] enhances generalization to unseen tasks, im-"
proving zero-shot performance significantly and reducing mis-,proving zero-shot performance significantly and reducing mis-
"In addition to better generalization and domain adaptation,","In addition to better generalization and domain adaptation,"
"LLMs appear to have emergent abilities, such as reasoning,","LLMs appear to have emergent abilities, such as reasoning,"
"planning, decision-making, in-context learning, answering in","planning, decision-making, in-context learning, answering in"
These abilities are known to be ac-,These abilities are known to be ac-
quired by them due to their gigantic scale even when the pre-,quired by them due to their gigantic scale even when the pre-
trained LLMs are not trained specifically to possess these at-,trained LLMs are not trained specifically to possess these at-
"tributes [22, 23, 24].","tributes [22, 23, 24]."
Such abilities have led LLMs to be widely,Such abilities have led LLMs to be widely
"adopted in diverse settings, including multi-modal, robotics,","adopted in diverse settings, including multi-modal, robotics,"
"tool manipulation, question answering, autonomous agents, etc.","tool manipulation, question answering, autonomous agents, etc."
Various improvements have also been suggested in these areas,Various improvements have also been suggested in these areas
"either by task-specific training [25, 26, 27, 28, 29, 30, 31] or","either by task-specific training [25, 26, 27, 28, 29, 30, 31] or"
The LLMs abilities to solve diverse tasks with human-level,The LLMs abilities to solve diverse tasks with human-level
"performance come at the cost of slow training and inference,","performance come at the cost of slow training and inference,"
"extensive hardware requirements, and higher running costs.","extensive hardware requirements, and higher running costs."
Such requirements have limited their adoption and opened up,Such requirements have limited their adoption and opened up
"opportunities to devise better architectures [15, 33, 34, 35]","opportunities to devise better architectures [15, 33, 34, 35]"
"and training strategies [36, 37, 21, 38, 39, 40, 41].","and training strategies [36, 37, 21, 38, 39, 40, 41]."
"eter efficient tuning [38, 41, 40], pruning [42, 43], quantiza-","eter efficient tuning [38, 41, 40], pruning [42, 43], quantiza-"
"tion [44, 45], knowledge distillation, and context length inter-","tion [44, 45], knowledge distillation, and context length inter-"
"polation [46, 47, 48, 49] among others are some of the methods","polation [46, 47, 48, 49] among others are some of the methods"
widely studied for efficient LLM utilization.,widely studied for efficient LLM utilization.
"Due to the success of LLMs on a wide variety of tasks, the","Due to the success of LLMs on a wide variety of tasks, the"
research literature has recently experienced a large influx of,research literature has recently experienced a large influx of
"LLMs literature in surveys [50, 51, 52, 53], and topic-specific","LLMs literature in surveys [50, 51, 52, 53], and topic-specific"
"surveys in [54, 55, 56, 57, 58].","surveys in [54, 55, 56, 57, 58]."
"In contrast to these surveys, our","In contrast to these surveys, our"
contribution focuses on providing a comprehensive yet concise,contribution focuses on providing a comprehensive yet concise
overview of the general direction of LLM research.,overview of the general direction of LLM research.
This arti-,This arti-
cle summarizes architectural and training details of pre-trained,cle summarizes architectural and training details of pre-trained
LLMs and delves deeper into the details of concepts like fine-,LLMs and delves deeper into the details of concepts like fine-
"tuning, multi-modal LLMs, augmented LLMs, datasets, eval-","tuning, multi-modal LLMs, augmented LLMs, datasets, eval-"
"uation, applications, challenges, and others to provide a self-","uation, applications, challenges, and others to provide a self-"
contained comprehensive overview.,contained comprehensive overview.
Our key contributions are,Our key contributions are
"• We present a survey on the developments in LLM research,","• We present a survey on the developments in LLM research,"
"providing a concise, comprehensive overview of the direc-","providing a concise, comprehensive overview of the direc-"
• We present extensive summaries of pre-trained models that,• We present extensive summaries of pre-trained models that
include fine-grained details of architecture and training de-,include fine-grained details of architecture and training de-
• We summarize major findings of the popular contributions,• We summarize major findings of the popular contributions
and provide a detailed discussion on the key design and,and provide a detailed discussion on the key design and
development aspects of LLMs to help practitioners effec-,development aspects of LLMs to help practitioners effec-
"• In this self-contained article, we cover a range of con-","• In this self-contained article, we cover a range of con-"
cepts to present the general direction of LLMs compre-,cepts to present the general direction of LLMs compre-
"hensively, including background, pre-training, fine-tuning,","hensively, including background, pre-training, fine-tuning,"
Page 3:,Page 3:
"Figure 3: A broader overview of LLMs, dividing LLMs into seven branches: 1.","Figure 3: A broader overview of LLMs, dividing LLMs into seven branches: 1."
Pre-Training 2.,Pre-Training 2.
Fine-Tuning 3.,Fine-Tuning 3.
Efficient 4.,Efficient 4.
Inference 5.,Inference 5.
Evaluation 6.,Evaluation 6.
Applications,Applications
"multi-modal LLMs, augmented LLMs, LLMs-powered","multi-modal LLMs, augmented LLMs, LLMs-powered"
We loosely follow the existing terminology to ensure a stan-,We loosely follow the existing terminology to ensure a stan-
dardized outlook of this research direction.,dardized outlook of this research direction.
"For instance, fol-","For instance, fol-"
"lowing [50], our survey discusses pre-trained LLMs with 10B","lowing [50], our survey discusses pre-trained LLMs with 10B"
parameters or more.,parameters or more.
We refer the readers interested in smaller,We refer the readers interested in smaller
"pre-trained models to [51, 52, 53].","pre-trained models to [51, 52, 53]."
The organization of this paper is as follows.,The organization of this paper is as follows.
Section 2 discusses,Section 2 discusses
the background of LLMs.,the background of LLMs.
"Section 3 focuses on LLMs overview,","Section 3 focuses on LLMs overview,"
"architectures, training pipelines and strategies, fine-tuning, and","architectures, training pipelines and strategies, fine-tuning, and"
utilization in different domains.,utilization in different domains.
Section 4 highlights the config-,Section 4 highlights the config-
uration and parameters that play a crucial role in the function-,uration and parameters that play a crucial role in the function-
ing of these models.,ing of these models.
Summary and discussions are presented,Summary and discussions are presented
in section 3.8.,in section 3.8.
"The LLM training and evaluation, datasets, and","The LLM training and evaluation, datasets, and"
"benchmarks are discussed in section 5, followed by challenges","benchmarks are discussed in section 5, followed by challenges"
"and future directions, and conclusion in sections 7 and 8, re-","and future directions, and conclusion in sections 7 and 8, re-"
Page 4:,Page 4:
We provide the relevant background to understand the fun-,We provide the relevant background to understand the fun-
damentals related to LLMs in this section.,damentals related to LLMs in this section.
We briefly discuss,We briefly discuss
necessary components in LLMs and refer the readers interested,necessary components in LLMs and refer the readers interested
in details to the original works.,in details to the original works.
Tokenization [59] is an essential pre-processing step in,Tokenization [59] is an essential pre-processing step in
LLM training that parses the text into non-decomposing units,LLM training that parses the text into non-decomposing units
called tokens.,called tokens.
"Tokens can be characters, subwords [60], sym-","Tokens can be characters, subwords [60], sym-"
"bols [61], or words, depending on the tokenization process.","bols [61], or words, depending on the tokenization process."
Some of the commonly used tokenization schemes in LLMs,Some of the commonly used tokenization schemes in LLMs
"include wordpiece [62], byte pair encoding (BPE) [61], and un-","include wordpiece [62], byte pair encoding (BPE) [61], and un-"
igramLM [60].,igramLM [60].
Readers are encouraged to refer to [63] for a,Readers are encouraged to refer to [63] for a
The transformer processes input sequences in parallel and,The transformer processes input sequences in parallel and
ule in the transformer does not capture positional information.,ule in the transformer does not capture positional information.
"As a result, positional encodings were introduced in trans-","As a result, positional encodings were introduced in trans-"
"former [64], where a positional embedding vector is added to","former [64], where a positional embedding vector is added to"
the token embedding.,the token embedding.
Variants of positional embedding include,Variants of positional embedding include
"absolute, relative, or learned positional encodings.","absolute, relative, or learned positional encodings."
Within rel-,Within rel-
"ative encoding, Alibi and RoPE are two widely used positional","ative encoding, Alibi and RoPE are two widely used positional"
Alibi [65]: It subtracts a scalar bias from the attention score,Alibi [65]: It subtracts a scalar bias from the attention score
that increases with the distance between token positions.,that increases with the distance between token positions.
This,This
favors using recent tokens for attention.,favors using recent tokens for attention.
RoPE [66]: It rotates query and key representations at an an-,RoPE [66]: It rotates query and key representations at an an-
gle proportional to the token absolute position in the input,gle proportional to the token absolute position in the input
"sequence, resulting in a relative positional encoding scheme","sequence, resulting in a relative positional encoding scheme"
which decays with the distance between the tokens.,which decays with the distance between the tokens.
Attention assigns weights to input tokens based on impor-,Attention assigns weights to input tokens based on impor-
tance so that the model gives more emphasis to relevant tokens.,tance so that the model gives more emphasis to relevant tokens.
"Attention in transformers [64] calculates query, key, and value","Attention in transformers [64] calculates query, key, and value"
"mappings for input sequences, where the attention score is","mappings for input sequences, where the attention score is"
"obtained by multiplying the query and key, and later used to","obtained by multiplying the query and key, and later used to"
weight values.,weight values.
We discuss different attention strategies used in,We discuss different attention strategies used in
"Self-Attention [64]: Calculates attention using queries, keys,","Self-Attention [64]: Calculates attention using queries, keys,"
and values from the same block (encoder or decoder).,and values from the same block (encoder or decoder).
"Cross Attention: It is used in encoder-decoder architectures,","Cross Attention: It is used in encoder-decoder architectures,"
"where encoder outputs are the queries, and key-value pairs","where encoder outputs are the queries, and key-value pairs"
Sparse Attention [67]: Self-attention has O(n2) time complex-,Sparse Attention [67]: Self-attention has O(n2) time complex-
ity which becomes infeasible for large sequences.,ity which becomes infeasible for large sequences.
To speed,To speed
"up the computation, sparse attention [67] iteratively calculates","up the computation, sparse attention [67] iteratively calculates"
attention in sliding windows for speed gains.,attention in sliding windows for speed gains.
Flash Attention [68]: Memory access is the major bottleneck,Flash Attention [68]: Memory access is the major bottleneck
in calculating attention using GPUs.,in calculating attention using GPUs.
attention employs input tiling to minimize the memory reads,attention employs input tiling to minimize the memory reads
and writes between the GPU high bandwidth memory (HBM),and writes between the GPU high bandwidth memory (HBM)
The activation functions serve a crucial role in the curve-,The activation functions serve a crucial role in the curve-
fitting abilities of neural networks [69].,fitting abilities of neural networks [69].
We discuss activation,We discuss activation
functions used in LLMs in this section.,functions used in LLMs in this section.
ReLU [70]: The Rectified linear unit (ReLU) is defined as:,ReLU [70]: The Rectified linear unit (ReLU) is defined as:
GeLU [71]: The Gaussian Error Linear Unit (GeLU) is the,GeLU [71]: The Gaussian Error Linear Unit (GeLU) is the
"combination of ReLU, dropout [72] and zoneout [73].","combination of ReLU, dropout [72] and zoneout [73]."
GLU variants [74]: The Gated Linear Unit [75] is a neural,GLU variants [74]: The Gated Linear Unit [75] is a neural
network layer that is an element-wise product (⊗) of a linear,network layer that is an element-wise product (⊗) of a linear
transformation and a sigmoid transformed (σ) linear projection,transformation and a sigmoid transformed (σ) linear projection
of the input given as:,of the input given as:
"GLU(x, W, V, b, c) = (xW + b) ⊗σ(xV + c),","GLU(x, W, V, b, c) = (xW + b) ⊗σ(xV + c),"
"where X is the input of layer and l, W, b, V and c are learned","where X is the input of layer and l, W, b, V and c are learned"
parameters.,parameters.
Other GLU variants [74] used in LLMs are:,Other GLU variants [74] used in LLMs are:
"ReGLU(x, W, V, b, c) = max(0, xW + b)⊗,","ReGLU(x, W, V, b, c) = max(0, xW + b)⊗,"
"GEGLU(x, W, V, b, c) = GELU(xW + b) ⊗(xV + c),","GEGLU(x, W, V, b, c) = GELU(xW + b) ⊗(xV + c),"
"S wiGLU(x, W, V, b, c, β) = S wishβ(xW + b) ⊗(xV + c).","S wiGLU(x, W, V, b, c, β) = S wishβ(xW + b) ⊗(xV + c)."
Layer normalization leads to faster convergence and is an in-,Layer normalization leads to faster convergence and is an in-
tegrated component of transformers [64].,tegrated component of transformers [64].
In addition to Layer-,In addition to Layer-
"Norm [76] and RMSNorm [77], LLMs use pre-layer normal-","Norm [76] and RMSNorm [77], LLMs use pre-layer normal-"
"ization [78], applying it before multi-head attention (MHA).","ization [78], applying it before multi-head attention (MHA)."
Pre-norm is shown to provide training stability in LLMs.,Pre-norm is shown to provide training stability in LLMs.
An-,An-
"other normalization variant, DeepNorm [79] fixes the issue with","other normalization variant, DeepNorm [79] fixes the issue with"
This section describes distributed LLM training approaches,This section describes distributed LLM training approaches
briefly.,briefly.
"More details are available in [13, 37, 80, 81].","More details are available in [13, 37, 80, 81]."
Data Parallelism: Data parallelism replicates the model on,Data Parallelism: Data parallelism replicates the model on
multiple devices where data in a batch gets divided across de-,multiple devices where data in a batch gets divided across de-
vices.,vices.
At the end of each training iteration weights are synchro-,At the end of each training iteration weights are synchro-
Tensor Parallelism: Tensor parallelism shards a tensor compu-,Tensor Parallelism: Tensor parallelism shards a tensor compu-
tation across devices.,tation across devices.
It is also known as horizontal parallelism,It is also known as horizontal parallelism
Pipeline Parallelism: Pipeline parallelism shards model layers,Pipeline Parallelism: Pipeline parallelism shards model layers
across different devices.,across different devices.
This is also known as vertical paral-,This is also known as vertical paral-
Model Parallelism: A combination of tensor and pipeline par-,Model Parallelism: A combination of tensor and pipeline par-
allelism is known as model parallelism.,allelism is known as model parallelism.
"3D Parallelism: A combination of data, tensor, and model par-","3D Parallelism: A combination of data, tensor, and model par-"
allelism is known as 3D parallelism.,allelism is known as 3D parallelism.
Optimizer Parallelism: Optimizer parallelism also known as,Optimizer Parallelism: Optimizer parallelism also known as
zero redundancy optimizer [37] implements optimizer state,zero redundancy optimizer [37] implements optimizer state
"partitioning, gradient partitioning, and parameter partitioning","partitioning, gradient partitioning, and parameter partitioning"
across devices to reduce memory consumption while keeping,across devices to reduce memory consumption while keeping
the communication costs as low as possible.,the communication costs as low as possible.
Page 5:,Page 5:
Some commonly used libraries for LLMs training are:,Some commonly used libraries for LLMs training are:
Transformers [82]: The library provides access to various pre-,Transformers [82]: The library provides access to various pre-
"trained transformer models with APIs to train, fine-tune, infer,","trained transformer models with APIs to train, fine-tune, infer,"
DeepSpeed [36]: A library for scalable distributed training and,DeepSpeed [36]: A library for scalable distributed training and
inference of deep learning models.,inference of deep learning models.
Megatron-LM [80]: It provides GPU-optimized techniques for,Megatron-LM [80]: It provides GPU-optimized techniques for
JAX [83]: A Python library for high-performance numerical,JAX [83]: A Python library for high-performance numerical
computing and scaleable machine learning.,computing and scaleable machine learning.
It can differenti-,It can differenti-
ate native Python and NumPy functions and execute them on,ate native Python and NumPy functions and execute them on
Colossal-AI [84]: A collection of components to write dis-,Colossal-AI [84]: A collection of components to write dis-
BMTrain [81]: A library to write efficient stand-alone LLMs,BMTrain [81]: A library to write efficient stand-alone LLMs
Provides API to build mixture-of-experts,Provides API to build mixture-of-experts
MindSpore [86]: A deep learning training and inference frame-,MindSpore [86]: A deep learning training and inference frame-
"work extendable to mobile, edge, and cloud computing.","work extendable to mobile, edge, and cloud computing."
PyTorch [87]: A framework developed by Facebook AI Re-,PyTorch [87]: A framework developed by Facebook AI Re-
search lab (FAIR) to build deep learning models.,search lab (FAIR) to build deep learning models.
The main,The main
features of PyTorch include a dynamic computation graph and,features of PyTorch include a dynamic computation graph and
A deep learning framework written by,A deep learning framework written by
Google.,Google.
The key features of TensorFlow are graph-based com-,The key features of TensorFlow are graph-based com-
"putation, eager execution, scalability, etc.","putation, eager execution, scalability, etc."
MXNet [89]: Apache MXNet is a deep learning framework,MXNet [89]: Apache MXNet is a deep learning framework
"with support to write programs in multiple languages, includ-","with support to write programs in multiple languages, includ-"
"ing, Python, C++, Scala, R, etc.","ing, Python, C++, Scala, R, etc."
It also provides support for,It also provides support for
dynamic and static computation graphs.,dynamic and static computation graphs.
This section briefly summarizes data preprocessing tech-,This section briefly summarizes data preprocessing tech-
niques used in LLMs training.,niques used in LLMs training.
"Quality Filtering: For better results, training data quality is","Quality Filtering: For better results, training data quality is"
essential.,essential.
Some approaches to filtering data are: 1) classifier-,Some approaches to filtering data are: 1) classifier-
train a classifier on high-quality data and predict the quality of,train a classifier on high-quality data and predict the quality of
"text for filtering, whereas heuristics-based employ some rules","text for filtering, whereas heuristics-based employ some rules"
"for filtering like language, metrics, statistics, and keywords.","for filtering like language, metrics, statistics, and keywords."
Data Deduplication: Duplicated data can affect model per-,Data Deduplication: Duplicated data can affect model per-
"formance and increase data memorization; therefore, to train","formance and increase data memorization; therefore, to train"
"LLMs, data deduplication is one of the preprocessing steps.","LLMs, data deduplication is one of the preprocessing steps."
"This can be performed at multiple levels, like sentences,","This can be performed at multiple levels, like sentences,"
Privacy Reduction: Most of the training data for LLMs is,Privacy Reduction: Most of the training data for LLMs is
"information; therefore, many LLMs employ heuristics-based","information; therefore, many LLMs employ heuristics-based"
"methods to filter information such as names, addresses, and","methods to filter information such as names, addresses, and"
phone numbers to avoid learning personal information.,phone numbers to avoid learning personal information.
Here we discuss the variants of the transformer architectures,Here we discuss the variants of the transformer architectures
used in LLMs.,used in LLMs.
The difference arises due to the application of,The difference arises due to the application of
"Figure 4: An example of attention patterns in language models, image is taken","Figure 4: An example of attention patterns in language models, image is taken"
"Figure 5: An example of language model training objectives, image from [93].","Figure 5: An example of language model training objectives, image from [93]."
the attention and the connection of transformer blocks.,the attention and the connection of transformer blocks.
An il-,An il-
lustration of attention patterns of these architectures is shown,lustration of attention patterns of these architectures is shown
Encoder Decoder: This architecture processes inputs through,Encoder Decoder: This architecture processes inputs through
the encoder and passes the intermediate representation to the,the encoder and passes the intermediate representation to the
decoder to generate the output.,decoder to generate the output.
"Here, the encoder sees the","Here, the encoder sees the"
complete sequence utilizing self-attention whereas the decoder,complete sequence utilizing self-attention whereas the decoder
processes the sequence one after the other with implementing,processes the sequence one after the other with implementing
Causal Decoder: A type of architecture that does not have an,Causal Decoder: A type of architecture that does not have an
"encoder and processes and generates output using a decoder,","encoder and processes and generates output using a decoder,"
where the predicted token depends only on the previous time,where the predicted token depends only on the previous time
"Prefix Decoder: It is also known as a non-causal decoder,","Prefix Decoder: It is also known as a non-causal decoder,"
where the attention calculation is not strictly dependent on the,where the attention calculation is not strictly dependent on the
past information and the attention is bidirectional.,past information and the attention is bidirectional.
An example,An example
of a non-causal attention mask is shown in Figure 4.,of a non-causal attention mask is shown in Figure 4.
Mixture-of-Experts: It is a variant of transformer architecture,Mixture-of-Experts: It is a variant of transformer architecture
with parallel independent experts and a router to route tokens,with parallel independent experts and a router to route tokens
to experts.,to experts.
These experts are feed-forward layers after the at-,These experts are feed-forward layers after the at-
tention block [90].,tention block [90].
Mixture-of-Experts (MoE) is an efficient,Mixture-of-Experts (MoE) is an efficient
sparse architecture that offers comparable performance to dense,sparse architecture that offers comparable performance to dense
models and allows increasing the model size without increas-,models and allows increasing the model size without increas-
ing the computational cost by activating only a few experts at a,ing the computational cost by activating only a few experts at a
This section describes LLMs pre-training objectives.,This section describes LLMs pre-training objectives.
more details see the paper [93].,more details see the paper [93].
Full Language Modeling: An autoregressive language model-,Full Language Modeling: An autoregressive language model-
ing objective where the model is asked to predict future tokens,ing objective where the model is asked to predict future tokens
"given the previous tokens, an example is shown in Figure 5.","given the previous tokens, an example is shown in Figure 5."
"Prefix Language Modeling: A non-causal training objective,","Prefix Language Modeling: A non-causal training objective,"
where a prefix is chosen randomly and only remaining target,where a prefix is chosen randomly and only remaining target
tokens are used to calculate the loss.,tokens are used to calculate the loss.
An example is shown in,An example is shown in
Page 6:,Page 6:
Figure 6: A basic flow diagram depicting various stages of LLMs from pre-training to prompting/utilization.,Figure 6: A basic flow diagram depicting various stages of LLMs from pre-training to prompting/utilization.
Prompting LLMs to generate responses is possible at,Prompting LLMs to generate responses is possible at
"different training stages like pre-training, instruction-tuning, or alignment tuning.","different training stages like pre-training, instruction-tuning, or alignment tuning."
"“RL” stands for reinforcement learning, “RM” represents reward-modeling, and","“RL” stands for reinforcement learning, “RM” represents reward-modeling, and"
“RLHF” represents reinforcement learning with human feedback.,“RLHF” represents reinforcement learning with human feedback.
"Masked Language Modeling: In this training objective, tokens","Masked Language Modeling: In this training objective, tokens"
or spans (a sequence of tokens) are masked randomly and the,or spans (a sequence of tokens) are masked randomly and the
model is asked to predict masked tokens given the past and,model is asked to predict masked tokens given the past and
future context.,future context.
An example is shown in Figure 5.,An example is shown in Figure 5.
Unified Language Modeling: Unified language modeling [94],Unified Language Modeling: Unified language modeling [94]
"is a combination of causal, non-causal, and masked language","is a combination of causal, non-causal, and masked language"
training objectives.,training objectives.
"Here in masked language modeling, the","Here in masked language modeling, the"
"attention is not bidirectional but unidirectional, attending either","attention is not bidirectional but unidirectional, attending either"
Scaling laws study the optimal combination of model param-,Scaling laws study the optimal combination of model param-
"eters, dataset size, and computational resources that predict the","eters, dataset size, and computational resources that predict the"
improvement in the model performance.,improvement in the model performance.
It has been shown,It has been shown
"that the loss scales according to the power-law with model size,","that the loss scales according to the power-law with model size,"
"dataset size, and compute resources [95].","dataset size, and compute resources [95]."
This study suggests,This study suggests
larger models are more important than big data for better perfor-,larger models are more important than big data for better perfor-
mance.,mance.
Another variant of scaling law [96] suggests the model,Another variant of scaling law [96] suggests the model
size and the number of training tokens should be scaled equally.,size and the number of training tokens should be scaled equally.
This section discusses the fundamentals of LLMs adaptation,This section discusses the fundamentals of LLMs adaptation
"stages, from pre-training to fine-tuning for downstream tasks","stages, from pre-training to fine-tuning for downstream tasks"
and utilization.,and utilization.
An example of different training stages and in-,An example of different training stages and in-
ference in LLMs is shown in Figure 6.,ference in LLMs is shown in Figure 6.
"In this paper, we refer","In this paper, we refer"
"to alignment-tuning as aligning with human preferences, while","to alignment-tuning as aligning with human preferences, while"
occasionally the literature uses the term alignment for different,occasionally the literature uses the term alignment for different
"In the very first stage, the model is trained in a self-","In the very first stage, the model is trained in a self-"
supervised manner on a large corpus to predict the next to-,supervised manner on a large corpus to predict the next to-
kens given the input.,kens given the input.
The design choices of LLMs vary from,The design choices of LLMs vary from
encoder-decoder to decoder-only architectures with different,encoder-decoder to decoder-only architectures with different
"building blocks and loss functions in sections 2.5, 2.4, 2.10.","building blocks and loss functions in sections 2.5, 2.4, 2.10."
There are different styles to fine-tune an LLM.,There are different styles to fine-tune an LLM.
This section,This section
Transfer Learning: The pre-trained LLMs perform well for,Transfer Learning: The pre-trained LLMs perform well for
"various tasks [6, 15].","various tasks [6, 15]."
"However, to improve the performance for","However, to improve the performance for"
Page 7:,Page 7:
"a downstream task, pre-trained models are fine-tuned with the","a downstream task, pre-trained models are fine-tuned with the"
"task-specific data [10, 11], known as transfer learning.","task-specific data [10, 11], known as transfer learning."
Instruction-tuning: To enable a model to respond to user,Instruction-tuning: To enable a model to respond to user
"queries effectively, the pre-trained model is fine-tuned on in-","queries effectively, the pre-trained model is fine-tuned on in-"
"struction formatted data i.e., instruction and an input-output","struction formatted data i.e., instruction and an input-output"
pair.,pair.
Instructions generally comprise multi-task data in plain,Instructions generally comprise multi-task data in plain
"natural language, guiding the model to respond according to the","natural language, guiding the model to respond according to the"
prompt and the input.,prompt and the input.
This type of fine-tuning improves zero-,This type of fine-tuning improves zero-
shot generalization and downstream task performance.,shot generalization and downstream task performance.
Details,Details
on formatting instruction data and its various styles are avail-,on formatting instruction data and its various styles are avail-
"able in [16, 50, 97].","able in [16, 50, 97]."
"Alignment-tuning: LLMs are prone to generating false, biased,","Alignment-tuning: LLMs are prone to generating false, biased,"
and harmful text.,and harmful text.
"To make them helpful, honest, and harmless,","To make them helpful, honest, and harmless,"
models are aligned using human feedback.,models are aligned using human feedback.
Alignment involves,Alignment involves
asking LLMs to generate unexpected responses and then updat-,asking LLMs to generate unexpected responses and then updat-
"ing their parameters to avoid such responses [20, 21, 98].","ing their parameters to avoid such responses [20, 21, 98]."
It ensures LLMs operate according to human intentions and,It ensures LLMs operate according to human intentions and
values.,values.
A model is defined to be an “aligned” model if the,A model is defined to be an “aligned” model if the
"model fulfills three criteria of helpful, honest, and harmless or","model fulfills three criteria of helpful, honest, and harmless or"
Researchers employ reinforcement learning with human feed-,Researchers employ reinforcement learning with human feed-
back (RLHF) [100] for model alignment.,back (RLHF) [100] for model alignment.
"In RLHF, a fine-tuned","In RLHF, a fine-tuned"
model on demonstrations is further trained with reward model-,model on demonstrations is further trained with reward model-
"ing (RM) and reinforcement learning (RL), shown in Figure 6.","ing (RM) and reinforcement learning (RL), shown in Figure 6."
Below we briefly discuss RM and RL pipelines in RLHF.,Below we briefly discuss RM and RL pipelines in RLHF.
Reward modeling: trains a model to rank generated responses,Reward modeling: trains a model to rank generated responses
according to human preferences using a classification objec-,according to human preferences using a classification objec-
tive.,tive.
To train the classifier humans annotate LLMs generated,To train the classifier humans annotate LLMs generated
responses based on the HHH criteria.,responses based on the HHH criteria.
Reinforcement learning: in combination with the reward model,Reinforcement learning: in combination with the reward model
is used for alignment in the next stage.,is used for alignment in the next stage.
The previously trained,The previously trained
reward model ranks LLM-generated responses into preferred,reward model ranks LLM-generated responses into preferred
"vs. non-preferred, which is used to align the model with proxi-","vs. non-preferred, which is used to align the model with proxi-"
mal policy optimization (PPO).,mal policy optimization (PPO).
This process repeats iteratively,This process repeats iteratively
Prompting is a method to query trained LLMs for generating,Prompting is a method to query trained LLMs for generating
"responses, as illustrated in Figure 6.","responses, as illustrated in Figure 6."
LLMs can be prompted in,LLMs can be prompted in
"various prompt setups, where they can be adapted to the instruc-","various prompt setups, where they can be adapted to the instruc-"
tions without fine-tuning and in other cases with fine-tuning on,tions without fine-tuning and in other cases with fine-tuning on
"data containing different prompt styles [16, 101, 102].","data containing different prompt styles [16, 101, 102]."
A good,A good
guide on prompt engineering is available at [32].,guide on prompt engineering is available at [32].
"Below, we","Below, we"
will discuss various widely used prompt setups.,will discuss various widely used prompt setups.
Zero-Shot Prompting: LLMs are zero-shot learners and ca-,Zero-Shot Prompting: LLMs are zero-shot learners and ca-
pable of answering queries never seen before.,pable of answering queries never seen before.
This style of,This style of
prompting requires LLMs to answer user questions without see-,prompting requires LLMs to answer user questions without see-
ing any examples in the prompt.,ing any examples in the prompt.
"In-context Learning: Also known as few-shot learning, here,","In-context Learning: Also known as few-shot learning, here,"
multiple input-output demonstration pairs are shown to the,multiple input-output demonstration pairs are shown to the
model to generate the desired response.,model to generate the desired response.
This adaptation style,This adaptation style
is also called few-shot learning.,is also called few-shot learning.
A discussion on formatting in-,A discussion on formatting in-
"context learning (ICL) templates is available in [54, 50, 18, 16].","context learning (ICL) templates is available in [54, 50, 18, 16]."
Reasoning in LLMs: LLMs are zero-shot reasoners and can,Reasoning in LLMs: LLMs are zero-shot reasoners and can
"be provoked to generate answers to logical problems, task","be provoked to generate answers to logical problems, task"
"planning, critical thinking, etc.","planning, critical thinking, etc."
with reasoning.,with reasoning.
"reasons is possible only by using different prompting styles,","reasons is possible only by using different prompting styles,"
whereas to improve LLMs further on reasoning tasks many,whereas to improve LLMs further on reasoning tasks many
"methods [16, 97] train them on reasoning datasets.","methods [16, 97] train them on reasoning datasets."
We discuss,We discuss
various prompting techniques for reasoning below.,various prompting techniques for reasoning below.
Chain-of-Thought (CoT): A special case of prompting where,Chain-of-Thought (CoT): A special case of prompting where
demonstrations contain reasoning information aggregated with,demonstrations contain reasoning information aggregated with
inputs and outputs so that the model generates outcomes with,inputs and outputs so that the model generates outcomes with
step-by-step reasoning.,step-by-step reasoning.
More details on CoT prompts are avail-,More details on CoT prompts are avail-
"able in [55, 103, 101].","able in [55, 103, 101]."
Improves CoT performance by generat-,Improves CoT performance by generat-
ing multiple responses and selecting the most frequent an-,ing multiple responses and selecting the most frequent an-
Tree-of-Thought (ToT): Explores multiple reasoning paths,Tree-of-Thought (ToT): Explores multiple reasoning paths
with possibilities to look ahead and backtrack for problem-,with possibilities to look ahead and backtrack for problem-
"Single-Turn Instructions: In this prompting setup, LLMs are","Single-Turn Instructions: In this prompting setup, LLMs are"
queried only once with all the relevant information in the,queried only once with all the relevant information in the
prompt.,prompt.
LLMs generate responses by understanding the con-,LLMs generate responses by understanding the con-
text either in a zero-shot or few-shot setting.,text either in a zero-shot or few-shot setting.
Multi-Turn Instructions: Solving a complex task requires mul-,Multi-Turn Instructions: Solving a complex task requires mul-
"tiple interactions with LLMs, where feedback and responses","tiple interactions with LLMs, where feedback and responses"
from the other tools are given as input to the LLM for the next,from the other tools are given as input to the LLM for the next
rounds.,rounds.
This style of using LLMs in the loop is common in,This style of using LLMs in the loop is common in
"This section reviews LLMs, briefly describing their architec-","This section reviews LLMs, briefly describing their architec-"
"tures, training objectives, pipelines, datasets, and fine-tuning","tures, training objectives, pipelines, datasets, and fine-tuning"
"Here, we provide summaries of various well-known pre-","Here, we provide summaries of various well-known pre-"
"trained LLMs with significant discoveries, changing the course","trained LLMs with significant discoveries, changing the course"
of research and development in NLP.,of research and development in NLP.
These LLMs have consid-,These LLMs have consid-
"erably improved the performance in NLU and NLG domains,","erably improved the performance in NLU and NLG domains,"
and are widely fine-tuned for downstream tasks.,and are widely fine-tuned for downstream tasks.
"Moreover, We","Moreover, We"
also identify key findings and insights of pre-trained LLMs in,also identify key findings and insights of pre-trained LLMs in
Table 1 and 2 that improve their performance.,Table 1 and 2 that improve their performance.
T5 [10]: An encoder-decoder model employing a unified text-,T5 [10]: An encoder-decoder model employing a unified text-
to-text training for all NLP problems is shown in Figure 7.,to-text training for all NLP problems is shown in Figure 7.
T5,T5
places layer normalization outside the residual path in a conven-,places layer normalization outside the residual path in a conven-
tional transformer model [64].,tional transformer model [64].
It uses masked language mod-,It uses masked language mod-
eling as a pre-training objective where spans (consecutive to-,eling as a pre-training objective where spans (consecutive to-
kens) are replaced with a single mask instead of separate masks,kens) are replaced with a single mask instead of separate masks
for each token.,for each token.
This type of masking speeds up the training as,This type of masking speeds up the training as
it produces shorter sequences.,it produces shorter sequences.
"After pre-training, the model is","After pre-training, the model is"
fine-tuned using adapter layers [106] for downstream tasks.,fine-tuned using adapter layers [106] for downstream tasks.
GPT-3 [6]: The GPT-3 architecture is the same as the GPT-,GPT-3 [6]: The GPT-3 architecture is the same as the GPT-
2 [5] but with dense and sparse attention in transformer layers,2 [5] but with dense and sparse attention in transformer layers
similar to the Sparse Transformer [67].,similar to the Sparse Transformer [67].
It shows that large mod-,It shows that large mod-
els can train on larger batch sizes with a lower learning rate to,els can train on larger batch sizes with a lower learning rate to
"decide the batch size during training, GPT-3 uses the gradient","decide the batch size during training, GPT-3 uses the gradient"
noise scale as in [107].,noise scale as in [107].
"Overall, GPT-3 increases model param-","Overall, GPT-3 increases model param-"
eters to 175B showing that the performance of large language,eters to 175B showing that the performance of large language
Page 8:,Page 8:
"Figure 7: Unified text-to-text training example, source image from [10].","Figure 7: Unified text-to-text training example, source image from [10]."
"Figure 8: The image is the article of [108], showing an example of PanGu-α","Figure 8: The image is the article of [108], showing an example of PanGu-α"
models improves with the scale and is competitive with the fine-,models improves with the scale and is competitive with the fine-
mT5 [11]: A multilingual T5 model [10] trained on the mC4,mT5 [11]: A multilingual T5 model [10] trained on the mC4
dataset with 101 languages.,dataset with 101 languages.
The dataset is extracted from the,The dataset is extracted from the
public common crawl scrape.,public common crawl scrape.
The model uses a larger vocab-,The model uses a larger vocab-
"ulary size of 250,000 to cover multiple languages.","ulary size of 250,000 to cover multiple languages."
To avoid,To avoid
"over-fitting or under-fitting for a language, mT5 employs a data","over-fitting or under-fitting for a language, mT5 employs a data"
sampling procedure to select samples from all languages.,sampling procedure to select samples from all languages.
The,The
"paper suggests using a small amount of pre-training datasets,","paper suggests using a small amount of pre-training datasets,"
including all languages when fine-tuning for a task using En-,including all languages when fine-tuning for a task using En-
glish language data.,glish language data.
This allows the model to generate correct,This allows the model to generate correct
PanGu-α [108]: An autoregressive model that has a query,PanGu-α [108]: An autoregressive model that has a query
"layer at the end of standard transformer layers, example shown","layer at the end of standard transformer layers, example shown"
"in Figure 8, to predict the next token.","in Figure 8, to predict the next token."
Its structure is similar to,Its structure is similar to
the transformer layer but with an additional embedding for the,the transformer layer but with an additional embedding for the
"next position in the attention mechanism, given in Eq.","next position in the attention mechanism, given in Eq."
3.,3.
CPM-2 [12]: Cost-efficient Pre-trained language Models,CPM-2 [12]: Cost-efficient Pre-trained language Models
(CPM-2) pre-trains bilingual (English and Chinese) 11B and,(CPM-2) pre-trains bilingual (English and Chinese) 11B and
198B mixture-of-experts (MoE) models on the WuDaoCor-,198B mixture-of-experts (MoE) models on the WuDaoCor-
pus [109] dataset.,pus [109] dataset.
The tokenization process removes “_” white,The tokenization process removes “_” white
space tokens in the sentencepiece tokenizer.,space tokens in the sentencepiece tokenizer.
The models are,The models are
"trained with knowledge inheritance, starting with only the Chi-","trained with knowledge inheritance, starting with only the Chi-"
nese language in the first stage and then adding English and,nese language in the first stage and then adding English and
Chinese data.,Chinese data.
This trained model gets duplicated multiple times,This trained model gets duplicated multiple times
to initialize the 198B MoE model.,to initialize the 198B MoE model.
"Moreover, to use the model","Moreover, to use the model"
"for downstream tasks, CPM-2 experimented with both com-","for downstream tasks, CPM-2 experimented with both com-"
plete fine-tuning and prompt fine-tuning as in [40] where only,plete fine-tuning and prompt fine-tuning as in [40] where only
prompt-related parameters are updated by inserting prompts at,prompt-related parameters are updated by inserting prompts at
"various positions, front, middle, and back.","various positions, front, middle, and back."
CPM-2 also pro-,CPM-2 also pro-
"poses the INFMOE, a memory-efficient framework with a strat-","poses the INFMOE, a memory-efficient framework with a strat-"
egy to dynamically offload parameters to the CPU for inference,egy to dynamically offload parameters to the CPU for inference
at a 100B scale.,at a 100B scale.
It overlaps data movement with inference com-,It overlaps data movement with inference com-
putation for lower inference time.,putation for lower inference time.
ERNIE 3.0 [110]: ERNIE 3.0 takes inspiration from multi-,ERNIE 3.0 [110]: ERNIE 3.0 takes inspiration from multi-
task learning to build a modular architecture using Transformer-,task learning to build a modular architecture using Transformer-
XL [111] as the backbone.,XL [111] as the backbone.
The universal representation mod-,The universal representation mod-
"ule is shared by all the tasks, which serve as the basic block","ule is shared by all the tasks, which serve as the basic block"
"for task-specific representation modules, which are all trained","for task-specific representation modules, which are all trained"
"jointly for natural language understanding, natural language","jointly for natural language understanding, natural language"
"generation, and knowledge extraction.","generation, and knowledge extraction."
This LLM is primar-,This LLM is primar-
ily focused on the Chinese language.,ily focused on the Chinese language.
It claims to train on the,It claims to train on the
"largest Chinese text corpora for LLM training, and achieved","largest Chinese text corpora for LLM training, and achieved"
state-of-the-art in 54 Chinese NLP tasks.,state-of-the-art in 54 Chinese NLP tasks.
Jurassic-1 [112]: A pair of auto-regressive language mod-,Jurassic-1 [112]: A pair of auto-regressive language mod-
"els, including a 7B-parameter J1-Large model and a 178B-","els, including a 7B-parameter J1-Large model and a 178B-"
"Jurassic-1 comprise word pieces, complete words, and multi-","Jurassic-1 comprise word pieces, complete words, and multi-"
"word expressions without any word boundaries, where possible","word expressions without any word boundaries, where possible"
out-of-vocabulary instances are interpreted as Unicode bytes.,out-of-vocabulary instances are interpreted as Unicode bytes.
"Compared to the GPT-3 counterparts, the Jurassic-1 models","Compared to the GPT-3 counterparts, the Jurassic-1 models"
apply a more balanced depth-to-width self-attention architec-,apply a more balanced depth-to-width self-attention architec-
ture [113] and an improved tokenizer for a faster prediction,ture [113] and an improved tokenizer for a faster prediction
"based on broader resources, achieving a comparable perfor-","based on broader resources, achieving a comparable perfor-"
mance in zero-shot learning tasks and a superior performance in,mance in zero-shot learning tasks and a superior performance in
few-shot learning tasks given the ability to feed more examples,few-shot learning tasks given the ability to feed more examples
HyperCLOVA [114]: A Korean language model with GPT-3,HyperCLOVA [114]: A Korean language model with GPT-3
Yuan 1.0 [115]: Trained on a Chinese corpus with 5TB of,Yuan 1.0 [115]: Trained on a Chinese corpus with 5TB of
high-quality text collected from the Internet.,high-quality text collected from the Internet.
A Massive Data,A Massive Data
Filtering System (MDFS) built on Spark is developed to pro-,Filtering System (MDFS) built on Spark is developed to pro-
cess the raw data via coarse and fine filtering techniques.,cess the raw data via coarse and fine filtering techniques.
To,To
speed up the training of Yuan 1.0 to save energy expenses and,speed up the training of Yuan 1.0 to save energy expenses and
"carbon emissions, various factors that improve the performance","carbon emissions, various factors that improve the performance"
of distributed training are incorporated in architecture and train-,of distributed training are incorporated in architecture and train-
ing: like increasing the hidden state size improves pipeline and,ing: like increasing the hidden state size improves pipeline and
"tensor parallelism performance, larger micro batches improve","tensor parallelism performance, larger micro batches improve"
"pipeline parallelism performance, and larger global batch size","pipeline parallelism performance, and larger global batch size"
improve data parallelism performance.,improve data parallelism performance.
"In practice, the Yuan 1.0","In practice, the Yuan 1.0"
"model performs well on text classification, Winograd Schema,","model performs well on text classification, Winograd Schema,"
"natural language inference, and reading comprehension tasks.","natural language inference, and reading comprehension tasks."
Gopher [116]: The Gopher family of models ranges from,Gopher [116]: The Gopher family of models ranges from
44M to 280B parameters in size to study the effect of scale,44M to 280B parameters in size to study the effect of scale
on the LLMs performance.,on the LLMs performance.
"The 280B model beats GPT-3 [6],","The 280B model beats GPT-3 [6],"
"Jurrasic-1 [112], MT-NLG [117], and others on 81% of the","Jurrasic-1 [112], MT-NLG [117], and others on 81% of the"
ERNIE 3.0 TITAN [35]: ERNIE 3.0 Titan extends ERNIE 3.0,ERNIE 3.0 TITAN [35]: ERNIE 3.0 Titan extends ERNIE 3.0
by training a larger model with 26x the number of parameters,by training a larger model with 26x the number of parameters
of the latter.,of the latter.
This bigger model outperformed other state-of-the-,This bigger model outperformed other state-of-the-
art models in 68 NLP tasks.,art models in 68 NLP tasks.
LLMs produce text with incorrect,LLMs produce text with incorrect
facts.,facts.
In order to have control of the generated text with fac-,In order to have control of the generated text with fac-
"tual consistency, ERNIE 3.0 Titan adds another task, Credible","tual consistency, ERNIE 3.0 Titan adds another task, Credible"
"and Controllable Generations, to its multi-task learning setup.","and Controllable Generations, to its multi-task learning setup."
Page 9:,Page 9:
It introduces additional self-supervised adversarial and control-,It introduces additional self-supervised adversarial and control-
"lable language modeling losses to the pre-training step, which","lable language modeling losses to the pre-training step, which"
enables ERNIE 3.0 Titan to beat other LLMs in their manually,enables ERNIE 3.0 Titan to beat other LLMs in their manually
selected Factual QA task set evaluations.,selected Factual QA task set evaluations.
GPT-NeoX-20B [118]: An auto-regressive model that largely,GPT-NeoX-20B [118]: An auto-regressive model that largely
"follows GPT-3 with a few deviations in architecture design,","follows GPT-3 with a few deviations in architecture design,"
trained on the Pile dataset without any data deduplication.,trained on the Pile dataset without any data deduplication.
GPT-,GPT-
NeoX has parallel attention and feed-forward layers in a trans-,NeoX has parallel attention and feed-forward layers in a trans-
"former block, given in Eq.","former block, given in Eq."
"4, that increases throughput by 15%.","4, that increases throughput by 15%."
"It uses rotary positional embedding [66], applying it to only","It uses rotary positional embedding [66], applying it to only"
25% of embedding vector dimension as in [119].,25% of embedding vector dimension as in [119].
This reduces,This reduces
the computation without performance degradation.,the computation without performance degradation.
As opposed,As opposed
"to GPT-3, which uses dense and sparse layers, GPT-NeoX-20B","to GPT-3, which uses dense and sparse layers, GPT-NeoX-20B"
uses only dense layers.,uses only dense layers.
The hyperparameter tuning at this scale,The hyperparameter tuning at this scale
"is difficult; therefore, the model chooses hyperparameters from","is difficult; therefore, the model chooses hyperparameters from"
the method [6] and interpolates values between 13B and 175B,the method [6] and interpolates values between 13B and 175B
models for the 20B model.,models for the 20B model.
The model training is distributed,The model training is distributed
among GPUs using both tensor and pipeline parallelism.,among GPUs using both tensor and pipeline parallelism.
x + Attn(LN1(x)) + FF(LN2(x)),x + Attn(LN1(x)) + FF(LN2(x))
"OPT [14]: It is a clone of GPT-3, developed to open-source","OPT [14]: It is a clone of GPT-3, developed to open-source"
a model that replicates GPT-3 performance.,a model that replicates GPT-3 performance.
Training of OPT,Training of OPT
employs dynamic loss scaling [120] and restarts from an earlier,employs dynamic loss scaling [120] and restarts from an earlier
checkpoint with a lower learning rate whenever loss divergence,checkpoint with a lower learning rate whenever loss divergence
is observed.,is observed.
"Overall, the performance of OPT-175B models is","Overall, the performance of OPT-175B models is"
comparable to the GPT3-175B model.,comparable to the GPT3-175B model.
BLOOM [13]: A causal decoder model trained on the ROOTS,BLOOM [13]: A causal decoder model trained on the ROOTS
corpus to open-source an LLM.,corpus to open-source an LLM.
The architecture of BLOOM is,The architecture of BLOOM is
"shown in Figure 9, with differences like ALiBi positional em-","shown in Figure 9, with differences like ALiBi positional em-"
"bedding, an additional normalization layer after the embedding","bedding, an additional normalization layer after the embedding"
layer as suggested by the bitsandbytes1 library.,layer as suggested by the bitsandbytes1 library.
These changes,These changes
stabilize training with improved downstream performance.,stabilize training with improved downstream performance.
GLaM [91]: Generalist Language Model (GLaM) represents a,GLaM [91]: Generalist Language Model (GLaM) represents a
family of language models using a sparsely activated decoder-,family of language models using a sparsely activated decoder-
"only mixture-of-experts (MoE) structure [121, 90].","only mixture-of-experts (MoE) structure [121, 90]."
"more model capacity while reducing computation, the experts","more model capacity while reducing computation, the experts"
are sparsely activated where only the best two experts are used,are sparsely activated where only the best two experts are used
to process each input token.,to process each input token.
"The largest GLaM model, GLaM","The largest GLaM model, GLaM"
"(64B/64E), is about 7× larger than GPT-3 [6], while only part of","(64B/64E), is about 7× larger than GPT-3 [6], while only part of"
the parameters are activated per input token.,the parameters are activated per input token.
The largest GLaM,The largest GLaM
(64B/64E) model achieves better overall results as compared,(64B/64E) model achieves better overall results as compared
to GPT-3 while consuming only one-third of GPT-3’s training,to GPT-3 while consuming only one-third of GPT-3’s training
MT-NLG [117]: A 530B causal decoder based on the GPT-,MT-NLG [117]: A 530B causal decoder based on the GPT-
2 architecture that has roughly 3× GPT-3 model parameters.,2 architecture that has roughly 3× GPT-3 model parameters.
MT-NLG is trained on filtered high-quality data collected from,MT-NLG is trained on filtered high-quality data collected from
various public datasets and blends various types of datasets in a,various public datasets and blends various types of datasets in a
"single batch, which beats GPT-3 on several evaluations.","single batch, which beats GPT-3 on several evaluations."
Chinchilla [96]: A causal decoder trained on the same dataset,Chinchilla [96]: A causal decoder trained on the same dataset
as the Gopher [116] but with a little different data sampling,as the Gopher [116] but with a little different data sampling
distribution (sampled from MassiveText).,distribution (sampled from MassiveText).
The model architec-,The model architec-
"ture is similar to the one used for Gopher, with the exception of","ture is similar to the one used for Gopher, with the exception of"
AdamW optimizer instead of Adam.,AdamW optimizer instead of Adam.
Chinchilla identifies the,Chinchilla identifies the
Figure 9: The BLOOM architecture example sourced from [13].,Figure 9: The BLOOM architecture example sourced from [13].
relationship that model size should be doubled for every dou-,relationship that model size should be doubled for every dou-
bling of training tokens.,bling of training tokens.
Over 400 language models ranging,Over 400 language models ranging
from 70 million to over 16 billion parameters on 5 to 500 bil-,from 70 million to over 16 billion parameters on 5 to 500 bil-
lion tokens are trained to get the estimates for compute-optimal,lion tokens are trained to get the estimates for compute-optimal
training under a given budget.,training under a given budget.
The authors train a 70B model,The authors train a 70B model
with the same compute budget as Gopher (280B) but with 4,with the same compute budget as Gopher (280B) but with 4
times more data.,times more data.
"It outperforms Gopher [116], GPT-3 [6], and","It outperforms Gopher [116], GPT-3 [6], and"
"others on various downstream tasks, after fine-tuning.","others on various downstream tasks, after fine-tuning."
"AlexaTM [122]: An encoder-decoder model, where encoder","AlexaTM [122]: An encoder-decoder model, where encoder"
weights and decoder embeddings are initialized with a pre-,weights and decoder embeddings are initialized with a pre-
trained encoder to speed up training.,trained encoder to speed up training.
The encoder stays frozen,The encoder stays frozen
for the initial 100k steps and is later unfrozen for end-to-end,for the initial 100k steps and is later unfrozen for end-to-end
training.,training.
The model is trained on a combination of denoising,The model is trained on a combination of denoising
"and causal language modeling (CLM) objectives, concatenat-","and causal language modeling (CLM) objectives, concatenat-"
ing a [CLM] token at the beginning for mode switching.,ing a [CLM] token at the beginning for mode switching.
Dur-,Dur-
"ing training, the CLM task is applied for 20% of the time, which","ing training, the CLM task is applied for 20% of the time, which"
improves the in-context learning performance.,improves the in-context learning performance.
PaLM [15]: A causal decoder with parallel attention and,PaLM [15]: A causal decoder with parallel attention and
feed-forward layers similar to Eq.,feed-forward layers similar to Eq.
"4, speeding up training by","4, speeding up training by"
a factor of 15.,a factor of 15.
Additional changes to the conventional trans-,Additional changes to the conventional trans-
"former model include SwiGLU activation, RoPE embeddings,","former model include SwiGLU activation, RoPE embeddings,"
multi-query attention that saves computation cost during decod-,multi-query attention that saves computation cost during decod-
"ing, and shared input-output embeddings.","ing, and shared input-output embeddings."
"During training, loss","During training, loss"
"spiking was observed, and to fix it, model training was restarted","spiking was observed, and to fix it, model training was restarted"
from a 100-step earlier checkpoint by skipping 200-500 batches,from a 100-step earlier checkpoint by skipping 200-500 batches
around the spike.,around the spike.
"Moreover, the model was found to memo-","Moreover, the model was found to memo-"
"rize around 2.4% of the training data at the 540B model scale,","rize around 2.4% of the training data at the 540B model scale,"
whereas this number was lower for smaller models.,whereas this number was lower for smaller models.
"PaLM-2 [123]: A smaller multi-lingual variant of PaLM,","PaLM-2 [123]: A smaller multi-lingual variant of PaLM,"
trained for larger iterations on a better quality dataset.,trained for larger iterations on a better quality dataset.
PaLM-,PaLM-
"2 shows significant improvements over PaLM, while reducing","2 shows significant improvements over PaLM, while reducing"
training and inference costs due to its smaller size.,training and inference costs due to its smaller size.
To lessen,To lessen
"toxicity and memorization, it appends special tokens with a","toxicity and memorization, it appends special tokens with a"
"fraction of pre-training data, which shows a reduction in gener-","fraction of pre-training data, which shows a reduction in gener-"
U-PaLM [124]: This method trains PaLM for 0.1% addi-,U-PaLM [124]: This method trains PaLM for 0.1% addi-
tional compute with the UL2 (also named as UL2Restore) ob-,tional compute with the UL2 (also named as UL2Restore) ob-
"jective [125], using the same dataset it outperforms the baseline","jective [125], using the same dataset it outperforms the baseline"
"significantly on various NLP tasks, including zero-shot, few-","significantly on various NLP tasks, including zero-shot, few-"
"shot, commonsense reasoning, CoT, etc.","shot, commonsense reasoning, CoT, etc."
Training with UL2R,Training with UL2R
involves converting a causal decoder PaLM to a non-causal de-,involves converting a causal decoder PaLM to a non-causal de-
"coder PaLM and employing 50% sequential denoising, 25%","coder PaLM and employing 50% sequential denoising, 25%"
"regular denoising, and 25% extreme denoising loss functions.","regular denoising, and 25% extreme denoising loss functions."
Page 10:,Page 10:
UL2 [125]: An encoder-decoder architecture trained using a,UL2 [125]: An encoder-decoder architecture trained using a
mixture of denoisers (MoD) objective.,mixture of denoisers (MoD) objective.
Denoisers include 1),Denoisers include 1)
"R-Denoiser: a regular span masking, 2) S-Denoiser: which cor-","R-Denoiser: a regular span masking, 2) S-Denoiser: which cor-"
rupts consecutive tokens of a large sequence and 3) X-Denoiser:,rupts consecutive tokens of a large sequence and 3) X-Denoiser:
which corrupts a large number of tokens randomly.,which corrupts a large number of tokens randomly.
During pre-,During pre-
"training, UL2 includes a denoiser token from R, S, X to rep-","training, UL2 includes a denoiser token from R, S, X to rep-"
resent a denoising setup.,resent a denoising setup.
It helps improve fine-tuning perfor-,It helps improve fine-tuning perfor-
mance for downstream tasks that bind the task to one of the up-,mance for downstream tasks that bind the task to one of the up-
stream training modes.,stream training modes.
This MoD style of training outperforms,This MoD style of training outperforms
the T5 model on many benchmarks.,the T5 model on many benchmarks.
GLM-130B [33]: GLM-130B is a bilingual (English and Chi-,GLM-130B [33]: GLM-130B is a bilingual (English and Chi-
nese) model trained using an auto-regressive mask infilling pre-,nese) model trained using an auto-regressive mask infilling pre-
training objective similar to the GLM [126].,training objective similar to the GLM [126].
This training style,This training style
"makes the model bidirectional as compared to GPT-3, which is","makes the model bidirectional as compared to GPT-3, which is"
unidirectional.,unidirectional.
"As opposed to GLM, the training of GLM-130B","As opposed to GLM, the training of GLM-130B"
includes a small amount of multi-task instruction pre-training,includes a small amount of multi-task instruction pre-training
data (5% of the total data) along with self-supervised mask in-,data (5% of the total data) along with self-supervised mask in-
filling.,filling.
"To stabilize the training, it applies embedding layer gra-","To stabilize the training, it applies embedding layer gra-"
"LLaMA [127, 21]: A set of decoder-only language models","LLaMA [127, 21]: A set of decoder-only language models"
varying from 7B to 70B parameters.,varying from 7B to 70B parameters.
LLaMA models series is,LLaMA models series is
the most famous among the community for parameter efficiency,the most famous among the community for parameter efficiency
LLaMA-1 [127]: Implements efficient causal attention [128],LLaMA-1 [127]: Implements efficient causal attention [128]
by not storing and computing masked attention weights and,by not storing and computing masked attention weights and
key/query scores.,key/query scores.
Another optimization is reducing the number,Another optimization is reducing the number
"of activations recomputed in the backward pass, as in [129].","of activations recomputed in the backward pass, as in [129]."
LLaMA-2 [21]: This work is more focused on fine-tuning a,LLaMA-2 [21]: This work is more focused on fine-tuning a
safer and better LLaMA-2-Chat model for dialogue generation.,safer and better LLaMA-2-Chat model for dialogue generation.
The pre-trained model has 40% more training data with a larger,The pre-trained model has 40% more training data with a larger
context length and grouped-query attention.,context length and grouped-query attention.
LLaMA-3/3.1 [130]: A collection of models trained on a,LLaMA-3/3.1 [130]: A collection of models trained on a
seven times larger dataset as compared to LLaMA-2 with dou-,seven times larger dataset as compared to LLaMA-2 with dou-
"ble the context length, outperforming its previous variants and","ble the context length, outperforming its previous variants and"
PanGu-Σ [92]: An autoregressive model with parameters,PanGu-Σ [92]: An autoregressive model with parameters
copied from PanGu-α and extended to a trillion scale with Ran-,copied from PanGu-α and extended to a trillion scale with Ran-
"dom Routed Experts (RRE), the architectural diagram is shown","dom Routed Experts (RRE), the architectural diagram is shown"
in Figure 10.,in Figure 10.
"RRE is similar to the MoE architecture, with","RRE is similar to the MoE architecture, with"
"distinctions at the second level, where tokens are randomly","distinctions at the second level, where tokens are randomly"
routed to experts in a domain instead of using a learnable gat-,routed to experts in a domain instead of using a learnable gat-
ing method.,ing method.
The model has bottom layers densely activated and,The model has bottom layers densely activated and
"shared across all domains, whereas top layers are sparsely ac-","shared across all domains, whereas top layers are sparsely ac-"
tivated according to the domain.,tivated according to the domain.
This training style allows for,This training style allows for
extracting task-specific models and reduces catastrophic forget-,extracting task-specific models and reduces catastrophic forget-
ting effects in the case of continual learning.,ting effects in the case of continual learning.
Mixtral8x22b [131]: A mixture-of-experts (MoE) model with,Mixtral8x22b [131]: A mixture-of-experts (MoE) model with
eight distinct experts routes each token to two experts at each,eight distinct experts routes each token to two experts at each
layer and combines the outputs additively.,layer and combines the outputs additively.
Snowflake Arctic [132]: Arctic LLM is a hybrid of dense and,Snowflake Arctic [132]: Arctic LLM is a hybrid of dense and
mixture-of-experts (MoE) architecture.,mixture-of-experts (MoE) architecture.
The MoE (128×3.66B,The MoE (128×3.66B
MLP experts) is parallel to the dense transformer (10B) with,MLP experts) is parallel to the dense transformer (10B) with
only two experts activated.,only two experts activated.
"The model has many experts, com-","The model has many experts, com-"
"pared to other MoE LLMs [131, 133], to increase the model","pared to other MoE LLMs [131, 133], to increase the model"
capacity and provide an opportunity to choose among many ex-,capacity and provide an opportunity to choose among many ex-
perts for a diverse configuration.,perts for a diverse configuration.
The model has 480B param-,The model has 480B param-
"eters, and only 17B are active during a forward pass, reducing","eters, and only 17B are active during a forward pass, reducing"
"Grok [133, 134]: Grok is a family of LLMs including Grok-1","Grok [133, 134]: Grok is a family of LLMs including Grok-1"
"and Grok-1.5, released by XAI.","and Grok-1.5, released by XAI."
Grok-1 [133]: Grok-1 is a 314B parameters language MoE,Grok-1 [133]: Grok-1 is a 314B parameters language MoE
"model (eight experts), where two experts are activated per to-","model (eight experts), where two experts are activated per to-"
Grok-1.5 [134]: Grok-1.5 is a multi-modal LLM with a larger,Grok-1.5 [134]: Grok-1.5 is a multi-modal LLM with a larger
context length and improved performance.,context length and improved performance.
"Gemini [135, 136]: Gemini replaces Bard (based on PaLM)","Gemini [135, 136]: Gemini replaces Bard (based on PaLM)"
with multi-modal capabilities and significant language model-,with multi-modal capabilities and significant language model-
Gemini-1 [135]: The first-ever auto-regressive model to,Gemini-1 [135]: The first-ever auto-regressive model to
achieve human-level capabilities on the MMLU benchmark.,achieve human-level capabilities on the MMLU benchmark.
Gemini-1.5 [136]: A multi-modal LLM with MoE architec-,Gemini-1.5 [136]: A multi-modal LLM with MoE architec-
ture builds on the findings of Gemini-1.,ture builds on the findings of Gemini-1.
The model has a 2M,The model has a 2M
context window and can reason over information up to 10M,context window and can reason over information up to 10M
tokens.,tokens.
Such large context windows were never achieved pre-,Such large context windows were never achieved pre-
viously and shown to have a huge impact on performance gain.,viously and shown to have a huge impact on performance gain.
Nemotron-4 340B [137]: A decoder-only model that has been,Nemotron-4 340B [137]: A decoder-only model that has been
aligned on 98% synthetic data and only 2% manually annotated,aligned on 98% synthetic data and only 2% manually annotated
data.,data.
Utilizing synthetic data at a large proportion improves the,Utilizing synthetic data at a large proportion improves the
model performance significantly.,model performance significantly.
The paper suggested intro-,The paper suggested intro-
ducing alignment data with a smaller subset of previously seen,ducing alignment data with a smaller subset of previously seen
"data during the late stage of the model pre-training, enabling the","data during the late stage of the model pre-training, enabling the"
smooth transition from the pre-trained stage to the final train-,smooth transition from the pre-trained stage to the final train-
ing stage.,ing stage.
"To train better instruction-following models, weaker","To train better instruction-following models, weaker"
models are trained into stronger models iteratively.,models are trained into stronger models iteratively.
The syn-,The syn-
thetic data generated by the weaker instruction-tuned model is,thetic data generated by the weaker instruction-tuned model is
used to train a base model which is later supervised fine-tuned,used to train a base model which is later supervised fine-tuned
DeepSeek [138]: DeepSeek studies the LLMs scaling laws,DeepSeek [138]: DeepSeek studies the LLMs scaling laws
in detail to determine the optimal non-embedding model size,in detail to determine the optimal non-embedding model size
and training data.,and training data.
The experiments were performed for 8 bud-,The experiments were performed for 8 bud-
gets ranging from 1e17 to 3e20 training FLOPs.,gets ranging from 1e17 to 3e20 training FLOPs.
Each compute,Each compute
budget was tested against ten different models/data scales.,budget was tested against ten different models/data scales.
The,The
batch size and learning rates were also fitted for the given com-,batch size and learning rates were also fitted for the given com-
pute budget finding that the batch size should increase with,pute budget finding that the batch size should increase with
the increased compute budget while decreasing the learning,the increased compute budget while decreasing the learning
rate.,rate.
"Following are the equations for the optimal batch-size (B),","Following are the equations for the optimal batch-size (B),"
"learning rate (η), model size (M), and data (D):","learning rate (η), model size (M), and data (D):"
"Mbase = 0.1715, Dbase = 5.8316, a = 0.5243, b = 0.4757","Mbase = 0.1715, Dbase = 5.8316, a = 0.5243, b = 0.4757"
DeepSeek-v2 [139]: An MoE model that introduces multi-,DeepSeek-v2 [139]: An MoE model that introduces multi-
"head latent attention (MLA) to reduce inference costs, by com-","head latent attention (MLA) to reduce inference costs, by com-"
pressing Key-Value (KV) cache into a latent vector.,pressing Key-Value (KV) cache into a latent vector.
"achieves better performance than multi-head attention (MHA),","achieves better performance than multi-head attention (MHA),"
and other efficient attention mechanisms such as grouped query,and other efficient attention mechanisms such as grouped query
"attention (GQA), multi-query attention (MQA), etc.","attention (GQA), multi-query attention (MQA), etc."
Because,Because
"of MLA, DeepSeek-v2 achieves 5.76 times faster inference","of MLA, DeepSeek-v2 achieves 5.76 times faster inference"
throughput as compared to DeepSeek [138].,throughput as compared to DeepSeek [138].
Page 11:,Page 11:
CodeGen has a similar architecture to,CodeGen has a similar architecture to
"PaLM [15], i.e., parallel attention, MLP layers, and RoPE em-","PaLM [15], i.e., parallel attention, MLP layers, and RoPE em-"
beddings.,beddings.
The model is trained on both natural language and,The model is trained on both natural language and
programming language data sequentially (trained on the first,programming language data sequentially (trained on the first
"dataset, then the second, and so on) on the following datasets","dataset, then the second, and so on) on the following datasets"
"1) PILE, 2) BIGQUERY, and 3) BIGPYTHON.","1) PILE, 2) BIGQUERY, and 3) BIGPYTHON."
CodeGen pro-,CodeGen pro-
posed a multi-step approach to synthesizing code.,posed a multi-step approach to synthesizing code.
The purpose,The purpose
is to simplify the generation of long sequences where the previ-,is to simplify the generation of long sequences where the previ-
ous prompt and generated code are given as input with the next,ous prompt and generated code are given as input with the next
prompt to generate the next code sequence.,prompt to generate the next code sequence.
CodeGen open-,CodeGen open-
source a Multi-Turn Programming Benchmark (MTPB) to eval-,source a Multi-Turn Programming Benchmark (MTPB) to eval-
Codex [141]: This LLM is trained on a subset of public Python,Codex [141]: This LLM is trained on a subset of public Python
Github repositories to generate code from docstrings.,Github repositories to generate code from docstrings.
Com-,Com-
puter programming is an iterative process where the programs,puter programming is an iterative process where the programs
are often debugged and updated before fulfilling the require-,are often debugged and updated before fulfilling the require-
ments.,ments.
"Similarly, Codex generates 100 versions of a program","Similarly, Codex generates 100 versions of a program"
"by repetitive sampling for a given description, which produces","by repetitive sampling for a given description, which produces"
a working solution for 77.5% of the problems passing unit tests.,a working solution for 77.5% of the problems passing unit tests.
Its powerful version powers Github Copilot2.,Its powerful version powers Github Copilot2.
"AlphaCode [142]: A set of large language models, ranging","AlphaCode [142]: A set of large language models, ranging"
"from 300M to 41B parameters, designed for competition-level","from 300M to 41B parameters, designed for competition-level"
code generation tasks.,code generation tasks.
It uses the multi-query attention [143] to,It uses the multi-query attention [143] to
reduce memory and cache costs.,reduce memory and cache costs.
Since competitive program-,Since competitive program-
ming problems highly require deep reasoning and an under-,ming problems highly require deep reasoning and an under-
"standing of complex natural language algorithms, the Alpha-","standing of complex natural language algorithms, the Alpha-"
Code models are pre-trained on filtered GitHub code in popular,Code models are pre-trained on filtered GitHub code in popular
languages and then fine-tuned on a new competitive program-,languages and then fine-tuned on a new competitive program-
ming dataset named CodeContests.,ming dataset named CodeContests.
The CodeContests dataset,The CodeContests dataset
"mainly contains problems, solutions, and test cases collected","mainly contains problems, solutions, and test cases collected"
from the Codeforces platform3.,from the Codeforces platform3.
The pre-training employs stan-,The pre-training employs stan-
"dard language modeling objectives, while GOLD [144] with","dard language modeling objectives, while GOLD [144] with"
tempering [145] serves as the training objective for the fine-,tempering [145] serves as the training objective for the fine-
tuning on CodeContests data.,tuning on CodeContests data.
To evaluate the performance of,To evaluate the performance of
"AlphaCode, simulated programming competitions are hosted","AlphaCode, simulated programming competitions are hosted"
"on the Codeforces platform: overall, AlphaCode ranks at the","on the Codeforces platform: overall, AlphaCode ranks at the"
"top 54.3% among over 5000 competitors, where its Codeforces","top 54.3% among over 5000 competitors, where its Codeforces"
rating is within the top 28% of recently participated users.,rating is within the top 28% of recently participated users.
"CodeT5+ [34]: CodeT5+ is based on CodeT5 [146], with","CodeT5+ [34]: CodeT5+ is based on CodeT5 [146], with"
"shallow encoder and deep decoder, trained in multiple stages","shallow encoder and deep decoder, trained in multiple stages"
initially unimodal data (code) and later bimodal data (text-code,initially unimodal data (code) and later bimodal data (text-code
pairs).,pairs).
Each training stage has different training objectives and,Each training stage has different training objectives and
"activates different model blocks encoder, decoder, or both ac-","activates different model blocks encoder, decoder, or both ac-"
cording to the task.,cording to the task.
The unimodal pre-training includes span,The unimodal pre-training includes span
"denoising and CLM objectives, whereas bimodal pre-training","denoising and CLM objectives, whereas bimodal pre-training"
"objectives contain contrastive learning, matching, and CLM for","objectives contain contrastive learning, matching, and CLM for"
text-code pairs.,text-code pairs.
CodeT5+ adds special tokens with the text to,CodeT5+ adds special tokens with the text to
"enable task modes, for example, [CLS ] for contrastive loss,","enable task modes, for example, [CLS ] for contrastive loss,"
"[Match] for text-code matching, etc.","[Match] for text-code matching, etc."
StarCoder [147]: A decoder-only model with the SantaCoder,StarCoder [147]: A decoder-only model with the SantaCoder
"architecture, employing Flash attention to scale up the context","architecture, employing Flash attention to scale up the context"
length to 8k.,length to 8k.
"The StarCoder trains an encoder to filter names,","The StarCoder trains an encoder to filter names,"
"emails, and other personal data from the training data.","emails, and other personal data from the training data."
Its fine-,Its fine-
"tuned variant outperforms PaLM, LLaMA, and LAMDA on","tuned variant outperforms PaLM, LLaMA, and LAMDA on"
Galactica [148]: A large curated corpus of human scientific,Galactica [148]: A large curated corpus of human scientific
"knowledge with 48 million papers, textbooks, lecture notes,","knowledge with 48 million papers, textbooks, lecture notes,"
"millions of compounds and proteins, scientific websites, en-","millions of compounds and proteins, scientific websites, en-"
"cyclopedias, and more are trained using the metaseq library3,","cyclopedias, and more are trained using the metaseq library3,"
which is built on PyTorch and fairscale [149].,which is built on PyTorch and fairscale [149].
The model wraps,The model wraps
reasoning datasets with the < work > token to provide step-by-,reasoning datasets with the < work > token to provide step-by-
"step reasoning context to the model, which has been shown to","step reasoning context to the model, which has been shown to"
improve the performance on reasoning tasks.,improve the performance on reasoning tasks.
LaMDA [150]: A decoder-only model pre-trained on pub-,LaMDA [150]: A decoder-only model pre-trained on pub-
"lic dialog data, public dialog utterances, and public web doc-","lic dialog data, public dialog utterances, and public web doc-"
"uments, where more than 90% of the pre-training data is in","uments, where more than 90% of the pre-training data is in"
English.,English.
LaMDA is trained with the objective of producing re-,LaMDA is trained with the objective of producing re-
"sponses that exhibit high levels of quality, safety, and grounded-","sponses that exhibit high levels of quality, safety, and grounded-"
ness.,ness.
"To achieve this, discriminative and generative fine-tuning","To achieve this, discriminative and generative fine-tuning"
techniques are incorporated to enhance the model’s safety and,techniques are incorporated to enhance the model’s safety and
quality aspects.,quality aspects.
"As a result, the LaMDA models can be utilized","As a result, the LaMDA models can be utilized"
as a general language model performing various tasks.,as a general language model performing various tasks.
BloombergGPT [151]: A non-causal decoder model trained,BloombergGPT [151]: A non-causal decoder model trained
using both financial (“FINPILE” from the Bloomberg archive),using both financial (“FINPILE” from the Bloomberg archive)
and general-purpose datasets.,and general-purpose datasets.
The model’s architecture is sim-,The model’s architecture is sim-
ilar to the BLOOM [13] and OPT [14].,ilar to the BLOOM [13] and OPT [14].
It allocates 50B param-,It allocates 50B param-
eters to different blocks of the model using the approach [113].,eters to different blocks of the model using the approach [113].
"For effective training, BloombergGPT packs documents to-","For effective training, BloombergGPT packs documents to-"
gether with < |endoftext| > to use the maximum sequence,gether with < |endoftext| > to use the maximum sequence
"length, uses warmup batch size starting from 1024 to 2048, and","length, uses warmup batch size starting from 1024 to 2048, and"
manually reduces the learning rate multiple times during the,manually reduces the learning rate multiple times during the
Xuan Yuan 2.0 [152]: A Chinese financial chat model with,Xuan Yuan 2.0 [152]: A Chinese financial chat model with
BLOOM’s [13] architecture trained on a combination of general,BLOOM’s [13] architecture trained on a combination of general
"purpose, financial, general purpose instructions, and financial","purpose, financial, general purpose instructions, and financial"
institutions datasets.,institutions datasets.
Xuan Yuan 2.0 combined the pre-training,Xuan Yuan 2.0 combined the pre-training
and fine-tuning stages to avoid catastrophic forgetting.,and fine-tuning stages to avoid catastrophic forgetting.
Pre-trained LLMs have excellent generalization abilities to,Pre-trained LLMs have excellent generalization abilities to
unseen tasks.,unseen tasks.
"However, because they are generally trained with","However, because they are generally trained with"
"the objective of next token prediction, LLMs have limited ca-","the objective of next token prediction, LLMs have limited ca-"
"pacity to follow user intent and are prone to generate unethical,","pacity to follow user intent and are prone to generate unethical,"
toxic or inaccurate responses [20].,toxic or inaccurate responses [20].
For their effective utiliza-,For their effective utiliza-
"tion, LLMs are fine-tuned to follow instructions [16, 17, 97] and","tion, LLMs are fine-tuned to follow instructions [16, 17, 97] and"
"generate safe responses [20], which also results in increasing","generate safe responses [20], which also results in increasing"
"zero-shot, few-shot, and cross-task generalization [97, 16, 18],","zero-shot, few-shot, and cross-task generalization [97, 16, 18],"
"with minimal compute increment, e.g., 0.2% of the total pre-","with minimal compute increment, e.g., 0.2% of the total pre-"
training for PaLM 540B [16].,training for PaLM 540B [16].
We review various fine-tuned LLMs and strategies for effective,We review various fine-tuned LLMs and strategies for effective
Page 12:,Page 12:
Table 1: Noteworthy findings and insights of pre-trained Large Language Models.,Table 1: Noteworthy findings and insights of pre-trained Large Language Models.
• Encoder and decoder with shared parameters perform equivalently when parameters are not shared,• Encoder and decoder with shared parameters perform equivalently when parameters are not shared
• Fine-tuning model layers (adapter layers) work better than the conventional way of training on only,• Fine-tuning model layers (adapter layers) work better than the conventional way of training on only
"• Few-shot performance of LLMs is better than the zero-shot, suggesting that LLMs are meta-","• Few-shot performance of LLMs is better than the zero-shot, suggesting that LLMs are meta-"
• Large multi-lingual models perform equivalently to single language models on downstream tasks.,• Large multi-lingual models perform equivalently to single language models on downstream tasks.
"However, smaller multi-lingual models perform worse","However, smaller multi-lingual models perform worse"
• LLMs have good few shot capabilities,• LLMs have good few shot capabilities
• Prompt fine-tuning requires updating very few parameters while achieving performance compara-,• Prompt fine-tuning requires updating very few parameters while achieving performance compara-
ble to full model fine-tuning,ble to full model fine-tuning
• Prompt fine-tuning takes more time to converge as compared to full model fine-tuning,• Prompt fine-tuning takes more time to converge as compared to full model fine-tuning
• Inserting prompt tokens in-between sentences can allow the model to understand relations between,• Inserting prompt tokens in-between sentences can allow the model to understand relations between
"• In an analysis, CPM-2 finds that prompts work as a provider (additional context) and aggregator","• In an analysis, CPM-2 finds that prompts work as a provider (additional context) and aggregator"
(aggregate information with the input text) for the model,(aggregate information with the input text) for the model
• A modular LLM architecture with a universal representation module and task-specific representa-,• A modular LLM architecture with a universal representation module and task-specific representa-
tion module helps in the finetuning phase,tion module helps in the finetuning phase
• Optimizing the parameters of a task-specific representation network during the fine-tuning phase is,• Optimizing the parameters of a task-specific representation network during the fine-tuning phase is
an efficient way to take advantage of the powerful pre-trained model,an efficient way to take advantage of the powerful pre-trained model
• The performance of LLM is highly related to the network size,• The performance of LLM is highly related to the network size
"• To improve runtime performance, more operations can be performed in parallel (width) rather than","• To improve runtime performance, more operations can be performed in parallel (width) rather than"
"• To efficiently represent and fit more text in the same context length, the model uses a larger vo-","• To efficiently represent and fit more text in the same context length, the model uses a larger vo-"
cabulary to train a SentencePiece tokenizer without restricting it to word boundaries.,cabulary to train a SentencePiece tokenizer without restricting it to word boundaries.
This further,This further
benefits in few-shot learning tasks,benefits in few-shot learning tasks
"• By employing prompt-based tuning, the performances of models can be improved, often surpassing","• By employing prompt-based tuning, the performances of models can be improved, often surpassing"
those of state-of-the-art models when the backward gradients of inputs are accessible,those of state-of-the-art models when the backward gradients of inputs are accessible
• The model architecture that excels in pre-training and fine-tuning cases may exhibit contrasting,• The model architecture that excels in pre-training and fine-tuning cases may exhibit contrasting
behavior in zero-shot and few-shot learning,behavior in zero-shot and few-shot learning
• Relative encodings enable the model to evaluate for longer sequences than training.,• Relative encodings enable the model to evaluate for longer sequences than training.
• Additional self-supervised adversarial loss to distinguish between real and generated text improves,• Additional self-supervised adversarial loss to distinguish between real and generated text improves
the model performance as compared to ERNIE 3.0,the model performance as compared to ERNIE 3.0
• Parallel attention + FF layers speed-up training 15% with the same performance as with cascaded,• Parallel attention + FF layers speed-up training 15% with the same performance as with cascaded
• Initializing feed-forward output layers before residuals with scheme in [153] avoids activations,• Initializing feed-forward output layers before residuals with scheme in [153] avoids activations
from growing with increasing depth and width,from growing with increasing depth and width
• Training on Pile outperforms GPT-3 on five-shot,• Training on Pile outperforms GPT-3 on five-shot
Table Continued on Next Page,Table Continued on Next Page
Page 13:,Page 13:
• Restart training from an earlier checkpoint with a lower learning rate if loss diverges,• Restart training from an earlier checkpoint with a lower learning rate if loss diverges
• Model is prone to generate repetitive text and stuck in a loop,• Model is prone to generate repetitive text and stuck in a loop
"• Galactica’s performance has continued to improve across validation set, in-domain, and out-of-","• Galactica’s performance has continued to improve across validation set, in-domain, and out-of-"
"domain benchmarks, even with multiple repetitions of the corpus, which is superior to existing","domain benchmarks, even with multiple repetitions of the corpus, which is superior to existing"
• A working memory token approach can achieve strong performance over existing methods on,• A working memory token approach can achieve strong performance over existing methods on
mathematical MMLU and MATH benchmarks.,mathematical MMLU and MATH benchmarks.
It sets a new state-of-the-art on several downstream,It sets a new state-of-the-art on several downstream
tasks such as PubMedQA (77.6%) and MedMCQA dev (52.9%),tasks such as PubMedQA (77.6%) and MedMCQA dev (52.9%)
• The model capacity can be maintained at reduced computation by replacing the feed-forward layer,• The model capacity can be maintained at reduced computation by replacing the feed-forward layer
in each transformer layer with a mixture-of-experts (MoE),in each transformer layer with a mixture-of-experts (MoE)
• The model trained on filtered data shows consistently better performances on both NLG and NLU,• The model trained on filtered data shows consistently better performances on both NLG and NLU
"tasks, where the effect of filtering is more significant on the former tasks","tasks, where the effect of filtering is more significant on the former tasks"
"• Filtered pretraining corpora play a crucial role in the generation capability of LLMs, especially for","• Filtered pretraining corpora play a crucial role in the generation capability of LLMs, especially for"
• The scaling of GLaM MoE models can be achieved by increasing the size or number of experts in,• The scaling of GLaM MoE models can be achieved by increasing the size or number of experts in
the MoE layer.,the MoE layer.
"Given a fixed budget of computation, more experts contribute to a better perfor-","Given a fixed budget of computation, more experts contribute to a better perfor-"
• The model can be fine-tuned to learn to call different external information resources and tools,• The model can be fine-tuned to learn to call different external information resources and tools
"• For higher effectiveness and efficiency, a transformer model can be asymmetrically constructed","• For higher effectiveness and efficiency, a transformer model can be asymmetrically constructed"
with a shallower encoder and a deeper decoder,with a shallower encoder and a deeper decoder
"• To achieve better performances, it is necessary to employ strategies such as massively scaling","• To achieve better performances, it is necessary to employ strategies such as massively scaling"
"upsampling, followed by the filtering and clustering of samples into a compact set","upsampling, followed by the filtering and clustering of samples into a compact set"
• The utilization of novel sampling-efficient transformer architectures designed to facilitate large-,• The utilization of novel sampling-efficient transformer architectures designed to facilitate large-
• Simplifying problem descriptions can effectively improve the model’s performance,• Simplifying problem descriptions can effectively improve the model’s performance
• The model size and the number of training tokens should be scaled proportionately: for each dou-,• The model size and the number of training tokens should be scaled proportionately: for each dou-
"bling of the model size, the number of training tokens should be doubled as well","bling of the model size, the number of training tokens should be doubled as well"
• English-centric models produce better translations when translating to English as compared to non-,• English-centric models produce better translations when translating to English as compared to non-
• Generalized models can have equivalent performance for language translation to specialized small,• Generalized models can have equivalent performance for language translation to specialized small
• Larger models have a higher percentage of training data memorization,• Larger models have a higher percentage of training data memorization
"• Performance has not yet saturated even at 540B scale, which means larger models are likely to","• Performance has not yet saturated even at 540B scale, which means larger models are likely to"
• Encoder-decoder architecture is more suitable to train LLMs given bidirectional attention to the,• Encoder-decoder architecture is more suitable to train LLMs given bidirectional attention to the
• Causal Language Modeling (CLM) task can be added to benefit the model with efficient in-context,• Causal Language Modeling (CLM) task can be added to benefit the model with efficient in-context
• Placing layer norm at the beginning of each transformer layer improves the training stability,• Placing layer norm at the beginning of each transformer layer improves the training stability
Table Continued on Next Page,Table Continued on Next Page
Page 14:,Page 14:
• Training with a mixture of denoisers outperforms PaLM when trained further for a few more FLOPs,• Training with a mixture of denoisers outperforms PaLM when trained further for a few more FLOPs
• Training with a mixture of denoisers improves the infilling ability and open-ended text generation,• Training with a mixture of denoisers improves the infilling ability and open-ended text generation
• Mode switching training enables better performance on downstream tasks,• Mode switching training enables better performance on downstream tasks
• CoT prompting outperforms standard prompting for UL2,• CoT prompting outperforms standard prompting for UL2
• Pre-training data with a small proportion of multi-task instruction data improves the overall model,• Pre-training data with a small proportion of multi-task instruction data improves the overall model
• Multi-step prompting for code synthesis leads to a better user intent understanding and code gen-,• Multi-step prompting for code synthesis leads to a better user intent understanding and code gen-
• A constant performance improvement is observed when scaling the model,• A constant performance improvement is observed when scaling the model
• Smaller models can achieve good performances with more training data and computing time,• Smaller models can achieve good performances with more training data and computing time
• Sparse models provide the benefits of large models at a lower computation cost,• Sparse models provide the benefits of large models at a lower computation cost
• Randomly Routed Experts reduces catastrophic forgetting effects which in turn is essential for,• Randomly Routed Experts reduces catastrophic forgetting effects which in turn is essential for
• Randomly Routed Experts allow extracting a domain-specific sub-model in deployment which is,• Randomly Routed Experts allow extracting a domain-specific sub-model in deployment which is
cost-efficient while maintaining a performance similar to the original,cost-efficient while maintaining a performance similar to the original
• Pre-training with general-purpose and task-specific data improves task performance without hurt-,• Pre-training with general-purpose and task-specific data improves task performance without hurt-
• Combining pre-training and fine-tuning stages in single training avoids catastrophic forgetting,• Combining pre-training and fine-tuning stages in single training avoids catastrophic forgetting
• Causal LM is crucial for a model’s generation capability in encoder-decoder architectures,• Causal LM is crucial for a model’s generation capability in encoder-decoder architectures
"• Multiple training objectives like span corruption, Causal LM, matching, etc complement each other","• Multiple training objectives like span corruption, Causal LM, matching, etc complement each other"
• HHH prompt by Anthropic allows the model to follow instructions without fine-tuning,• HHH prompt by Anthropic allows the model to follow instructions without fine-tuning
• Model trained on unfiltered data is more toxic but may perform better on downstream tasks after,• Model trained on unfiltered data is more toxic but may perform better on downstream tasks after
• Model trained on unfiltered data requires fewer samples for safety alignment,• Model trained on unfiltered data requires fewer samples for safety alignment
• Data quality is important to train better models,• Data quality is important to train better models
• Model and data size should be scaled with 1:1 proportions,• Model and data size should be scaled with 1:1 proportions
• Smaller models trained for larger iterations outperform larger models,• Smaller models trained for larger iterations outperform larger models
• Increasing batch size gradually stabilizes the training without loss spikes,• Increasing batch size gradually stabilizes the training without loss spikes
• High-quality data at the final stages of training improves the model performance,• High-quality data at the final stages of training improves the model performance
• Increasing model context length windows step-wise allows it to better adapt to various sequence,• Increasing model context length windows step-wise allows it to better adapt to various sequence
• Model aligned iteratively on synthetic data with data generated from the previously aligned model,• Model aligned iteratively on synthetic data with data generated from the previously aligned model
• Batch size should increase with the increase in compute budget while decreasing the learning rate,• Batch size should increase with the increase in compute budget while decreasing the learning rate
• Mult-head latent attention (MLA) performs better than multi-head attention (MHA) while requiring,• Mult-head latent attention (MLA) performs better than multi-head attention (MHA) while requiring
"a significantly smaller KV cache, therefore achieving faster data generation","a significantly smaller KV cache, therefore achieving faster data generation"
Page 15:,Page 15:
Table 2: Key insights and findings from the study of instruction-tuned Large Language Models.,Table 2: Key insights and findings from the study of instruction-tuned Large Language Models.
• Multi-task prompting enables zero-shot generalization and outperforms baselines,• Multi-task prompting enables zero-shot generalization and outperforms baselines
• Even a single prompt per dataset task is enough to improve performance,• Even a single prompt per dataset task is enough to improve performance
"• To aid the model in effectively filtering and utilizing relevant information, human labelers play a","• To aid the model in effectively filtering and utilizing relevant information, human labelers play a"
crucial role in answering questions regarding the usefulness of the retrieved documents,crucial role in answering questions regarding the usefulness of the retrieved documents
• Interacting a fine-tuned language model with a text-based web-browsing environment can improve,• Interacting a fine-tuned language model with a text-based web-browsing environment can improve
end-to-end retrieval and synthesis via imitation learning and reinforcement learning,end-to-end retrieval and synthesis via imitation learning and reinforcement learning
• Generating answers with references can make labelers easily judge the factual accuracy of answers,• Generating answers with references can make labelers easily judge the factual accuracy of answers
• Instruction tuning leads to a stronger generalization of unseen tasks,• Instruction tuning leads to a stronger generalization of unseen tasks
• More tasks improve generalization whereas only increasing task instances does not help,• More tasks improve generalization whereas only increasing task instances does not help
• Supervised trained models are better than generalized models,• Supervised trained models are better than generalized models
• Models pre-trained with instructions and examples perform well for different types of inputs,• Models pre-trained with instructions and examples perform well for different types of inputs
• Instruction tuning enables zero-shot generalization to tasks never seen before,• Instruction tuning enables zero-shot generalization to tasks never seen before
• Multi-lingual training leads to even better zero-shot generalization for both English and non-,• Multi-lingual training leads to even better zero-shot generalization for both English and non-
• Training on machine-translated prompts improves performance for held-out tasks with non-English,• Training on machine-translated prompts improves performance for held-out tasks with non-English
• English only fine-tuning on multilingual pre-trained language model is enough to generalize to,• English only fine-tuning on multilingual pre-trained language model is enough to generalize to
• Creating a batch with multiple task examples is important for better performance,• Creating a batch with multiple task examples is important for better performance
"• Only example proportional sampling is not enough, training datasets should also be proportional","• Only example proportional sampling is not enough, training datasets should also be proportional"
• Fully held-out and partially supervised tasks performance improves by scaling tasks or categories,• Fully held-out and partially supervised tasks performance improves by scaling tasks or categories
whereas fully supervised tasks have no effect,whereas fully supervised tasks have no effect
• Including small amounts i.e.,• Including small amounts i.e.
5% of pretraining data during fine-tuning is effective,5% of pretraining data during fine-tuning is effective
"• Only 1% reasoning data improves the performance, adding more deteriorates performance","• Only 1% reasoning data improves the performance, adding more deteriorates performance"
• Adding dialogue data makes the performance worse,• Adding dialogue data makes the performance worse
• Labelers’ judgment and well-defined alignment rules help the model generate better responses,• Labelers’ judgment and well-defined alignment rules help the model generate better responses
• Good dialogue goals can be broken down into detailed natural language rules for the agent and the,• Good dialogue goals can be broken down into detailed natural language rules for the agent and the
• The combination of reinforcement learning (RL) with reranking yields optimal performance in,• The combination of reinforcement learning (RL) with reranking yields optimal performance in
terms of preference win rates and resilience against adversarial probing,terms of preference win rates and resilience against adversarial probing
• Finetuning with CoT improves performance on held-out tasks,• Finetuning with CoT improves performance on held-out tasks
• Fine-tuning along with CoT data improves reasoning abilities,• Fine-tuning along with CoT data improves reasoning abilities
• CoT tuning improves zero-shot reasoning,• CoT tuning improves zero-shot reasoning
• Performance improves with more tasks,• Performance improves with more tasks
• Instruction fine-tuning improves usability which otherwise is challenging for pre-trained models,• Instruction fine-tuning improves usability which otherwise is challenging for pre-trained models
• Improving the model’s performance with instruction tuning is compute-efficient,• Improving the model’s performance with instruction tuning is compute-efficient
• Multitask prompting enables zero-shot generalization abilities in LLM,• Multitask prompting enables zero-shot generalization abilities in LLM
• Fine-tuning with re-written instruction-tuning data into a complex set improves performance,• Fine-tuning with re-written instruction-tuning data into a complex set improves performance
"• Model learns to write safe responses with fine-tuning on safe demonstrations, while additional","• Model learns to write safe responses with fine-tuning on safe demonstrations, while additional"
RLHF step further improves model safety and make it less prone to jailbreak attacks,RLHF step further improves model safety and make it less prone to jailbreak attacks
• Less high quality data is enough for fine-tuned model generalization,• Less high quality data is enough for fine-tuned model generalization
Page 16:,Page 16:
"Figure 10: This example illustrates the PanGu-P architecture, as depicted in","Figure 10: This example illustrates the PanGu-P architecture, as depicted in"
the image sourced from [92].,the image sourced from [92].
3.2.1.,3.2.1.
Instruction-Tuning with Manually Created Datasets,Instruction-Tuning with Manually Created Datasets
Numerous hand-crafted instruction-tuning datasets with,Numerous hand-crafted instruction-tuning datasets with
different design choices are proposed in the literature to,different design choices are proposed in the literature to
instruction-tune LLMs.,instruction-tune LLMs.
The performance of fine-tuned LLMs,The performance of fine-tuned LLMs
"depends on multiple factors, such as dataset, instruction diver-","depends on multiple factors, such as dataset, instruction diver-"
"sity, prompting templates, model size, and training objectives.","sity, prompting templates, model size, and training objectives."
"Keeping this in view, diverse fine-tuned models have emerged","Keeping this in view, diverse fine-tuned models have emerged"
in the literature using manually created datasets.,in the literature using manually created datasets.
The models T0 [17] and mT0 (multi-lingual) [154] employ,The models T0 [17] and mT0 (multi-lingual) [154] employ
templates to convert existing datasets into prompt datasets.,templates to convert existing datasets into prompt datasets.
They have shown improvements in generalization to zero-shot,They have shown improvements in generalization to zero-shot
and held-out tasks.,and held-out tasks.
Tk-Instruct [18] fine-tuned the T5 model,Tk-Instruct [18] fine-tuned the T5 model
with in-context instructions to study generalization on unseen,with in-context instructions to study generalization on unseen
tasks when given in-context instructions during test time.,tasks when given in-context instructions during test time.
The,The
"model outperformed Instruct-GPT, despite being smaller in","model outperformed Instruct-GPT, despite being smaller in"
"size, i.e., 11B parameters as compared to 175B of GPT-3.","size, i.e., 11B parameters as compared to 175B of GPT-3."
Increasing Tasks and Prompt Setups: Zero-shot and few-shot,Increasing Tasks and Prompt Setups: Zero-shot and few-shot
performance improves significantly by expanding task collec-,performance improves significantly by expanding task collec-
tion and prompt styles.,tion and prompt styles.
OPT-IML [97] and Flan [16] curated,OPT-IML [97] and Flan [16] curated
"larger 2k and 1.8k task datasets, respectively.","larger 2k and 1.8k task datasets, respectively."
While increasing,While increasing
"task size alone is not enough, OPT-IML and Flan add more","task size alone is not enough, OPT-IML and Flan add more"
"prompting setups in their datasets, zero-shot, few-shot, and","prompting setups in their datasets, zero-shot, few-shot, and"
CoT.,CoT.
"In continuation, CoT Collection [101] fine-tunes Flan-T5","In continuation, CoT Collection [101] fine-tunes Flan-T5"
further on 1.88M CoT samples.,further on 1.88M CoT samples.
Another method [102] uses,Another method [102] uses
"symbolic tasks with tasks in T0, Flan, etc.","symbolic tasks with tasks in T0, Flan, etc."
3.2.2.,3.2.2.
Instruction-Tuning with LLMs Generated Datasets,Instruction-Tuning with LLMs Generated Datasets
Generating an instruction-tuning dataset requires carefully,Generating an instruction-tuning dataset requires carefully
"writing instructions and input-output pairs, which are often","writing instructions and input-output pairs, which are often"
"written by humans, smaller in size, and less diverse.","written by humans, smaller in size, and less diverse."
To over-,To over-
"come this, self-instruct [19] proposed an approach to prompt","come this, self-instruct [19] proposed an approach to prompt"
available LLMs to generate instruction-tuning datasets.,available LLMs to generate instruction-tuning datasets.
Self-,Self-
instruct outperformed models trained on manually created,instruct outperformed models trained on manually created
dataset SUPER-NATURALINSTRUCTIONS (a dataset with,dataset SUPER-NATURALINSTRUCTIONS (a dataset with
1600+ tasks) [18] by 33%.,1600+ tasks) [18] by 33%.
"It starts with a seed of 175 tasks, 1","It starts with a seed of 175 tasks, 1"
"instruction, and 1 sample per task and iteratively generates new","instruction, and 1 sample per task and iteratively generates new"
instructions (52k) and instances (82k input-output pairs) using,instructions (52k) and instances (82k input-output pairs) using
"Figure 11: An example image shows an instance of the Flan training paradigm,","Figure 11: An example image shows an instance of the Flan training paradigm,"
GPT-3 [6].,GPT-3 [6].
"Contrary to this, Dynosaur [155] uses the meta-data","Contrary to this, Dynosaur [155] uses the meta-data"
of datasets on Huggingface to prompt LLMs to generate multi-,of datasets on Huggingface to prompt LLMs to generate multi-
LLaMA Tuned: Various models in the literature instruction-,LLaMA Tuned: Various models in the literature instruction-
tune LLaMA [156] with GPT-3 [6] or GPT-4 [157] gener-,tune LLaMA [156] with GPT-3 [6] or GPT-4 [157] gener-
"Among these, Alpaca [158], Vicuna [159],","Among these, Alpaca [158], Vicuna [159],"
and LLaMA-GPT-4 [160] are a few general-purpose fine-tuned,and LLaMA-GPT-4 [160] are a few general-purpose fine-tuned
"models, where Alpaca is trained on 52k samples from text-","models, where Alpaca is trained on 52k samples from text-"
"davinci-003, Vicuna on 70k samples from ShareGPT.com, and","davinci-003, Vicuna on 70k samples from ShareGPT.com, and"
LLaMA-GPT-4 by re-creating Alpaca instructions from GPT-,LLaMA-GPT-4 by re-creating Alpaca instructions from GPT-
4.,4.
Goat [161] fine-tunes LLaMA for arithmetic tasks (1 million,Goat [161] fine-tunes LLaMA for arithmetic tasks (1 million
samples) by generating data from ChatGPT and outperforms,samples) by generating data from ChatGPT and outperforms
"GPT-4, PaLM, BLOOM, OPT, etc., attributing its success to the","GPT-4, PaLM, BLOOM, OPT, etc., attributing its success to the"
LLaMA’s consistent tokenization of numbers.,LLaMA’s consistent tokenization of numbers.
HuaTuo [162] is,HuaTuo [162] is
"a medical knowledge model, fine-tuned with a generated QA","a medical knowledge model, fine-tuned with a generated QA"
"Complex Instructions: Evol-Instruct [163, 164] prompts LLMs","Complex Instructions: Evol-Instruct [163, 164] prompts LLMs"
to convert given instructions into a more complex set.,to convert given instructions into a more complex set.
The in-,The in-
structions are iteratively evolved with re-writing instructions in,structions are iteratively evolved with re-writing instructions in
complex wording and creating new instructions.,complex wording and creating new instructions.
With this style,With this style
"of automated instruction generation, WizardLM [163] (fine-","of automated instruction generation, WizardLM [163] (fine-"
"tuned LLaMA on 250k instructions), outperforms Vicuna and","tuned LLaMA on 250k instructions), outperforms Vicuna and"
"Alpaca, and WizardCoder [164] (fine-tuned StarCoder) beats","Alpaca, and WizardCoder [164] (fine-tuned StarCoder) beats"
3.2.3.,3.2.3.
Aligning with Human Preferences,Aligning with Human Preferences
Incorporating human preferences into LLMs presents a,Incorporating human preferences into LLMs presents a
significant advantage in mitigating undesirable behaviors and,significant advantage in mitigating undesirable behaviors and
ensuring accurate outputs.,ensuring accurate outputs.
"The initial work on alignment, such","The initial work on alignment, such"
"as InstructGPT [20] aligns GPT-3 using a 3-step approach,","as InstructGPT [20] aligns GPT-3 using a 3-step approach,"
"instruction-tuning, reward modeling, and fine-tuning with","instruction-tuning, reward modeling, and fine-tuning with"
reinforcement learning (RL).,reinforcement learning (RL).
The supervised fine-tuned GPT-3,The supervised fine-tuned GPT-3
"on demonstrations is queried to generate responses, which","on demonstrations is queried to generate responses, which"
"human labelers rank according to human values, and a reward","human labelers rank according to human values, and a reward"
model is trained on the ranked data.,model is trained on the ranked data.
"Lastly, the GPT-3 is trained","Lastly, the GPT-3 is trained"
with proximal policy optimization (PPO) using rewards on the,with proximal policy optimization (PPO) using rewards on the
generated data from the reward model.,generated data from the reward model.
LLaMA 2-Chat [21],LLaMA 2-Chat [21]
improves alignment by dividing reward modeling into help-,improves alignment by dividing reward modeling into help-
fulness and safety rewards and using rejection sampling in,fulness and safety rewards and using rejection sampling in
addition to PPO.,addition to PPO.
The initial four versions of LLaMA 2-Chat,The initial four versions of LLaMA 2-Chat
are fine-tuned with rejection sampling and then with PPO on,are fine-tuned with rejection sampling and then with PPO on
Page 17:,Page 17:
Aligning with Supported Evidence: This style of alignment,Aligning with Supported Evidence: This style of alignment
"allows the model to generate responses with proofs and facts,","allows the model to generate responses with proofs and facts,"
"reduces hallucination, and assists humans more effectively,","reduces hallucination, and assists humans more effectively,"
which increases trust in the model’s output.,which increases trust in the model’s output.
"the RLHF training style, a reward model is trained to rank","the RLHF training style, a reward model is trained to rank"
generated responses containing web citations in answers,generated responses containing web citations in answers
"to questions, which is later used to train the model, as in","to questions, which is later used to train the model, as in"
"GopherCite [165], WebGPT [166], and Sparrow [167].","GopherCite [165], WebGPT [166], and Sparrow [167]."
The,The
"ranking model in Sparrow [167] is divided into two branches,","ranking model in Sparrow [167] is divided into two branches,"
"preference reward and rule reward, where human annotators","preference reward and rule reward, where human annotators"
adversarial probe the model to break a rule.,adversarial probe the model to break a rule.
These two rewards,These two rewards
together rank a response to train with RL.,together rank a response to train with RL.
Aligning Directly with SFT: The PPO in the RLHF pipeline,Aligning Directly with SFT: The PPO in the RLHF pipeline
"is complex, memory-intensive, and unstable, requiring mul-","is complex, memory-intensive, and unstable, requiring mul-"
"tiple models, reward, value, policy, and reference models.","tiple models, reward, value, policy, and reference models."
Avoiding this sophisticated alignment pipeline is possible by,Avoiding this sophisticated alignment pipeline is possible by
incorporating minimal changes in the supervised fine-tuning,incorporating minimal changes in the supervised fine-tuning
"(SFT) pipeline as in [168, 169, 170], with better or compa-","(SFT) pipeline as in [168, 169, 170], with better or compa-"
rable performance to PPO.,rable performance to PPO.
Direct preference optimization,Direct preference optimization
(DPO) [168] trains a model directly on the human-preferred,(DPO) [168] trains a model directly on the human-preferred
responses to maximize the likelihood of preferred against,responses to maximize the likelihood of preferred against
"unpreferred responses, with per-sample importance weight.","unpreferred responses, with per-sample importance weight."
Reward ranked fine-tuning RAFT [169] fine-tunes the model,Reward ranked fine-tuning RAFT [169] fine-tunes the model
on ranked responses by the reward model.,on ranked responses by the reward model.
Preference ranking,Preference ranking
optimization (PRO) [171] and RRHF [170] penalize the model,optimization (PRO) [171] and RRHF [170] penalize the model
to rank responses with human preferences and supervised loss.,to rank responses with human preferences and supervised loss.
"On the other hand, chain-of-hindsight (CoH) [172] provides","On the other hand, chain-of-hindsight (CoH) [172] provides"
"feedback to the model in language rather than reward, to learn","feedback to the model in language rather than reward, to learn"
human feedback is slow and costly.,human feedback is slow and costly.
The literature suggests a,The literature suggests a
semi-automated process to align LLMs by prompting LLMs to,semi-automated process to align LLMs by prompting LLMs to
"generate helpful, honest, and ethical responses to the queries,","generate helpful, honest, and ethical responses to the queries,"
and fine-tuning using the newly created dataset.,and fine-tuning using the newly created dataset.
Constitutional,Constitutional
"AI [173] replaces human feedback in RLHF with AI, calling","AI [173] replaces human feedback in RLHF with AI, calling"
it RL from AI feedback (RLAIF).,it RL from AI feedback (RLAIF).
AlpacaFarm [174] designs,AlpacaFarm [174] designs
prompts to imitate human feedback using LLMs APIs.,prompts to imitate human feedback using LLMs APIs.
Oppo-,Oppo-
"site to constitutional AI, AlpacaFarm injects noise in feedback","site to constitutional AI, AlpacaFarm injects noise in feedback"
"LLM with ICL examples, instructing the LLM about what the","LLM with ICL examples, instructing the LLM about what the"
response should contain to be considered useful and ethical.,response should contain to be considered useful and ethical.
The same LLM is later fine-tuned with the new dataset.,The same LLM is later fine-tuned with the new dataset.
Aligning with Prompts: LLMs can be steered with prompts to,Aligning with Prompts: LLMs can be steered with prompts to
"generate desirable responses without training [175, 176].","generate desirable responses without training [175, 176]."
The,The
self-correction prompting in [176] concatenates instructions,self-correction prompting in [176] concatenates instructions
"and CoT with questions, guiding the model to answer its","and CoT with questions, guiding the model to answer its"
instruction following a strategy to ensure moral safety before,instruction following a strategy to ensure moral safety before
the actual answer.,the actual answer.
This strategy is shown to reduce the harm in,This strategy is shown to reduce the harm in
"exhibit harmful behaviors, hallucinations, leaking personal in-","exhibit harmful behaviors, hallucinations, leaking personal in-"
"formation, and other shortcomings through adversarial probing.","formation, and other shortcomings through adversarial probing."
The models are susceptible to generating harmful responses,The models are susceptible to generating harmful responses
"even though they are aligned for safety [177, 178].","even though they are aligned for safety [177, 178]."
"teaming is a common approach to address illicit outputs, where","teaming is a common approach to address illicit outputs, where"
"the LLMs are prompted to generate harmful outputs [178, 179].","the LLMs are prompted to generate harmful outputs [178, 179]."
The dataset collected through red-teaming is used to fine-tune,The dataset collected through red-teaming is used to fine-tune
models for safety.,models for safety.
While red-teaming largely relies on human,While red-teaming largely relies on human
"annotators, another work [180] red-team LLMs to find prompts","annotators, another work [180] red-team LLMs to find prompts"
that lead to harmful outputs for other LLMs.,that lead to harmful outputs for other LLMs.
"Although fine-tuning boosts a model’s performance, it leads","Although fine-tuning boosts a model’s performance, it leads"
to catastrophic forgetting of previously learned information.,to catastrophic forgetting of previously learned information.
Concatenating fine-tuning data with a few randomly selected,Concatenating fine-tuning data with a few randomly selected
pre-training samples in every iteration avoids network forget-,pre-training samples in every iteration avoids network forget-
"ting [181, 152].","ting [181, 152]."
This is also effective in adapting LLMs for,This is also effective in adapting LLMs for
cases where fine-tuning data is small and the original capac-,cases where fine-tuning data is small and the original capac-
ity is to be maintained.,ity is to be maintained.
Prompt-based continued pre-training,Prompt-based continued pre-training
(PCP) [182] trains the model with text and instructions related,(PCP) [182] trains the model with text and instructions related
to tasks and then finally instruction-tunes the model for down-,to tasks and then finally instruction-tunes the model for down-
While fine-tuning data is generally many-fold smaller than,While fine-tuning data is generally many-fold smaller than
"the pre-training data, it still has to be large enough for accept-","the pre-training data, it still has to be large enough for accept-"
"able performance [16, 97, 18] and requires proportional com-","able performance [16, 97, 18] and requires proportional com-"
puting resources.,puting resources.
Studying the effects on performance with less,Studying the effects on performance with less
"data, existing literature [183, 184] finds that models trained","data, existing literature [183, 184] finds that models trained"
on less data can outperform models trained with more data.,on less data can outperform models trained with more data.
"In [183], 25% of the total downstream data is found enough","In [183], 25% of the total downstream data is found enough"
for state-of-the-art performance.,for state-of-the-art performance.
Selecting coreset-based 0.5%,Selecting coreset-based 0.5%
of the total instruction-tuning data improves the model perfor-,of the total instruction-tuning data improves the model perfor-
"mance by 2% in [184], as compared to the complete data tun-","mance by 2% in [184], as compared to the complete data tun-"
ing.,ing.
Less is more for alignment (LIMA) [185] uses only 1000,Less is more for alignment (LIMA) [185] uses only 1000
carefully created demonstrations to fine-tune the model and has,carefully created demonstrations to fine-tune the model and has
achieved comparable performance to GPT-4.,achieved comparable performance to GPT-4.
LLMs are trained with limited context windows due to ex-,LLMs are trained with limited context windows due to ex-
pensive attention and high memory requirements.,pensive attention and high memory requirements.
trained on limited sequence lengths fails to generalize to unseen,trained on limited sequence lengths fails to generalize to unseen
"lengths at inference time [186, 49].","lengths at inference time [186, 49]."
"Alternatively, LLMs with","Alternatively, LLMs with"
ALiBi [65] positional encodings can perform zero-shot length,ALiBi [65] positional encodings can perform zero-shot length
extrapolation.,extrapolation.
"However, ALiBi has less expressive power [66]","However, ALiBi has less expressive power [66]"
"and inferior performance on multiple benchmarks [46], and","and inferior performance on multiple benchmarks [46], and"
many LLMs use RoPE positional embedding that is unable to,many LLMs use RoPE positional embedding that is unable to
perform zero-shot extrapolation.,perform zero-shot extrapolation.
A larger context length has,A larger context length has
"benefits such as a better understanding of longer documents,","benefits such as a better understanding of longer documents,"
"more samples in in-context learning, execution of bigger rea-","more samples in in-context learning, execution of bigger rea-"
"soning processes, etc.","soning processes, etc."
Expanding context length during fine-,Expanding context length during fine-
"tuning is slow, inefficient, and computationally expensive [49].","tuning is slow, inefficient, and computationally expensive [49]."
"Therefore, researchers employ various context window extrap-","Therefore, researchers employ various context window extrap-"
"Position Interpolation: Rather than extrapolating, [49] shows","Position Interpolation: Rather than extrapolating, [49] shows"
that interpolating position encodings within the pre-trained con-,that interpolating position encodings within the pre-trained con-
text window are more effective.,text window are more effective.
The work demonstrates that,The work demonstrates that
only 1000 steps of fine-tuning are enough to achieve better re-,only 1000 steps of fine-tuning are enough to achieve better re-
sults on larger windows without reducing performance com-,sults on larger windows without reducing performance com-
pared to the original context size.,pared to the original context size.
Giraffe [46] uses power scal-,Giraffe [46] uses power scal-
"ing in RoPE, and YaRN [47] proposed NTK-aware interpola-","ing in RoPE, and YaRN [47] proposed NTK-aware interpola-"
Page 18:,Page 18:
one of the major constraints in training larger context win-,one of the major constraints in training larger context win-
"Using efficient attention variants, such as lo-","Using efficient attention variants, such as lo-"
"cal, sparse, and dilated attention, reduces the computation cost","cal, sparse, and dilated attention, reduces the computation cost"
LongT5 [48] proposes transient global atten-,LongT5 [48] proposes transient global atten-
"tion (TGlobal), applying attention to local and global tokens","tion (TGlobal), applying attention to local and global tokens"
"in T5 [10] with TGlobal attention, pre-trains the model on","in T5 [10] with TGlobal attention, pre-trains the model on"
"4098 sequence length, fine-tunes on larger window sizes, as","4098 sequence length, fine-tunes on larger window sizes, as"
"large as 16k, and improves task performance on longer inputs.","large as 16k, and improves task performance on longer inputs."
This shows the extrapolation ability of TGlobal attention with,This shows the extrapolation ability of TGlobal attention with
only fine-tuning.,only fine-tuning.
"COLT5 [187] uses two branches, one with","COLT5 [187] uses two branches, one with"
lightweight and the other with heavyweight attention and feed-,lightweight and the other with heavyweight attention and feed-
forward layers.,forward layers.
All tokens are processed from the lightweight,All tokens are processed from the lightweight
"branch, and only important tokens are routed to the heavy-","branch, and only important tokens are routed to the heavy-"
weight branch.,weight branch.
LongNet [188] replaces standard attention with,LongNet [188] replaces standard attention with
"dilated attention, expanding sequence length to 1 billion tokens.","dilated attention, expanding sequence length to 1 billion tokens."
"LongLoRA [189] proposes shift-short attention, used during","LongLoRA [189] proposes shift-short attention, used during"
fine-tuning to reduce dense attention costs.,fine-tuning to reduce dense attention costs.
"However, the model","However, the model"
during inference uses dense attention and achieves similar per-,during inference uses dense attention and achieves similar per-
formance as full attention fine-tuning.,formance as full attention fine-tuning.
Extrapolation without Training: LM-Infinite [186] and par-,Extrapolation without Training: LM-Infinite [186] and par-
allel context windows (PCW) [190] show length extrapolation,allel context windows (PCW) [190] show length extrapolation
is possible using pre-trained LLMs.,is possible using pre-trained LLMs.
LM-Infinite suggested Λ-,LM-Infinite suggested Λ-
shaped attention applied within the original context window,shaped attention applied within the original context window
limits.,limits.
"Likewise, PCW chunks larger inputs into the pre-trained","Likewise, PCW chunks larger inputs into the pre-trained"
context lengths and applies the same positional encodings to,context lengths and applies the same positional encodings to
LLMs are capable of learning from the examples concate-,LLMs are capable of learning from the examples concate-
"nated with the input, known as context augmentation, in-","nated with the input, known as context augmentation, in-"
"context learning (ICL), or few-shot prompting.","context learning (ICL), or few-shot prompting."
They show ex-,They show ex-
cellent generalization to unseen tasks with few-shot prompt-,cellent generalization to unseen tasks with few-shot prompt-
"ing, enabling LLMs to answer queries beyond the capacity ac-","ing, enabling LLMs to answer queries beyond the capacity ac-"
"quired during training [6, 55].","quired during training [6, 55]."
These emergent abilities allow,These emergent abilities allow
for adapting the model without fine-tuning—a costly process.,for adapting the model without fine-tuning—a costly process.
"Aside from this, hallucination, producing inaccurate, unsafe,","Aside from this, hallucination, producing inaccurate, unsafe,"
"or factually incorrect responses, is common for LLMs, which is","or factually incorrect responses, is common for LLMs, which is"
avoided by augmenting contextual data.,avoided by augmenting contextual data.
While the user can pro-,While the user can pro-
"vide in-context samples in the query [54, 32], here we specifi-","vide in-context samples in the query [54, 32], here we specifi-"
cally refer to the methods that access external storage program-,cally refer to the methods that access external storage program-
"matically, calling them augmented LLMs.","matically, calling them augmented LLMs."
The literature suggests various external memory designs to aug-,The literature suggests various external memory designs to aug-
"ment LLMs, long-term [191, 192, 193, 194], short-term [195],","ment LLMs, long-term [191, 192, 193, 194], short-term [195],"
"symbolic [196], and non-symbolic [197, 198].","symbolic [196], and non-symbolic [197, 198]."
The memory,The memory
"can be maintained in different formats such as documents, vec-","can be maintained in different formats such as documents, vec-"
"tors, or databases.","tors, or databases."
A few systems maintain intermediate mem-,A few systems maintain intermediate mem-
ory representations to retain information across multiple iter-,ory representations to retain information across multiple iter-
"ations [194, 192], while others extract important information","ations [194, 192], while others extract important information"
from the datasets and save it in memory for recall [199].,from the datasets and save it in memory for recall [199].
The,The
memory read and write operations are performed either with,memory read and write operations are performed either with
"or without LLMs cooperation [192, 200, 194, 201], acting as","or without LLMs cooperation [192, 200, 194, 201], acting as"
a feedback signal in [195].,a feedback signal in [195].
We discuss different types of aug-,We discuss different types of aug-
Figure 12: A flow diagram of Retrieval Augmented LLMs.,Figure 12: A flow diagram of Retrieval Augmented LLMs.
The retriever ex-,The retriever ex-
tracts a similar context to the input and forwards it to the LLM either in simple,tracts a similar context to the input and forwards it to the LLM either in simple
language or encoded through Fusion-in-Decoder (FiD).,language or encoded through Fusion-in-Decoder (FiD).
"Depending on the task,","Depending on the task,"
retrieval and generation may repeat multiple times.,retrieval and generation may repeat multiple times.
"LLMs may have limited memory and outdated information,","LLMs may have limited memory and outdated information,"
leading to inaccurate responses.,leading to inaccurate responses.
Retrieving relevant informa-,Retrieving relevant informa-
tion from external up-to-date storage enables the LLMs to,tion from external up-to-date storage enables the LLMs to
accurately answer with references and utilize more informa-,accurately answer with references and utilize more informa-
tion.,tion.
"With retrieval augmentation, smaller models have been","With retrieval augmentation, smaller models have been"
shown to perform at par with larger models.,shown to perform at par with larger models.
"For instance, the","For instance, the"
11B model can become competitive to 540B PaLM in [25] and,11B model can become competitive to 540B PaLM in [25] and
7.5B to 280B Gopher in [193].,7.5B to 280B Gopher in [193].
Retrieval augmented language,Retrieval augmented language
"modeling (RALM) has two major components, shown in","modeling (RALM) has two major components, shown in"
"Figure 12, namely: 1) retriever and 2) language model.","Figure 12, namely: 1) retriever and 2) language model."
"RALM, the retriever plays a crucial role in driving LLM","RALM, the retriever plays a crucial role in driving LLM"
"response, where incorrect information can steer LLMs to false","response, where incorrect information can steer LLMs to false"
behavior.,behavior.
This leads to the development of various methods to,This leads to the development of various methods to
retrieve accurate information and fuse with the query for better,retrieve accurate information and fuse with the query for better
Zero-Shot Retrieval Augmentation: This kind of augmen-,Zero-Shot Retrieval Augmentation: This kind of augmen-
tation keeps the original LLM architecture and weights,tation keeps the original LLM architecture and weights
"unchanged and uses BM25 [202], nearest neighbors, or frozen","unchanged and uses BM25 [202], nearest neighbors, or frozen"
pre-trained models like Bert [7] as a retriever.,pre-trained models like Bert [7] as a retriever.
The retrieved,The retrieved
information is provided as input to the model for response,information is provided as input to the model for response
"generation, shown to improve performance over LLMs without","generation, shown to improve performance over LLMs without"
"In some scenarios, multiple retrieval","In some scenarios, multiple retrieval"
iterations are required to complete the task.,iterations are required to complete the task.
generated in the first iteration is forwarded to the retriever,generated in the first iteration is forwarded to the retriever
to fetch similar documents.,to fetch similar documents.
Forward-looking active retrieval,Forward-looking active retrieval
(FLARE) [197] initially generates the response and corrects,(FLARE) [197] initially generates the response and corrects
the output by retrieving relevant documents if the response,the output by retrieving relevant documents if the response
contains low-confidence tokens.,contains low-confidence tokens.
"Similarly, RepoCoder [204]","Similarly, RepoCoder [204]"
fetches code snippets recursively for code completion.,fetches code snippets recursively for code completion.
Training with Retrieval Augmentation: To reduce failures in,Training with Retrieval Augmentation: To reduce failures in
"retrieval augmentation generation (RAG), researchers train or","retrieval augmentation generation (RAG), researchers train or"
fine-tune retrievers and LLMs with a retrieval augmentation,fine-tune retrievers and LLMs with a retrieval augmentation
pipeline.,pipeline.
We discuss the literature below based on their focus,We discuss the literature below based on their focus
on the respective training processes of the pipeline.,on the respective training processes of the pipeline.
Training LLM: Retrieval-enhanced transformer (RETRO) [193],Training LLM: Retrieval-enhanced transformer (RETRO) [193]
shows pre-training smaller LLMs with RAG pipeline outper-,shows pre-training smaller LLMs with RAG pipeline outper-
"forms larger LLMs, such as GPT-3 trained without RAG.","forms larger LLMs, such as GPT-3 trained without RAG."
RETRO uses a 2-trillion token subset of MassiveText as,RETRO uses a 2-trillion token subset of MassiveText as
Page 19:,Page 19:
The retrieval pipeline divides the input query,The retrieval pipeline divides the input query
into subsets and retrieves relevant chunks from the database,into subsets and retrieves relevant chunks from the database
"for each subset, encoded together with input intermediate","for each subset, encoded together with input intermediate"
representations for generating tokens.,representations for generating tokens.
It uses cross-chunked,It uses cross-chunked
attention to attend to previous chunks auto-regressively.,attention to attend to previous chunks auto-regressively.
study on RETRO [205] shows models pre-trained without RAG,study on RETRO [205] shows models pre-trained without RAG
but fine-tuned using RAG lack the performance gains obtained,but fine-tuned using RAG lack the performance gains obtained
Training Retriever: Quality of responses generated by LLMs,Training Retriever: Quality of responses generated by LLMs
is highly dependent on the in-context examples.,is highly dependent on the in-context examples.
"fore, [206, 207, 208, 209] train retrievers to retrieve accurate","fore, [206, 207, 208, 209] train retrievers to retrieve accurate"
few-shot samples while keeping the LLM frozen for gener-,few-shot samples while keeping the LLM frozen for gener-
Retrieved samples are ranked to build ground-truth,Retrieved samples are ranked to build ground-truth
"data to train retrievers with contrastive learning in [206, 208].","data to train retrievers with contrastive learning in [206, 208]."
RoBERTa is trained for downstream tasks in [207] for ICL,RoBERTa is trained for downstream tasks in [207] for ICL
REPLUG [209] trains the retriever with,REPLUG [209] trains the retriever with
supervised signals from the frozen LLM-generated outputs.,supervised signals from the frozen LLM-generated outputs.
Training Retriever and LLM: Further benefits are achieved by,Training Retriever and LLM: Further benefits are achieved by
"training both the retriever and the model in [25, 210, 211].","training both the retriever and the model in [25, 210, 211]."
In,In
"this case, the error propagates back to the retriever, updating","this case, the error propagates back to the retriever, updating"
both the language model and the retriever.,both the language model and the retriever.
language modeling (MLM) is a common pre-training objec-,language modeling (MLM) is a common pre-training objec-
"tive [25, 211], retrieval pre-trained transformer (RPT) [210]","tive [25, 211], retrieval pre-trained transformer (RPT) [210]"
used document chunk prediction as a pre-training objective for,used document chunk prediction as a pre-training objective for
documents with the query becomes infeasible as the sequence,documents with the query becomes infeasible as the sequence
length and sample size grow.,length and sample size grow.
Encoding the context and fusing,Encoding the context and fusing
it with the decoder (Fusion-in-Decoder) using cross-attention,it with the decoder (Fusion-in-Decoder) using cross-attention
makes it possible to augment more samples without increasing,makes it possible to augment more samples without increasing
"computation costs significantly [212, 193, 210, 25].","computation costs significantly [212, 193, 210, 25]."
"Locally stored memory, but external to","Locally stored memory, but external to"
"LLM, has limited information.","LLM, has limited information."
"However, a large amount of","However, a large amount of"
"information is available on the internet, which gets updated","information is available on the internet, which gets updated"
"Rather than storing information locally, various","Rather than storing information locally, various"
methods retrieve query-related context through a web search,methods retrieve query-related context through a web search
"and forward it to LLMs [213, 214, 166].","and forward it to LLMs [213, 214, 166]."
While RAG relies on the retriever to provide context to the,While RAG relies on the retriever to provide context to the
"LLM to answer queries, tool augmented LLMs capitalize on the","LLM to answer queries, tool augmented LLMs capitalize on the"
reasoning abilities of LLMs to iteratively plan by dividing tasks,reasoning abilities of LLMs to iteratively plan by dividing tasks
"into sub-tasks, selecting necessary tools, and taking actions to","into sub-tasks, selecting necessary tools, and taking actions to"
"complete the task [215, 216, 217, 27].","complete the task [215, 216, 217, 27]."
A generic pipeline of,A generic pipeline of
"tool-augmented LLMs is shown in Figure 13, where different","tool-augmented LLMs is shown in Figure 13, where different"
modules in Figure 13 are selected in a loop until the task com-,modules in Figure 13 are selected in a loop until the task com-
Zero-Shot Tool Augmentation: LLMs in-context learning and,Zero-Shot Tool Augmentation: LLMs in-context learning and
reasoning abilities enable them to interact with tools with-,reasoning abilities enable them to interact with tools with-
out training.,out training.
Automatic reasoning and tool-use (ART) [217],Automatic reasoning and tool-use (ART) [217]
builds a task library with demonstrations of reasoning steps and,builds a task library with demonstrations of reasoning steps and
calling external tools.,calling external tools.
It retrieves similar task examples and,It retrieves similar task examples and
provides the context to the LLM for inference.,provides the context to the LLM for inference.
Aside from,Aside from
"this, [218] shows tool documentation is enough to teach LLMs","this, [218] shows tool documentation is enough to teach LLMs"
to use tools without demonstrations.,to use tools without demonstrations.
RestGPT [219] integrates,RestGPT [219] integrates
LLMs with RESTful APIs by decomposing tasks into planning,LLMs with RESTful APIs by decomposing tasks into planning
Figure 13: A basic flow diagram of tool augmented LLMs.,Figure 13: A basic flow diagram of tool augmented LLMs.
Given an input and,Given an input and
"a set of available tools, the model generates a plan to complete the task.","a set of available tools, the model generates a plan to complete the task."
The,The
"tool augmented LLMs utilize different modules iteratively, such as retriever,","tool augmented LLMs utilize different modules iteratively, such as retriever,"
"tool execution, read-write to memory, feedback, etc., depending on the task.","tool execution, read-write to memory, feedback, etc., depending on the task."
and API selection steps.,and API selection steps.
The API selector understands the API,The API selector understands the API
documentation to select a suitable API for the task and plan the,documentation to select a suitable API for the task and plan the
execution.,execution.
ToolkenGPT [220] uses tools as tokens by concate-,ToolkenGPT [220] uses tools as tokens by concate-
nating tool embeddings with other token embeddings.,nating tool embeddings with other token embeddings.
During,During
"inference, the LLM generates the tool tokens representing the","inference, the LLM generates the tool tokens representing the"
"tool call, stops text generation, and restarts using the tool exe-","tool call, stops text generation, and restarts using the tool exe-"
Training with Tool Augmentation: LLMs are trained to inter-,Training with Tool Augmentation: LLMs are trained to inter-
"act with diverse tools, enhancing planning abilities to overcome","act with diverse tools, enhancing planning abilities to overcome"
"the limitations of zero-shot tool augmentation [221, 27, 222,","the limitations of zero-shot tool augmentation [221, 27, 222,"
223].,223].
Gorilla [221] instruction-tunes LLaMA with information,Gorilla [221] instruction-tunes LLaMA with information
retrieval from API documentation.,retrieval from API documentation.
It uses the self-instruct [19],It uses the self-instruct [19]
data generation pipeline with GPT-4 by providing in-context,data generation pipeline with GPT-4 by providing in-context
examples retrieved from API documentation.,examples retrieved from API documentation.
Tool augmented,Tool augmented
language model (TALM) [27] fine-tunes T5 [10] for tool use,language model (TALM) [27] fine-tunes T5 [10] for tool use
"with a self-play approach, where it iteratively completes tool","with a self-play approach, where it iteratively completes tool"
manipulation tasks and includes them back in the training set.,manipulation tasks and includes them back in the training set.
ToolLLM [223] collects 16k APIs from RapidAPI.,ToolLLM [223] collects 16k APIs from RapidAPI.
It samples,It samples
APIs from the list to generate an instruction-tuning dataset us-,APIs from the list to generate an instruction-tuning dataset us-
ing ChatGPT in single-tool and multi-tool scenarios.,ing ChatGPT in single-tool and multi-tool scenarios.
For high-,For high-
"quality datasets, ToolLLM suggested a depth-first search-based","quality datasets, ToolLLM suggested a depth-first search-based"
decision tree (DFSDT) method to generate ground-truths with,decision tree (DFSDT) method to generate ground-truths with
Multimodal Tool Augmentation: The compositional reasoning,Multimodal Tool Augmentation: The compositional reasoning
capacity of LLMs allows them to manipulate tools in multi-,capacity of LLMs allows them to manipulate tools in multi-
"modal settings [215, 216, 224].","modal settings [215, 216, 224]."
Following the pipeline shown,Following the pipeline shown
"in Figure 13, the LLM outlines a plan, generally executing in a","in Figure 13, the LLM outlines a plan, generally executing in a"
sequence: Plan →Tool selection →Execute →Inspect →,sequence: Plan →Tool selection →Execute →Inspect →
"Generate, to respond to the user query.","Generate, to respond to the user query."
"Here, the database of","Here, the database of"
"tools is rich in modalities, including text, images, etc.","tools is rich in modalities, including text, images, etc."
Many of,Many of
the multimodal tool augmentation systems employ multimodal,the multimodal tool augmentation systems employ multimodal
"LLMs [31, 225, 224, 216], while others utilize single modality","LLMs [31, 225, 224, 216], while others utilize single modality"
Page 20:,Page 20:
LLMs and generate a plan on using different modality tools to,LLMs and generate a plan on using different modality tools to
"AI agents are autonomous entities, capable of planning,","AI agents are autonomous entities, capable of planning,"
"decision-making, and performing actions to achieve complex","decision-making, and performing actions to achieve complex"
"In the early days, AI agents were rule-based, de-","In the early days, AI agents were rule-based, de-"
"signed for narrow tasks, and had limited capabilities, such","signed for narrow tasks, and had limited capabilities, such"
as Clippy [227] and Deep Blue [228].,as Clippy [227] and Deep Blue [228].
LLMs abilities to respond to dynamic scenarios have made it,LLMs abilities to respond to dynamic scenarios have made it
"possible to incorporate them in diverse applications, includ-","possible to incorporate them in diverse applications, includ-"
"ing LLMs-powered agents [224, 216], where LLMs behave","ing LLMs-powered agents [224, 216], where LLMs behave"
as the brain of agents.,as the brain of agents.
LLMs have been incorporated in web,LLMs have been incorporated in web
"agents [166, 167], coding agents [229], tool agents [27, 223],","agents [166, 167], coding agents [229], tool agents [27, 223],"
"embodied agents [26], and conversational agents [195], requir-","embodied agents [26], and conversational agents [195], requir-"
"ing minimal to no fine-tuning"".","ing minimal to no fine-tuning""."
Below we summarize the re-,Below we summarize the re-
search in LLMs-based autonomous agents.,search in LLMs-based autonomous agents.
For a more detailed,For a more detailed
"discussion, please refer to [230, 231].","discussion, please refer to [230, 231]."
LLMs Steering Autonomous Agents: LLMs are the cognitive,LLMs Steering Autonomous Agents: LLMs are the cognitive
controllers of the autonomous agents.,controllers of the autonomous agents.
"They generate plans, rea-","They generate plans, rea-"
"son about tasks, incorporate memory to complete tasks, and","son about tasks, incorporate memory to complete tasks, and"
adapt the outline depending on the feedback from the environ-,adapt the outline depending on the feedback from the environ-
ment.,ment.
"Depending on the acquired capabilities of LLMs, many","Depending on the acquired capabilities of LLMs, many"
"methods fine-tune, propose a better prompting approach, or uti-","methods fine-tune, propose a better prompting approach, or uti-"
lize different modules to enhance agents’ performance.,lize different modules to enhance agents’ performance.
Mod-,Mod-
ules and strategies employed in autonomous agents are briefly,ules and strategies employed in autonomous agents are briefly
Planning and Reasoning: Completing a complex task requires,Planning and Reasoning: Completing a complex task requires
"human-like logical thinking, planning necessary steps, and","human-like logical thinking, planning necessary steps, and"
reasoning current and future directions.,reasoning current and future directions.
Prompting methods,Prompting methods
"like chain-of-thoughts [103], tree-of-thoughts [105], and self-","like chain-of-thoughts [103], tree-of-thoughts [105], and self-"
"consistency [104] are central to agents, eliciting LLMs to rea-","consistency [104] are central to agents, eliciting LLMs to rea-"
son its actions and choose among different paths for task com-,son its actions and choose among different paths for task com-
pletion.,pletion.
When LLMs are prompted with a task description and,When LLMs are prompted with a task description and
"a sequence of actions, they can accurately generate plan ac-","a sequence of actions, they can accurately generate plan ac-"
tions without any fine-tuning [232].,tions without any fine-tuning [232].
Reasoning via planning,Reasoning via planning
(RAP) [233] incorporates a re-purposed LLM as a world model,(RAP) [233] incorporates a re-purposed LLM as a world model
to reason about future outcomes and explore alternative paths,to reason about future outcomes and explore alternative paths
for task completion.,for task completion.
Retroformer [234] uses a retrospective,Retroformer [234] uses a retrospective
LLM to improve main LLM planning and reasoning capabil-,LLM to improve main LLM planning and reasoning capabil-
ities by providing helpful task cues.,ities by providing helpful task cues.
Feedback: LLMs in open-loop systems generate plans and as-,Feedback: LLMs in open-loop systems generate plans and as-
sume that the agent will complete them successfully.,sume that the agent will complete them successfully.
"However,","However,"
the actual scenario is different with failures and variable re-,the actual scenario is different with failures and variable re-
sponses from the environment.,sponses from the environment.
"To correctly complete tasks,","To correctly complete tasks,"
many methods use LLMs in a closed-loop where the action re-,many methods use LLMs in a closed-loop where the action re-
sponse is provided as feedback to the LLMs to re-assess and,sponse is provided as feedback to the LLMs to re-assess and
"update the plan as required [235, 236, 237, 195].","update the plan as required [235, 236, 237, 195]."
Another di-,Another di-
rection of research exploits LLMs as reward functions to train,rection of research exploits LLMs as reward functions to train
reinforcement learning (RL) policies instead of humans [238].,reinforcement learning (RL) policies instead of humans [238].
Memory: LLMs can learn from the context provided in the,Memory: LLMs can learn from the context provided in the
prompt.,prompt.
"In addition to internal memory, various systems em-","In addition to internal memory, various systems em-"
ploy external memory to save the response history.,ploy external memory to save the response history.
ion [195] maintains an episodic memory to use previous re-,ion [195] maintains an episodic memory to use previous re-
sponses as feedback to improve future decision-making.,sponses as feedback to improve future decision-making.
Retro-,Retro-
former [234] improves its responses by employing short-term,former [234] improves its responses by employing short-term
"and long-term memory, where short-term memory contains re-","and long-term memory, where short-term memory contains re-"
cent responses and long-term memory keeps summarized failed,cent responses and long-term memory keeps summarized failed
attempts to add in the prompt as reflection.,attempts to add in the prompt as reflection.
Multi-Agents Systems: LLMs can play user-defined roles and,Multi-Agents Systems: LLMs can play user-defined roles and
behave like a specific domain expert.,behave like a specific domain expert.
"In multi-agent systems,","In multi-agent systems,"
"each LLM is assigned a unique role, simulating human behav-","each LLM is assigned a unique role, simulating human behav-"
ior and collaborating with other agents to complete a complex,ior and collaborating with other agents to complete a complex
"instruction-following, however, utilizing them for physically","instruction-following, however, utilizing them for physically"
"grounded tasks requires adaptation, as they lack real-world","grounded tasks requires adaptation, as they lack real-world"
knowledge.,knowledge.
This could lead to generating illogical responses,This could lead to generating illogical responses
"for a particular physical situation [240, 26].","for a particular physical situation [240, 26]."
make LLMs aware of the available low-level task operations.,make LLMs aware of the available low-level task operations.
LLM (Say) builds a high-level plan to complete the task and,LLM (Say) builds a high-level plan to complete the task and
a learned affordance function (Can) explores the possibility of,a learned affordance function (Can) explores the possibility of
executing the plan in the real world.,executing the plan in the real world.
SayCan uses RL to train,SayCan uses RL to train
the language-conditioned affordance function.,the language-conditioned affordance function.
PaLM-E enables,PaLM-E enables
the LLM to solve grounded tasks by training multi-modal LLM,the LLM to solve grounded tasks by training multi-modal LLM
feeding inputs directly from the sensors.,feeding inputs directly from the sensors.
"Manipulation: In the area of manipulation [236, 241], LLMs","Manipulation: In the area of manipulation [236, 241], LLMs"
"enhance a robot’s dexterity and adaptability, excelling in tasks","enhance a robot’s dexterity and adaptability, excelling in tasks"
"like object recognition, grasping, and collaboration.","like object recognition, grasping, and collaboration."
They ana-,They ana-
lyze visual and spatial information to determine the most effec-,lyze visual and spatial information to determine the most effec-
tive approach to interact with objects.,tive approach to interact with objects.
Navigation: LLMs enhance a robot’s ability to navigate com-,Navigation: LLMs enhance a robot’s ability to navigate com-
"plex environments with precision and adaptability [242, 243,","plex environments with precision and adaptability [242, 243,"
"244, 245].","244, 245]."
They generate feasible paths and trajectories for,They generate feasible paths and trajectories for
"robots, accounting for intricate environmental details [246].","robots, accounting for intricate environmental details [246]."
This ability is valuable in scenarios requiring precise and,This ability is valuable in scenarios requiring precise and
dynamically adaptable navigation in environments like ware-,dynamically adaptable navigation in environments like ware-
"houses, transport, healthcare facilities, and residences.","houses, transport, healthcare facilities, and residences."
Deploying LLMs in production is expensive.,Deploying LLMs in production is expensive.
Reducing their,Reducing their
running costs while preserving performance is an appealing,running costs while preserving performance is an appealing
area of research.,area of research.
This section summarizes the approaches sug-,This section summarizes the approaches sug-
gested to enhance LLMs’ efficiency.,gested to enhance LLMs’ efficiency.
Fine-tuning LLMs with tens or hundreds of billions of pa-,Fine-tuning LLMs with tens or hundreds of billions of pa-
"rameters, such as GPT-3 (175B), BLOOM (176B), MT-NLG","rameters, such as GPT-3 (175B), BLOOM (176B), MT-NLG"
"(540B), etc., is computationally intensive and time-consuming.","(540B), etc., is computationally intensive and time-consuming."
"To avoid complete model fine-tuning, numerous parameter-","To avoid complete model fine-tuning, numerous parameter-"
"efficient fine-tuning (PEFT) techniques [40, 247, 41, 38, 39] try","efficient fine-tuning (PEFT) techniques [40, 247, 41, 38, 39] try"
to achieve acceptable model fine-tuning performance at reduced,to achieve acceptable model fine-tuning performance at reduced
costs.,costs.
"As compared to full fine-tuning [248], PEFT performs","As compared to full fine-tuning [248], PEFT performs"
"better in low-resource setups, achieves comparable perfor-","better in low-resource setups, achieves comparable perfor-"
"mance on medium-resource scenarios, and performs worse than","mance on medium-resource scenarios, and performs worse than"
full fine-tuning under high-resource availability.,full fine-tuning under high-resource availability.
An overview,An overview
of different PEFT approaches is shown in Figure 14.,of different PEFT approaches is shown in Figure 14.
Adapter Tuning: Adds a few trainable parameters within the,Adapter Tuning: Adds a few trainable parameters within the
transformer block.,transformer block.
The adapter layer is a sequence of feature,The adapter layer is a sequence of feature
"downscaling, non-linearity, and upscaling [106].","downscaling, non-linearity, and upscaling [106]."
Variants of,Variants of
adapter tuning inject adapter layers sequentially [106] and in,adapter tuning inject adapter layers sequentially [106] and in
"parallel [38], whereas the mixture of adapter (AdaMix) [249]","parallel [38], whereas the mixture of adapter (AdaMix) [249]"
Page 21:,Page 21:
"Figure 14: Illustration of parameter-efficient fine-tuning paradigms, where x is input and h is hidden state, figure courtesy [38].","Figure 14: Illustration of parameter-efficient fine-tuning paradigms, where x is input and h is hidden state, figure courtesy [38]."
Parallel adapter and LoRA fall in,Parallel adapter and LoRA fall in
employs multiple adapter modules in a single layer.,employs multiple adapter modules in a single layer.
AdaMix,AdaMix
routes input instances randomly to one of the multiple down-,routes input instances randomly to one of the multiple down-
scale and upscale modules.,scale and upscale modules.
The mixture of adapters is averaged,The mixture of adapters is averaged
out for inference to avoid additional latency.,out for inference to avoid additional latency.
Low-Rank Adap-,Low-Rank Adap-
tation (LoRA) [250] learns low-rank decomposed matrices to,tation (LoRA) [250] learns low-rank decomposed matrices to
freeze original weights.,freeze original weights.
The learned weights are fused with the,The learned weights are fused with the
"original weights for inference, avoiding latency.","original weights for inference, avoiding latency."
Prompt Tuning: Prompting is an effective way to adapt a,Prompt Tuning: Prompting is an effective way to adapt a
pre-trained LLM for the downstream task.,pre-trained LLM for the downstream task.
"However, manual","However, manual"
"prompts bring uncertainty in the model’s prediction, where a","prompts bring uncertainty in the model’s prediction, where a"
change in a single word drops the performance [247].,change in a single word drops the performance [247].
Prompt,Prompt
tuning alleviates this problem by fine-tuning only 0.001%-3%,tuning alleviates this problem by fine-tuning only 0.001%-3%
additional parameters [251].,additional parameters [251].
It concatenates trainable prompt,It concatenates trainable prompt
"parameters with the model embeddings [247, 40, 251].","parameters with the model embeddings [247, 40, 251]."
Task-,Task-
specific fixed discrete prompts are concatenated with input em-,specific fixed discrete prompts are concatenated with input em-
beddings in [40].,beddings in [40].
"As discrete prompts bring instability, prompts","As discrete prompts bring instability, prompts"
"are encoded through a learnable mapping in P-Tuning [247],","are encoded through a learnable mapping in P-Tuning [247],"
"naming continuous prompts, which are appended with the dis-","naming continuous prompts, which are appended with the dis-"
Only the prompt encoder is trainable in the,Only the prompt encoder is trainable in the
model.,model.
"In an extension of P-Tuning, continuous prompts are","In an extension of P-Tuning, continuous prompts are"
concatenated with each layer of the network in [251].,concatenated with each layer of the network in [251].
Progres-,Progres-
sive prompts [252] avoid catastrophic forgetting and transfer,sive prompts [252] avoid catastrophic forgetting and transfer
previously learned knowledge by sequentially adding trainable,previously learned knowledge by sequentially adding trainable
prompt embeddings to the previously frozen task embeddings.,prompt embeddings to the previously frozen task embeddings.
Prefix Tuning: A set of trainable task-specific prefix vectors,Prefix Tuning: A set of trainable task-specific prefix vectors
are appended to the frozen transformer layers in prefix tun-,are appended to the frozen transformer layers in prefix tun-
ing [41].,ing [41].
The prefix vectors are virtual tokens attended by the,The prefix vectors are virtual tokens attended by the
context tokens on the right.,context tokens on the right.
"In addition, adaptive prefix tun-","In addition, adaptive prefix tun-"
ing [253] applies a gating mechanism to control the information,ing [253] applies a gating mechanism to control the information
from the prefix and actual tokens.,from the prefix and actual tokens.
Bias Tuning: Fine-tuning only bias terms in small to medium,Bias Tuning: Fine-tuning only bias terms in small to medium
training data has been found effective in BitFit [254].,training data has been found effective in BitFit [254].
method achieves full fine-tuning performance for tasks with less,method achieves full fine-tuning performance for tasks with less
training data and comparable performance with more training,training data and comparable performance with more training
LLMs require extensive computing and memory for infer-,LLMs require extensive computing and memory for infer-
Deploying a 175B parameter GPT-3 model needs at,Deploying a 175B parameter GPT-3 model needs at
least five 80GB A100 GPUs and 350GB of memory to store in,least five 80GB A100 GPUs and 350GB of memory to store in
FP16 format [44].,FP16 format [44].
Such demanding requirements for deploying,Such demanding requirements for deploying
LLMs make it harder for smaller organizations to utilize them.,LLMs make it harder for smaller organizations to utilize them.
Model compression is an effective solution but comes at the cost,Model compression is an effective solution but comes at the cost
"of degraded performance, especially at large scales greater than","of degraded performance, especially at large scales greater than"
6B.,6B.
These models exhibit very large magnitude outliers that do,These models exhibit very large magnitude outliers that do
"not exist in smaller models [255], making it challenging and re-","not exist in smaller models [255], making it challenging and re-"
"quiring specialized methods for quantizing LLMs [44, 256].","quiring specialized methods for quantizing LLMs [44, 256]."
Post-Training Quantization: Minimal or no training is re-,Post-Training Quantization: Minimal or no training is re-
"quired in this type of quantization, without significantly com-","quired in this type of quantization, without significantly com-"
promising the model performance.,promising the model performance.
LLM-8-bit [255] uses full-,LLM-8-bit [255] uses full-
precision matrix multiplication for weights associated with out-,precision matrix multiplication for weights associated with out-
lier features and 8-bit for remaining features.,lier features and 8-bit for remaining features.
The lower pre-,The lower pre-
cision multiplication outputs are converted to FP-16 and con-,cision multiplication outputs are converted to FP-16 and con-
catenated with others.,catenated with others.
The quantized models have homogenous,The quantized models have homogenous
"word embeddings, which may degrade their performance.","word embeddings, which may degrade their performance."
To,To
"fix this, token-level knowledge distillation is employed in [45]","fix this, token-level knowledge distillation is employed in [45]"
along with independent quantization scaling factors for each,along with independent quantization scaling factors for each
module due to varying weight distribution.,module due to varying weight distribution.
Feature distribu-,Feature distribu-
tions are asymmetric and appear in different channels; outlier,tions are asymmetric and appear in different channels; outlier
suppression [257] shifts and scales per-channel activation dis-,suppression [257] shifts and scales per-channel activation dis-
tributions for effective quantization.,tributions for effective quantization.
SmoothQuant [44] quan-,SmoothQuant [44] quan-
tizes activations and weights to INT8 format by smoothing,tizes activations and weights to INT8 format by smoothing
activations and migrating the quantization difficulty toward,activations and migrating the quantization difficulty toward
weights.,weights.
It multiplies the inverse of the smoothing factor with,It multiplies the inverse of the smoothing factor with
"weights, which introduces a few outliers in the weights but is","weights, which introduces a few outliers in the weights but is"
easier to quantify than unsmoothed activations.,easier to quantify than unsmoothed activations.
OPTQ [256],OPTQ [256]
uses the optimal brain compression (OBC) [258] algorithm to,uses the optimal brain compression (OBC) [258] algorithm to
quantize the model layer-by-layer and update weights to com-,quantize the model layer-by-layer and update weights to com-
To improve speed and per-,To improve speed and per-
"formance, OPTQ updates weights in arbitrary order, employs","formance, OPTQ updates weights in arbitrary order, employs"
"lazy updates, and uses better Cholesky kernels.","lazy updates, and uses better Cholesky kernels."
Outlier-aware,Outlier-aware
weight quantization (OWQ) [259] uses the OPTQ algorithm for,weight quantization (OWQ) [259] uses the OPTQ algorithm for
"quantization but assigns higher precision to vulnerable weights,","quantization but assigns higher precision to vulnerable weights,"
causing outliers and lower precision for others.,causing outliers and lower precision for others.
a quantized model is fine-tuned in,a quantized model is fine-tuned in
"quantization-aware training (QAT) [260, 261, 262].","quantization-aware training (QAT) [260, 261, 262]."
pha Tuning quantizes the model using binary coding quan-,pha Tuning quantizes the model using binary coding quan-
tization (BCQ) [263] and fine-tunes only quantization scal-,tization (BCQ) [263] and fine-tunes only quantization scal-
This approach improves performance over,This approach improves performance over
Page 22:,Page 22:
parameter-efficient fine-tuning of the pre-trained model.,parameter-efficient fine-tuning of the pre-trained model.
Sim-,Sim-
"ilarly, parameter-efficient and quantization-aware adaptation","ilarly, parameter-efficient and quantization-aware adaptation"
(PEQA) [264] reduces the precision of fully-connected layers,(PEQA) [264] reduces the precision of fully-connected layers
and fine-tunes only quantization scaling parameters.,and fine-tunes only quantization scaling parameters.
QAT [262] generates training data from the pre-trained network,QAT [262] generates training data from the pre-trained network
and trains a quantized student model with knowledge distilla-,and trains a quantized student model with knowledge distilla-
tion.,tion.
QLoRA [261] fine-tunes 4-bit quantized pre-trained LLM,QLoRA [261] fine-tunes 4-bit quantized pre-trained LLM
"with LoRA [250] using a 4-bit normal float, which shows better","with LoRA [250] using a 4-bit normal float, which shows better"
performance over a 4-bit integer and float.,performance over a 4-bit integer and float.
Pruning is an alternative approach to quantization to com-,Pruning is an alternative approach to quantization to com-
"press model size, thereby reducing LLMs deployment costs","press model size, thereby reducing LLMs deployment costs"
significantly.,significantly.
"Compared to task-agnostic pruning, task-specific","Compared to task-agnostic pruning, task-specific"
"pruning is easily achievable with good performance, where a","pruning is easily achievable with good performance, where a"
model is fine-tuned on the downstream task and pruned for,model is fine-tuned on the downstream task and pruned for
faster inference.,faster inference.
It is possible to prune LLMs for individual,It is possible to prune LLMs for individual
"tasks, but the cost of pruning and deploying task-specific mod-","tasks, but the cost of pruning and deploying task-specific mod-"
els is high.,els is high.
"To overcome this, many structured and unstructured","To overcome this, many structured and unstructured"
pruning methods for LLMs have been proposed to maintain rea-,pruning methods for LLMs have been proposed to maintain rea-
sonable performance across all tasks while shrinking the model,sonable performance across all tasks while shrinking the model
Unstructured Pruning: This kind of pruning removes less im-,Unstructured Pruning: This kind of pruning removes less im-
portant weights without maintaining any structure.,portant weights without maintaining any structure.
LLM pruning methods take advantage of the unique charac-,LLM pruning methods take advantage of the unique charac-
"teristics of LLMs, uncommon for smaller models, where a","teristics of LLMs, uncommon for smaller models, where a"
small subset of hidden states are activated with large magni-,small subset of hidden states are activated with large magni-
tude [255].,tude [255].
Pruning by weights and activations (Wanda) [265],Pruning by weights and activations (Wanda) [265]
"prunes weights in every row based on importance, calculated","prunes weights in every row based on importance, calculated"
by multiplying the weights with the norm of input.,by multiplying the weights with the norm of input.
The pruned,The pruned
"model does not require fine-tuning, thereby saving computa-","model does not require fine-tuning, thereby saving computa-"
tional costs.,tional costs.
Outlier weighed layerwise sparsity (OWL) [267],Outlier weighed layerwise sparsity (OWL) [267]
extends Wanda with non-uniform layer pruning.,extends Wanda with non-uniform layer pruning.
It shows that,It shows that
"the number of outliers varies for different layers; therefore, the","the number of outliers varies for different layers; therefore, the"
model should have variable pruning ratios for better perfor-,model should have variable pruning ratios for better perfor-
mance for every layer.,mance for every layer.
Contrastive pruning (CAP) [43] itera-,Contrastive pruning (CAP) [43] itera-
tively prunes the model by training the sparse model using con-,tively prunes the model by training the sparse model using con-
"trastive loss between pre-trained, fine-tuned, and snapshots of","trastive loss between pre-trained, fine-tuned, and snapshots of"
previous sparse models to learn task-specific and task-agnostic,previous sparse models to learn task-specific and task-agnostic
"Structured Pruning: Here, the parameters are removed in","Structured Pruning: Here, the parameters are removed in"
"groups, rows, columns, or matrices, which speeds up the","groups, rows, columns, or matrices, which speeds up the"
inference because of effective hardware tensor core utiliza-,inference because of effective hardware tensor core utiliza-
LLM-Pruner [42] employs a 3-stage structured,LLM-Pruner [42] employs a 3-stage structured
"pruning strategy, identifying the groups of hidden states caus-","pruning strategy, identifying the groups of hidden states caus-"
"ing each other to activate during the forward-pass, keeping im-","ing each other to activate during the forward-pass, keeping im-"
"portant groups and removing less important ones, and fine-","portant groups and removing less important ones, and fine-"
tuning the pruned model with LoRA.,tuning the pruned model with LoRA.
Sparsity-induced mask,Sparsity-induced mask
learning (SIMPLE) [268] prunes the network using learnable,learning (SIMPLE) [268] prunes the network using learnable
masks.,masks.
"Similarly, another method prunes LLMs by learning","Similarly, another method prunes LLMs by learning"
masks and removing unimportant rank-1 components of the,masks and removing unimportant rank-1 components of the
Inspired by the success of LLMs in natural language process-,Inspired by the success of LLMs in natural language process-
"ing applications, an increasing number of research works are","ing applications, an increasing number of research works are"
now facilitating LLMs to perceive different modalities of infor-,now facilitating LLMs to perceive different modalities of infor-
"mation like image [269, 270, 271], video [272, 273, 274], au-","mation like image [269, 270, 271], video [272, 273, 274], au-"
"dio [275, 274, 276], etc.","dio [275, 274, 276], etc."
Multimodal LLMs (MLLMs) present,Multimodal LLMs (MLLMs) present
substantial benefits compared to standard LLMs that process,substantial benefits compared to standard LLMs that process
only text.,only text.
By incorporating information from various modal-,By incorporating information from various modal-
"ities, MLLMs can achieve a deeper understanding of context,","ities, MLLMs can achieve a deeper understanding of context,"
leading to more intelligent responses infused with a variety of,leading to more intelligent responses infused with a variety of
expressions.,expressions.
"Importantly, MLLMs align closely with human","Importantly, MLLMs align closely with human"
"perceptual experiences, leveraging the synergistic nature of our","perceptual experiences, leveraging the synergistic nature of our"
multisensory inputs to form a comprehensive understanding of,multisensory inputs to form a comprehensive understanding of
"the world [276, 26].","the world [276, 26]."
"Coupled with a user-friendly interface,","Coupled with a user-friendly interface,"
"MLLMs can offer intuitive, flexible, and adaptable interactions,","MLLMs can offer intuitive, flexible, and adaptable interactions,"
allowing users to engage with intelligent assistants through a,allowing users to engage with intelligent assistants through a
spectrum of input methods.,spectrum of input methods.
According to the ways of construct-,According to the ways of construct-
"ing models, current MLLMs can be generally divided into three","ing models, current MLLMs can be generally divided into three"
"streams: pre-training, fine-tuning, and prompting.","streams: pre-training, fine-tuning, and prompting."
In this sec-,In this sec-
"tion, we will discuss more details of these main streams, as well","tion, we will discuss more details of these main streams, as well"
as the important application of MLLMs in visual reasoning.,as the important application of MLLMs in visual reasoning.
Pre-training: This stream of MLLMs intends to support differ-,Pre-training: This stream of MLLMs intends to support differ-
ent modalities using unified end-to-end models.,ent modalities using unified end-to-end models.
"For instance,","For instance,"
Flamingo [269] applies gated cross-attention to fuse vision and,Flamingo [269] applies gated cross-attention to fuse vision and
"language modalities, which are collected from pre-trained and","language modalities, which are collected from pre-trained and"
"frozen visual encoder and LLM, respectively.","frozen visual encoder and LLM, respectively."
"Moreover, BLIP-","Moreover, BLIP-"
2 [270] proposes a two-stage strategy to pre-train a Querying,2 [270] proposes a two-stage strategy to pre-train a Querying
Transformer (Q-Former) for the alignment between vision and,Transformer (Q-Former) for the alignment between vision and
"language modalities: in the first stage, vision-language repre-","language modalities: in the first stage, vision-language repre-"
sentation learning is bootstrapped from a frozen visual encoder;,sentation learning is bootstrapped from a frozen visual encoder;
"and in the second stage, a frozen LLM bootstraps vision-to-","and in the second stage, a frozen LLM bootstraps vision-to-"
language generative learning for zero-shot image-to-text gen-,language generative learning for zero-shot image-to-text gen-
eration.,eration.
"Similarly, MiniGPT-4 [277] deploys pre-trained and","Similarly, MiniGPT-4 [277] deploys pre-trained and"
"frozen ViT [278], Q-Former and Vicuna LLM [159], only train-","frozen ViT [278], Q-Former and Vicuna LLM [159], only train-"
ing the linear projection layer for vision and language modali-,ing the linear projection layer for vision and language modali-
Fine-tuning: Derived from instruction tuning [16] for NLP,Fine-tuning: Derived from instruction tuning [16] for NLP
"tasks [20, 16, 97], researchers are fine-tune pre-trained LLMs","tasks [20, 16, 97], researchers are fine-tune pre-trained LLMs"
using multimodal instructions.,using multimodal instructions.
"Following this method, LLMs","Following this method, LLMs"
can be easily and effectively extended as multimodal chat-,can be easily and effectively extended as multimodal chat-
"bots [277, 271, 29] and multimodal task solvers [279, 30, 280].","bots [277, 271, 29] and multimodal task solvers [279, 30, 280]."
The key issue of this stream of MLLMs is to collect multi-,The key issue of this stream of MLLMs is to collect multi-
modal instruction-following data for fine-tuning [58].,modal instruction-following data for fine-tuning [58].
To ad-,To ad-
"dress this issue, the solutions of benchmark adaptation [279,","dress this issue, the solutions of benchmark adaptation [279,"
"281, 282], self-instruction [19, 31, 283], and hybrid composi-","281, 282], self-instruction [19, 31, 283], and hybrid composi-"
"tion [284, 280] are employed, respectively.","tion [284, 280] are employed, respectively."
To mitigate the gap,To mitigate the gap
between the original language modality and additional modal-,between the original language modality and additional modal-
"ities, the learnable interface is introduced to connect differ-","ities, the learnable interface is introduced to connect differ-"
ent modalities from frozen pre-trained models.,ent modalities from frozen pre-trained models.
the learnable interface is expected to work in a parameter-,the learnable interface is expected to work in a parameter-
"efficient tuning manner: e.g., LLaMA-Adapter [285] applies","efficient tuning manner: e.g., LLaMA-Adapter [285] applies"
"an efficient transformer-based adapter module for training,","an efficient transformer-based adapter module for training,"
and LaVIN [284] dynamically learns the multimodal feature,and LaVIN [284] dynamically learns the multimodal feature
weights using a mixture-of-modality adapter.,weights using a mixture-of-modality adapter.
Different from,Different from
"the learnable interface, the expert models can directly convert","the learnable interface, the expert models can directly convert"
"multimodalities into language: e.g., VideoChat-Text [272] in-","multimodalities into language: e.g., VideoChat-Text [272] in-"
"corporates Whisper [286], a speech recognition expert model,","corporates Whisper [286], a speech recognition expert model,"
to generate the captions of given videos for the understanding,to generate the captions of given videos for the understanding
Different from the fine-tuning technique that,Different from the fine-tuning technique that
Page 23:,Page 23:
directly updates the model parameters given task-specific,directly updates the model parameters given task-specific
"datasets, the prompting technique provides certain context, ex-","datasets, the prompting technique provides certain context, ex-"
"amples, or instructions to the model, fulfilling specialized tasks","amples, or instructions to the model, fulfilling specialized tasks"
without changing the model parameters.,without changing the model parameters.
Since prompting can,Since prompting can
"significantly reduce the need for large-scale multimodal data,","significantly reduce the need for large-scale multimodal data,"
this technique is widely used to construct MLLMs.,this technique is widely used to construct MLLMs.
"Particularly,","Particularly,"
"to solve multimodal Chain of Thought (CoT) problems [103],","to solve multimodal Chain of Thought (CoT) problems [103],"
LLMs are prompted to generate both the reasoning process and,LLMs are prompted to generate both the reasoning process and
the answer given multimodal inputs [287].,the answer given multimodal inputs [287].
"On this front, differ-","On this front, differ-"
"ent learning paradigms are exploited in practice: for example,","ent learning paradigms are exploited in practice: for example,"
Multimodal-CoT [287] involves two stages of rationale genera-,Multimodal-CoT [287] involves two stages of rationale genera-
"tion and answer inference, where the input of the second stage","tion and answer inference, where the input of the second stage"
is a combination of the original input and the output of the first,is a combination of the original input and the output of the first
stage; and CoT-PT [288] applies both prompt tuning and spe-,stage; and CoT-PT [288] applies both prompt tuning and spe-
cific visual bias to generate a chain of reasoning implicitly.,cific visual bias to generate a chain of reasoning implicitly.
In,In
"addition to CoT problems, LLMs can also be prompted with","addition to CoT problems, LLMs can also be prompted with"
"multimodal descriptions and tools, effectively dividing complex","multimodal descriptions and tools, effectively dividing complex"
"tasks into sub-tasks [289, 290].","tasks into sub-tasks [289, 290]."
Visual Reasoning Application: Recent visual reasoning sys-,Visual Reasoning Application: Recent visual reasoning sys-
"tems [291, 292, 216, 293] tend to apply LLMs for better visual","tems [291, 292, 216, 293] tend to apply LLMs for better visual"
information analysis and visual-language integration.,information analysis and visual-language integration.
Differ-,Differ-
"ent from previous works [294, 295] that rely on limited VQA","ent from previous works [294, 295] that rely on limited VQA"
"datasets and small-scale neural networks, current LLM-aided","datasets and small-scale neural networks, current LLM-aided"
"methods offer benefits of stronger generalization ability, emer-","methods offer benefits of stronger generalization ability, emer-"
"gent ability, and interactivity [58].","gent ability, and interactivity [58]."
To realize visual reasoning,To realize visual reasoning
"with the help of LLMs, prompting and fine-tuning techniques","with the help of LLMs, prompting and fine-tuning techniques"
"can also be utilized: for example, PointClip V2 [292] applies","can also be utilized: for example, PointClip V2 [292] applies"
"LLMs to generate 3D-specific prompts, which are encoded as","LLMs to generate 3D-specific prompts, which are encoded as"
textual features and then combined with visual features for,textual features and then combined with visual features for
3D recognition; and GPT4Tools [31] employs LoRA [250] to,3D recognition; and GPT4Tools [31] employs LoRA [250] to
fine-tune LLMs following tool-related instructions.,fine-tune LLMs following tool-related instructions.
"as a controller [293], decision maker [296], or semantics re-","as a controller [293], decision maker [296], or semantics re-"
"finer [291, 297], LLMs significantly facilitates the progress of","finer [291, 297], LLMs significantly facilitates the progress of"
"Due to the gigantic scale of LLMs, minor changes in archi-","Due to the gigantic scale of LLMs, minor changes in archi-"
tecture and training strategies have a big impact on performance,tecture and training strategies have a big impact on performance
and stability.,and stability.
"Here, we summarize key architectural modules","Here, we summarize key architectural modules"
"used in various LLMs, leading to better performance, reduced","used in various LLMs, leading to better performance, reduced"
"training time and memory, and better training stability.","training time and memory, and better training stability."
Layer Normalization: The performance and training stability,Layer Normalization: The performance and training stability
of LLMs are affected significantly by layer normalization.,of LLMs are affected significantly by layer normalization.
Pre-,Pre-
"norm, that is normalizing inputs rather than outputs, is more","norm, that is normalizing inputs rather than outputs, is more"
"common among LLMs stabilizing the training [6, 127, 108].","common among LLMs stabilizing the training [6, 127, 108]."
BLOOM [13] and AlexaTM [122] utilize an additional layer,BLOOM [13] and AlexaTM [122] utilize an additional layer
normalization before embedding layer to stabilize the training,normalization before embedding layer to stabilize the training
"of large-scale models, while the model’s zero-shot generaliza-","of large-scale models, while the model’s zero-shot generaliza-"
tion ability can be negatively impacted [13].,tion ability can be negatively impacted [13].
"However, another","However, another"
study [33] finds that pre-norm degrades fine-tuned model per-,study [33] finds that pre-norm degrades fine-tuned model per-
"formance as compared to post-norm, and there are no stability","formance as compared to post-norm, and there are no stability"
benefits of pre-norm beyond the 100B scale.,benefits of pre-norm beyond the 100B scale.
"Therefore, GLM-","Therefore, GLM-"
130B [33] used deep-norm which is a variant of post-norm for,130B [33] used deep-norm which is a variant of post-norm for
better downstream task performance after fine-tuning.,better downstream task performance after fine-tuning.
"Positional Encoding: Like other building blocks of the model,","Positional Encoding: Like other building blocks of the model,"
positional encoding also affects the performance and training,positional encoding also affects the performance and training
BLOOM [13] finds ALiBi outperforms,BLOOM [13] finds ALiBi outperforms
learned and rotary positional encodings.,learned and rotary positional encodings.
GLM-130B [33] identifies rotary positional encoding as being,GLM-130B [33] identifies rotary positional encoding as being
better than ALiBi.,better than ALiBi.
"So, there is no conclusion in the literature","So, there is no conclusion in the literature"
"Parallel Attention: In this type of attention, feed-forward and","Parallel Attention: In this type of attention, feed-forward and"
attention layers are parallel to each other rather than sequen-,attention layers are parallel to each other rather than sequen-
tial in a transformer block.,tial in a transformer block.
It has been shown to reduce train-,It has been shown to reduce train-
ing time by 15%.,ing time by 15%.
There is no evidence of performance drop,There is no evidence of performance drop
due to this change in the literature and it is used by the models,due to this change in the literature and it is used by the models
"PaLM [15], GPT-NeoX [118], and CodeGen [140].","PaLM [15], GPT-NeoX [118], and CodeGen [140]."
Multi-Query Attention It has shared key and value attention,Multi-Query Attention It has shared key and value attention
heads in a transformer block while query attention heads are,heads in a transformer block while query attention heads are
projected as usual.,projected as usual.
This reduces memory usage and speeds up,This reduces memory usage and speeds up
sampling in autoregressive decoding.,sampling in autoregressive decoding.
No performance degrada-,No performance degrada-
tion has been observed with this change and it makes the train-,tion has been observed with this change and it makes the train-
ing efficient allowing larger batch sizes.,ing efficient allowing larger batch sizes.
Multi-query attention,Multi-query attention
"is used in [15, 142].","is used in [15, 142]."
Mixture of Experts: This type of architecture enables eas-,Mixture of Experts: This type of architecture enables eas-
"ily scaling models to trillions of parameters [92, 91].","ily scaling models to trillions of parameters [92, 91]."
Only a,Only a
few experts are activated during the computation making them,few experts are activated during the computation making them
compute-efficient.,compute-efficient.
The performance of MoE models is better,The performance of MoE models is better
than dense models for the same amount of data and requires less,than dense models for the same amount of data and requires less
computation during fine-tuning to achieve performance similar,computation during fine-tuning to achieve performance similar
to dense models as discussed in [91].,to dense models as discussed in [91].
MoE architectures are,MoE architectures are
"less prone to catastrophic forgetting, therefore are more suited","less prone to catastrophic forgetting, therefore are more suited"
for continual learning [92].,for continual learning [92].
Extracting smaller sub-models for,Extracting smaller sub-models for
"downstream tasks is possible without losing any performance,","downstream tasks is possible without losing any performance,"
making MoE architecture hardware-friendly [92].,making MoE architecture hardware-friendly [92].
Sparse vs Dense Activated: GPT-3 [6] uses sparse transform-,Sparse vs Dense Activated: GPT-3 [6] uses sparse transform-
ers [67] whereas GLaM [91] and PanGu-P [92] use MoE [121],ers [67] whereas GLaM [91] and PanGu-P [92] use MoE [121]
architectures to lower computational costs and increase the,architectures to lower computational costs and increase the
model size and capacity.,model size and capacity.
"According to the literature, sparse","According to the literature, sparse"
modules do not degrade the model’s performance [67].,modules do not degrade the model’s performance [67].
How-,How-
"ever, more experiments are required to verify this statement.","ever, more experiments are required to verify this statement."
Training models at a huge scale require tricks to reduce train-,Training models at a huge scale require tricks to reduce train-
"ing costs, avoid loss divergence, and achieve better perfor-","ing costs, avoid loss divergence, and achieve better perfor-"
mance.,mance.
We summarize and discuss some of these key tricks,We summarize and discuss some of these key tricks
Mixed Precision: It is a famous method for LLMs to reduce,Mixed Precision: It is a famous method for LLMs to reduce
memory usage and improve training efficiency.,memory usage and improve training efficiency.
In mixed pre-,In mixed pre-
"cision, forward and backward passes are performed in FP16","cision, forward and backward passes are performed in FP16"
format whereas optimizer states and master weights are kept,format whereas optimizer states and master weights are kept
in FP32 format [120].,in FP32 format [120].
A drawback associated with this for-,A drawback associated with this for-
mat change is training instability due to a smaller value range,mat change is training instability due to a smaller value range
resulting in loss spikes [33].,resulting in loss spikes [33].
An alternative to FP16 is BF16,An alternative to FP16 is BF16
which has a comparatively larger range and performs precision-,which has a comparatively larger range and performs precision-
sensitive operations like gradient accumulation and softmax in,sensitive operations like gradient accumulation and softmax in
FP32 [13].,FP32 [13].
BF16 has better performance and training stability,BF16 has better performance and training stability
"but uses more memory and is supported on specific hardware,","but uses more memory and is supported on specific hardware,"
"for example, A100 GPUs.","for example, A100 GPUs."
"Therefore, its adoption in LLMs is","Therefore, its adoption in LLMs is"
Training Instability: Loss divergence or spiking is a common,Training Instability: Loss divergence or spiking is a common
issue in LLMs that occurs multiple times during training.,issue in LLMs that occurs multiple times during training.
This,This
Page 24:,Page 24:
happens in the presence of gradient clipping [15].,happens in the presence of gradient clipping [15].
To mitigate,To mitigate
"this problem, many approaches suggest restarting training from","this problem, many approaches suggest restarting training from"
"an earlier checkpoint [15, 33, 91], skipping 200-500 earlier","an earlier checkpoint [15, 33, 91], skipping 200-500 earlier"
data batches at the point of divergence in [15] and re-shuffling,data batches at the point of divergence in [15] and re-shuffling
batches in [91].,batches in [91].
The embedding layer gradient shrink proves to,The embedding layer gradient shrink proves to
further stabilize the training as its gradient norm is significantly,further stabilize the training as its gradient norm is significantly
larger than the other layers [33].,larger than the other layers [33].
Another suggestion to improve,Another suggestion to improve
training stability for larger models is not to use biases in dense,training stability for larger models is not to use biases in dense
and norm layers as in [15].,and norm layers as in [15].
Weight Initialization: It plays a significant role in model con-,Weight Initialization: It plays a significant role in model con-
feed-forward layers before residuals with,feed-forward layers before residuals with
d as in [153] and,d as in [153] and
other layers with the small initialization scheme [298].,other layers with the small initialization scheme [298].
This,This
avoids activations growing exponentially with increasing depth.,avoids activations growing exponentially with increasing depth.
MT-NLG [117] found higher variance for weight initialization,MT-NLG [117] found higher variance for weight initialization
"leads to unstable training, hence validating small initialization","leads to unstable training, hence validating small initialization"
scheme [298].,scheme [298].
Various models perform random weight initial-,Various models perform random weight initial-
"ization which can cause bad initialization, Galactica [148] sug-","ization which can cause bad initialization, Galactica [148] sug-"
gests a longer warmup to negate the effect.,gests a longer warmup to negate the effect.
Learning Rate: A suitable learning rate is important for sta-,Learning Rate: A suitable learning rate is important for sta-
ble training.,ble training.
"It is suggested to use a lower value [13, 15, 124]","It is suggested to use a lower value [13, 15, 124]"
with warmup and decay (cosine or linear).,with warmup and decay (cosine or linear).
"Usually, the learn-","Usually, the learn-"
ing rate is within the range 1e−4 to 8e−4.,ing rate is within the range 1e−4 to 8e−4.
"Moreover, MT-NLG","Moreover, MT-NLG"
(530B) [117] and GPT-NeoX (20B) [118] suggest interpolat-,(530B) [117] and GPT-NeoX (20B) [118] suggest interpolat-
ing learning rates based on the model size using the GPT-3 [6],ing learning rates based on the model size using the GPT-3 [6]
models ranging between 13B and 175B.,models ranging between 13B and 175B.
This avoids tuning the,This avoids tuning the
"Training Parallelism: 3D parallelism, a combination of data,","Training Parallelism: 3D parallelism, a combination of data,"
"pipeline, and tensor parallelism, is the most utilized training","pipeline, and tensor parallelism, is the most utilized training"
"parallelism approach in LLMs [33, 15, 14, 13, 117, 115, 112].","parallelism approach in LLMs [33, 15, 14, 13, 117, 115, 112]."
"In addition to 3D parallelism, BLOOM [13] uses a zero op-","In addition to 3D parallelism, BLOOM [13] uses a zero op-"
timizer [37] to shard optimizer states.,timizer [37] to shard optimizer states.
PanGu-Σ [92] go beyond 3D parallelism and apply 5D paral-,PanGu-Σ [92] go beyond 3D parallelism and apply 5D paral-
lelism which additionally contains optimizer parallelism and,lelism which additionally contains optimizer parallelism and
Mode Switching: It adds task-related tokens at the beginning,Mode Switching: It adds task-related tokens at the beginning
of the text during training.,of the text during training.
These tokens refer to the natural,These tokens refer to the natural
language understanding and natural language generation tasks,language understanding and natural language generation tasks
which are shown to improve downstream task performance,which are shown to improve downstream task performance
"in [125, 124, 122].","in [125, 124, 122]."
"During fine-tuning and inference, tokens","During fine-tuning and inference, tokens"
are appended based on the downstream tasks.,are appended based on the downstream tasks.
Controllable Text Generation: Generating credible and con-,Controllable Text Generation: Generating credible and con-
trolled text from a pre-trained model is challenging.,trolled text from a pre-trained model is challenging.
GPT-3 [6],GPT-3 [6]
and other LLMs use in-context learning to control generated,and other LLMs use in-context learning to control generated
text.,text.
While in-context learning helps in controlling the gener-,While in-context learning helps in controlling the gener-
"ated text, ERNIE 3.0 Titan [35] suggests using adversarial loss","ated text, ERNIE 3.0 Titan [35] suggests using adversarial loss"
to rank its generated text for credibility and soft prompts such as,to rank its generated text for credibility and soft prompts such as
"genre, topic, keywords, sentiment, and length for better control","genre, topic, keywords, sentiment, and length for better control"
3.8.3.,3.8.3.
Supervised Models vs Generalized Models,Supervised Models vs Generalized Models
Although generalized models are capable of performing di-,Although generalized models are capable of performing di-
verse tasks with good performance they have not yet outper-,verse tasks with good performance they have not yet outper-
formed models trained in supervised settings.,formed models trained in supervised settings.
The supervised,The supervised
trained models are still state-of-the-art in various NLP tasks by,trained models are still state-of-the-art in various NLP tasks by
"a large margin as shown in [6, 15, 18].","a large margin as shown in [6, 15, 18]."
LLMs perform well in zero-shot and few-shot settings.,LLMs perform well in zero-shot and few-shot settings.
But,But
the performance difference between zero-shot and few-shot is,the performance difference between zero-shot and few-shot is
"large for pre-trained models [6, 15], naming LLMs as meta-","large for pre-trained models [6, 15], naming LLMs as meta-"
learners [6].,learners [6].
LLMs zero-shot evaluations underperform unsu-,LLMs zero-shot evaluations underperform unsu-
pervised methods in neural machine translation [6].,pervised methods in neural machine translation [6].
The liter-,The liter-
ature shows pre-training is not enough for good zero-shot per-,ature shows pre-training is not enough for good zero-shot per-
"formance [15, 16].","formance [15, 16]."
To improve the zero-shot performance the,To improve the zero-shot performance the
literature suggests using instruction fine-tuning that improves,literature suggests using instruction fine-tuning that improves
the zero-shot performance significantly and outperforms base-,the zero-shot performance significantly and outperforms base-
lines.,lines.
Instruction fine-tuning has also been shown to improve,Instruction fine-tuning has also been shown to improve
zero-shot generalization to unseen tasks.,zero-shot generalization to unseen tasks.
"Another model, Flan-","Another model, Flan-"
"PaLM [16], unlocks zero-shot reasoning with CoT training.","PaLM [16], unlocks zero-shot reasoning with CoT training."
3.8.5.,3.8.5.
Encoder vs Decoder vs Encoder-Decoder,Encoder vs Decoder vs Encoder-Decoder
"Traditionally, these architectures perform well for different","Traditionally, these architectures perform well for different"
"tasks, for example, encoder-only for NLU tasks, decoder-only","tasks, for example, encoder-only for NLU tasks, decoder-only"
"for NLG, and encoder-decoder for sequence2sequence model-","for NLG, and encoder-decoder for sequence2sequence model-"
ing.,ing.
Encoder-only models are famous for smaller models such,Encoder-only models are famous for smaller models such
"as Bert [7], RoBERTa [299], etc., whereas LLMs are either","as Bert [7], RoBERTa [299], etc., whereas LLMs are either"
"decoder-only [6, 118, 13] or encoder-decoder [10, 11, 122].","decoder-only [6, 118, 13] or encoder-decoder [10, 11, 122]."
"While decoder-only models are good at NLG tasks, various","While decoder-only models are good at NLG tasks, various"
"LLMs, PaLM [15], OPT [14], GPT-3 [6], BLOOM [13],","LLMs, PaLM [15], OPT [14], GPT-3 [6], BLOOM [13],"
"LLaMA [156], are decoder-only models with significant per-","LLaMA [156], are decoder-only models with significant per-"
formance gains on both NLU and NLG tasks.,formance gains on both NLU and NLG tasks.
In contradic-,In contradic-
"tion to this, T5 [10] and UL2 [125] identify encoder-decoder","tion to this, T5 [10] and UL2 [125] identify encoder-decoder"
models out-performing decoder-only models.,models out-performing decoder-only models.
"In another study,","In another study,"
PaLM [15] finds increasing the size of decoder-only models,PaLM [15] finds increasing the size of decoder-only models
can reduce the performance gap between decoder-only and,can reduce the performance gap between decoder-only and
Although decoder-only architectures have become a trend for,Although decoder-only architectures have become a trend for
"LLMs, many recently proposed approaches [125, 122] use","LLMs, many recently proposed approaches [125, 122] use"
mode-switching tokens in text with encoder-decoder architec-,mode-switching tokens in text with encoder-decoder architec-
tures to enable task-specific modes.,tures to enable task-specific modes.
"Similarly, CodeT5+ [34]","Similarly, CodeT5+ [34]"
uses an encoder-decoder architecture with multiple training ob-,uses an encoder-decoder architecture with multiple training ob-
"jectives for different tasks, activating the encoder, decoder, or","jectives for different tasks, activating the encoder, decoder, or"
both according to the tasks.,both according to the tasks.
These variations in architecture,These variations in architecture
and training objectives allow a model to perform well in differ-,and training objectives allow a model to perform well in differ-
ent settings.,ent settings.
"Because of this dynamic configuration, the future","Because of this dynamic configuration, the future"
of LLMs can be attributed to encoder-decoder architectures.,of LLMs can be attributed to encoder-decoder architectures.
We provide different statistics of pre-trained and instruction-,We provide different statistics of pre-trained and instruction-
tuned models in this section.,tuned models in this section.
This includes information such as,This includes information such as
"publication venue, license type, model creators, steps trained,","publication venue, license type, model creators, steps trained,"
"parallelism, etc in Table 3 and Table 4.","parallelism, etc in Table 3 and Table 4."
Architecture details,Architecture details
of pre-trained LLMs are available in Table 5.,of pre-trained LLMs are available in Table 5.
Providing these,Providing these
details for instruction-tuned models is unnecessary because it,details for instruction-tuned models is unnecessary because it
fine-tunes pre-trained models for instruction datasets.,fine-tunes pre-trained models for instruction datasets.
"Hence,","Hence,"
architectural details are the same as the baselines.,architectural details are the same as the baselines.
"Moreover,","Moreover,"
optimization settings for various LLMs are available in Table 6,optimization settings for various LLMs are available in Table 6
and Table 7.,and Table 7.
"We do not include details on precision, warmup,","We do not include details on precision, warmup,"
and weight decay in Table 7.,and weight decay in Table 7.
These details are not as important,These details are not as important
"as others to mention for instruction-tuned models, and are not","as others to mention for instruction-tuned models, and are not"
Page 25:,Page 25:
Table 3: Summary of pre-trained LLMs (>10B).,Table 3: Summary of pre-trained LLMs (>10B).
Only the LLMs discussed individually in the previous sections are summarized.,Only the LLMs discussed individually in the previous sections are summarized.
“Data/Tokens” is the model’s,“Data/Tokens” is the model’s
"pre-training data, which is either the number of tokens or data size.","pre-training data, which is either the number of tokens or data size."
“Data Cleaning” indicates whether data cleaning is performed or not.,“Data Cleaning” indicates whether data cleaning is performed or not.
This includes heuristics,This includes heuristics
"(Heur), deduplication (Dedup), quality filtering (QF), and privacy filtering (PF), “Cost” is the calculated training cost obtained by multiplying the GPUs/TPUs","(Heur), deduplication (Dedup), quality filtering (QF), and privacy filtering (PF), “Cost” is the calculated training cost obtained by multiplying the GPUs/TPUs"
hourly rate with the number of GPUs and the training time.,hourly rate with the number of GPUs and the training time.
"The actual cost may vary due to many reasons such as using in-house GPUs or getting a discounted rate,","The actual cost may vary due to many reasons such as using in-house GPUs or getting a discounted rate,"
"re-training, number of employees working on the problem, etc.","re-training, number of employees working on the problem, etc."
"“Training Parallelism” indicates distributed training using data parallelism (D), tensor parallelism","“Training Parallelism” indicates distributed training using data parallelism (D), tensor parallelism"
"(T), pipeline parallelism (P), context parallelism (C), model parallelism (M), optimizer parallelism (OP), and rematerialization (R), where for “Library” column,","(T), pipeline parallelism (P), context parallelism (C), model parallelism (M), optimizer parallelism (OP), and rematerialization (R), where for “Library” column,"
“DS” is a short form for Deep Speed.,“DS” is a short form for Deep Speed.
"In column “Commercial Use”, we assumed a model is for non-commercial purposes if its license is unavailable.","In column “Commercial Use”, we assumed a model is for non-commercial purposes if its license is unavailable."
Table 4: Summary of instruction tuned LLMs (>10B).,Table 4: Summary of instruction tuned LLMs (>10B).
All abbreviations are the same as Table 3.,All abbreviations are the same as Table 3.
Entries in “Data/Tokens” starting with “S-” represent the number,Entries in “Data/Tokens” starting with “S-” represent the number
Generating training and evaluation datasets is expensive be-,Generating training and evaluation datasets is expensive be-
cause of the large-scale data demand of LLMs.,cause of the large-scale data demand of LLMs.
"Hence, datasets","Hence, datasets"
for training and benchmarking these models are topics of key,for training and benchmarking these models are topics of key
importance.,importance.
A summary of datasets commonly used by LLMs,A summary of datasets commonly used by LLMs
The performance of LLMs largely depends on the training,The performance of LLMs largely depends on the training
"data’s quality, size, and diversity.","data’s quality, size, and diversity."
Preparing training datasets,Preparing training datasets
of high quality at a large scale is laborious.,of high quality at a large scale is laborious.
Researchers have,Researchers have
suggested various pre-training and fine-tuning datasets to en-,suggested various pre-training and fine-tuning datasets to en-
hance LLMs capabilities.,hance LLMs capabilities.
We summarize these efforts in Ta-,We summarize these efforts in Ta-
ble 8.,ble 8.
While numerous training datasets are available in the,While numerous training datasets are available in the
"literature, we cover the most widely used ones in our summary.","literature, we cover the most widely used ones in our summary."
Page 26:,Page 26:
Table 5: Architecture details of LLMs.,Table 5: Architecture details of LLMs.
"Here, “PE” is the positional embedding, “nL” is the number of layers, “nH” is the number of attention heads, “HS” is the","Here, “PE” is the positional embedding, “nL” is the number of layers, “nH” is the number of attention heads, “HS” is the"
5.2.,5.2.
Evaluation Datasets and Tasks,Evaluation Datasets and Tasks
The evaluation of LLMs is important in gauging their profi-,The evaluation of LLMs is important in gauging their profi-
ciency and limitations.,ciency and limitations.
This process measures the model’s abil-,This process measures the model’s abil-
"ity to comprehend, generate, and interact with human language","ity to comprehend, generate, and interact with human language"
across a spectrum of tasks.,across a spectrum of tasks.
Evaluating a language model (LM),Evaluating a language model (LM)
is divided into two broader categories: 1) natural language un-,is divided into two broader categories: 1) natural language un-
derstanding (NLU) and 2) natural language generation (NLG).,derstanding (NLU) and 2) natural language generation (NLG).
It is emphasized that tasks in NLU and NLG are softly catego-,It is emphasized that tasks in NLU and NLG are softly catego-
rized and are often used interchangeably in the literature.,rized and are often used interchangeably in the literature.
Natural Language Understanding: It measures the language,Natural Language Understanding: It measures the language
understanding capacity of LMs.,understanding capacity of LMs.
"It encompasses multiple tasks,","It encompasses multiple tasks,"
"including sentiment analysis, text classification, natural lan-","including sentiment analysis, text classification, natural lan-"
"guage inference (NLI), question answering (QA), common-","guage inference (NLI), question answering (QA), common-"
"sense reasoning (CR), mathematical reasoning (MR), reading","sense reasoning (CR), mathematical reasoning (MR), reading"
Natural Language Generation: It assesses the language gener-,Natural Language Generation: It assesses the language gener-
ation capabilities of LLMs by understanding the provided input,ation capabilities of LLMs by understanding the provided input
context.,context.
"It includes tasks such as summarization, sentence com-","It includes tasks such as summarization, sentence com-"
"pletion, machine translation (MT), dialogue generation, etc.","pletion, machine translation (MT), dialogue generation, etc."
"Numerous datasets are proposed for each task, evaluating","Numerous datasets are proposed for each task, evaluating"
LLMs against different characteristics.,LLMs against different characteristics.
To provide an overview,To provide an overview
"of evaluation datasets, we briefly discuss a few famous datasets","of evaluation datasets, we briefly discuss a few famous datasets"
within each category and offer a comprehensive list of datasets,within each category and offer a comprehensive list of datasets
in Table 9.,in Table 9.
"Moreover, we show a detailed overview of the train-","Moreover, we show a detailed overview of the train-"
ing datasets and evaluation tasks and benchmarks used by vari-,ing datasets and evaluation tasks and benchmarks used by vari-
ous pre-trained LLMs in Table 10 and fine-tuned LLMs in Ta-,ous pre-trained LLMs in Table 10 and fine-tuned LLMs in Ta-
ble 11.,ble 11.
We also compare the top-performing LLMs in various,We also compare the top-performing LLMs in various
NLP tasks in Table 12.,NLP tasks in Table 12.
MMLU [307]: A benchmark that measures the knowledge,MMLU [307]: A benchmark that measures the knowledge
acquired by models during pretraining and evaluates models in,acquired by models during pretraining and evaluates models in
"zero-shot and few-shot settings across 57 subjects, testing both","zero-shot and few-shot settings across 57 subjects, testing both"
world knowledge and problem-solving ability.,world knowledge and problem-solving ability.
SuperGLUE [2]: A more challenging and diverse successor,SuperGLUE [2]: A more challenging and diverse successor
"to the GLUE [309] benchmark, SuperGLUE includes a variety","to the GLUE [309] benchmark, SuperGLUE includes a variety"
"of language understanding tasks, such as question answering,","of language understanding tasks, such as question answering,"
"natural language inference, and co-reference resolution.","natural language inference, and co-reference resolution."
It is,It is
designed to provide a rigorous test of language understanding,designed to provide a rigorous test of language understanding
"and requires significant progress in areas like sample-efficient,","and requires significant progress in areas like sample-efficient,"
"transfer, multi-task, and unsupervised or self-supervised learn-","transfer, multi-task, and unsupervised or self-supervised learn-"
BIG-bench [308]: The BIG-bench (Behavior of Intelligent,BIG-bench [308]: The BIG-bench (Behavior of Intelligent
Generative Models Benchmark) is a large-scale benchmark de-,Generative Models Benchmark) is a large-scale benchmark de-
signed to test the abilities of LLMs across a wide range of,signed to test the abilities of LLMs across a wide range of
"tasks, including reasoning, creativity, ethics, and understanding","tasks, including reasoning, creativity, ethics, and understanding"
GLUE [309]: The General Language Understanding Evalua-,GLUE [309]: The General Language Understanding Evalua-
tion (GLUE) benchmark is a collection of resources for train-,tion (GLUE) benchmark is a collection of resources for train-
"ing, evaluating, and analyzing natural language understanding","ing, evaluating, and analyzing natural language understanding"
Page 27:,Page 27:
Table 6: Summary of optimization settings used for pre-trained LLMs.,Table 6: Summary of optimization settings used for pre-trained LLMs.
"The values for weight decay, gradient clipping, and dropout are 0.1, 1.0, and 0.1, respectively,","The values for weight decay, gradient clipping, and dropout are 0.1, 1.0, and 0.1, respectively,"
for most of the LLMs.,for most of the LLMs.
AdaFactorAdam AdamWFP16 BF16 Mixed Decay,AdaFactorAdam AdamWFP16 BF16 Mixed Decay
Table 7: Summary of optimization settings used for instruction-tuned LLMs.,Table 7: Summary of optimization settings used for instruction-tuned LLMs.
"Values for gradient clipping and dropout are the same as the pre-trained models, while","Values for gradient clipping and dropout are the same as the pre-trained models, while"
no model uses weight decay for instruction tuning.,no model uses weight decay for instruction tuning.
systems.,systems.
It includes a variety of tasks that test a wide range of,It includes a variety of tasks that test a wide range of
"linguistic phenomena, making it a comprehensive tool for eval-","linguistic phenomena, making it a comprehensive tool for eval-"
uating language understanding in AI.,uating language understanding in AI.
WinoGrande [354]: A large-scale dataset inspired by the orig-,WinoGrande [354]: A large-scale dataset inspired by the orig-
inal Winograd [357] Schema Challenge tests models on their,inal Winograd [357] Schema Challenge tests models on their
ability to resolve pronoun ambiguity and encourages the devel-,ability to resolve pronoun ambiguity and encourages the devel-
opment of models that understand the broad context in natural,opment of models that understand the broad context in natural
"CoQA [316]: A conversational question-answering dataset,","CoQA [316]: A conversational question-answering dataset,"
CoQA challenges models with questions that rely on conver-,CoQA challenges models with questions that rely on conver-
sation history and require free-form text answers.,sation history and require free-form text answers.
Its diverse,Its diverse
content from seven domains makes it a rigorous test for mod-,content from seven domains makes it a rigorous test for mod-
els’ ability to handle a wide range of topics and conversational,els’ ability to handle a wide range of topics and conversational
WiC [317]: This dataset assesses a model’s ability to dis-,WiC [317]: This dataset assesses a model’s ability to dis-
"cern word meanings based on context, aiding in tasks related","cern word meanings based on context, aiding in tasks related"
Page 28:,Page 28:
Table 8: Details of various well-known pre-training and fine-tuning datasets.,Table 8: Details of various well-known pre-training and fine-tuning datasets.
"Here, alignment means aligning with human preferences.","Here, alignment means aligning with human preferences."
"A clean, multilingual dataset with billions","A clean, multilingual dataset with billions"
A multilingual extension of the C4,A multilingual extension of the C4
"dataset, mC4 identifies over 100 lan-","dataset, mC4 identifies over 100 lan-"
guages using cld3 from 71 monthly web,guages using cld3 from 71 monthly web
A massive dataset comprised of 22 con-,A massive dataset comprised of 22 con-
46 natural and 13 programming lan-,46 natural and 13 programming lan-
99% of the data is in English,99% of the data is in English
Open-source replica of LLaMA dataset,Open-source replica of LLaMA dataset
Submissions and comments on Reddit,Submissions and comments on Reddit
Pool of Prompt (P3) [17],Pool of Prompt (P3) [17]
"A Subset of PromptSource, created from","A Subset of PromptSource, created from"
Extending P3 to total 46 languages,Extending P3 to total 46 languages
Extending P3 with additional multi-,Extending P3 with additional multi-
"lingual datasets, total 46 languages","lingual datasets, total 46 languages"
Generated 52k instructions with 82k sam-,Generated 52k instructions with 82k sam-
ples from 175 seed tasks using GPT-3,ples from 175 seed tasks using GPT-3
Employed self-instruct method to gener-,Employed self-instruct method to gener-
Recreated Alpaca dataset with GPT-4 in,Recreated Alpaca dataset with GPT-4 in
Carefully created samples to test perfor-,Carefully created samples to test perfor-
mance with fine-tuning on less data,mance with fine-tuning on less data
With over 100 million tokens from,With over 100 million tokens from
"Wikipedia’s top articles, this dataset is a rich resource for tasks","Wikipedia’s top articles, this dataset is a rich resource for tasks"
"that require understanding long-term dependencies, such as lan-","that require understanding long-term dependencies, such as lan-"
PG19 [319]: This is a digital library of diverse books from,PG19 [319]: This is a digital library of diverse books from
Project Gutenberg.,Project Gutenberg.
It is specifically designed to facilitate re-,It is specifically designed to facilitate re-
"search in unsupervised learning and language modeling, with a","search in unsupervised learning and language modeling, with a"
special focus on long-form content.,special focus on long-form content.
"C4 [10]: A clean, multilingual dataset, C4 offers billions of to-","C4 [10]: A clean, multilingual dataset, C4 offers billions of to-"
kens from web-crawled data.,kens from web-crawled data.
It is a comprehensive resource for,It is a comprehensive resource for
training advanced Transformer models on various languages.,training advanced Transformer models on various languages.
LCQMC [320]: The Large-scale Chinese Question Matching,LCQMC [320]: The Large-scale Chinese Question Matching
Corpus (LCQMC) is a dataset for evaluating the performance,Corpus (LCQMC) is a dataset for evaluating the performance
of models in semantic matching tasks.,of models in semantic matching tasks.
It contains pairs of ques-,It contains pairs of ques-
"tions in Chinese and their matching status, making it a valuable","tions in Chinese and their matching status, making it a valuable"
resource for research in Chinese language understanding.,resource for research in Chinese language understanding.
5.2.3.,5.2.3.
Story Cloze and Sentence Completion,Story Cloze and Sentence Completion
"StoryCloze [334]: It introduces a new “StoryCloze Test”, a","StoryCloze [334]: It introduces a new “StoryCloze Test”, a"
commonsense reasoning framework for evaluating story under-,commonsense reasoning framework for evaluating story under-
"standing, generation, and script learning.","standing, generation, and script learning."
It considers a model’s,It considers a model’s
ability to understand and generate coherent and sensible stories.,ability to understand and generate coherent and sensible stories.
LAMBADA [335]: This dataset evaluates contextual text un-,LAMBADA [335]: This dataset evaluates contextual text un-
derstanding through a word prediction task.,derstanding through a word prediction task.
Models must pre-,Models must pre-
"dict the last word of a passage, which is easy for humans when","dict the last word of a passage, which is easy for humans when"
"given the whole passage, but not when given only the last sen-","given the whole passage, but not when given only the last sen-"
5.2.4.,5.2.4.
Physical Knowledge and World Understanding,Physical Knowledge and World Understanding
PIQA [340]: A dataset that probes the physical knowledge of,PIQA [340]: A dataset that probes the physical knowledge of
"models, aiming to understand how well they are learning about","models, aiming to understand how well they are learning about"
TriviaQA [341]: A dataset that tests models on reading com-,TriviaQA [341]: A dataset that tests models on reading com-
"prehension and open domain question answering (QA) tasks,","prehension and open domain question answering (QA) tasks,"
with a focus on Information Retrieval (IR)-style QA.,with a focus on Information Retrieval (IR)-style QA.
"ARC [342]: A larger version of the ARC-Challenge, this","ARC [342]: A larger version of the ARC-Challenge, this"
"dataset contains both easy and challenging grade-school level,","dataset contains both easy and challenging grade-school level,"
multiple-choice science questions.,multiple-choice science questions.
It is a comprehensive test of,It is a comprehensive test of
a model’s ability to understand and answer complex questions.,a model’s ability to understand and answer complex questions.
"A subset of the ARC dataset, ARC-","A subset of the ARC dataset, ARC-"
"Easy, contains questions that are answered correctly by either","Easy, contains questions that are answered correctly by either"
a retrieval-based algorithm or a word co-occurrence algorithm.,a retrieval-based algorithm or a word co-occurrence algorithm.
Page 29:,Page 29:
Table 9: Categorized evaluation datasets used in evaluating LLMs.,Table 9: Categorized evaluation datasets used in evaluating LLMs.
"MMLU [307], SuperGLUE [2], BIG-bench [308], GLUE [309], BBH [308], CUGE [310], Zero-","MMLU [307], SuperGLUE [2], BIG-bench [308], GLUE [309], BBH [308], CUGE [310], Zero-"
"CLUE [311], FewCLUE [312], Blended Skill Talk [313], HELM [314], KLUE-STS [315]","CLUE [311], FewCLUE [312], Blended Skill Talk [313], HELM [314], KLUE-STS [315]"
"CoQA [316], WiC [317], Wikitext103 [318], PG19 [319], LCQMC [320], QQP [321], WinoGender [322],","CoQA [316], WiC [317], Wikitext103 [318], PG19 [319], LCQMC [320], QQP [321], WinoGender [322],"
"CB [323], FinRE [324], SanWen [325], AFQMC [311], BQ Corpus [326], CNSS [327], CKBQA 13 [328],","CB [323], FinRE [324], SanWen [325], AFQMC [311], BQ Corpus [326], CNSS [327], CKBQA 13 [328],"
"CLUENER [311], Weibo [329], AQuA [330], OntoNotes [331], HeadQA [332], Twitter Dataset [333]","CLUENER [311], Weibo [329], AQuA [330], OntoNotes [331], HeadQA [332], Twitter Dataset [333]"
"StoryCloze [334], LAMBADA [335], LCSTS [336], AdGen [337], E2E [338], CHID [339], CHID-","StoryCloze [334], LAMBADA [335], LCSTS [336], AdGen [337], E2E [338], CHID [339], CHID-"
"PIQA [340], TriviaQA [341], ARC [342], ARC-Easy [342], ARC-Challenge [342], PROST [343], Open-","PIQA [340], TriviaQA [341], ARC [342], ARC-Easy [342], ARC-Challenge [342], PROST [343], Open-"
"BookQA [344], WebNLG [345], DogWhistle Insider & Outsider [346]","BookQA [344], WebNLG [345], DogWhistle Insider & Outsider [346]"
"RACE [347], RACE-Middle [347], RACE-High [347], QuAC [348], StrategyQA [349], Quiz Bowl [350],","RACE [347], RACE-Middle [347], RACE-High [347], QuAC [348], StrategyQA [349], Quiz Bowl [350],"
"cMedQA [351],cMedQA2 [352], MATINF-QA [353]","cMedQA [351],cMedQA2 [352], MATINF-QA [353]"
"WinoGrande [354], HellaSwag [355], COPA [356], WSC [357], CSQA [358], SIQA [359], C3 [360],","WinoGrande [354], HellaSwag [355], COPA [356], WSC [357], CSQA [358], SIQA [359], C3 [360],"
"CLUEWSC2020 [311], CLUEWSC [311], CLUEWSC-FC [312], ReCoRD [361]","CLUEWSC2020 [311], CLUEWSC [311], CLUEWSC-FC [312], ReCoRD [361]"
"SQuAD [362], BoolQ [363], SQUADv2 [364], DROP [365], RTE [366], WebQA [367], CMRC2017 [368],","SQuAD [362], BoolQ [363], SQUADv2 [364], DROP [365], RTE [366], WebQA [367], CMRC2017 [368],"
"CMRC2018 [369], CMRC2019 [370], COTE-BD [371], COTE-DP [371], COTE-MFW [371], Mul-","CMRC2018 [369], CMRC2019 [370], COTE-BD [371], COTE-DP [371], COTE-MFW [371], Mul-"
"tiRC [372], Natural Questions [373], CNSE [327], DRCD [374], DuReader [375], Dureaderrobust [376],","tiRC [372], Natural Questions [373], CNSE [327], DRCD [374], DuReader [375], Dureaderrobust [376],"
"DuReader-QG [375], SciQ [377], Sogou-log [378], Dureaderrobust-QG [376], QA4MRE [379], KorQuAD","DuReader-QG [375], SciQ [377], Sogou-log [378], Dureaderrobust-QG [376], QA4MRE [379], KorQuAD"
"1.0 [380], CAIL2018-Task1 & Task2 [381]","1.0 [380], CAIL2018-Task1 & Task2 [381]"
"MATH [382], Math23k [383], GSM8K [384], MathQA [385], MGSM [386], MultiArith [387], AS-","MATH [382], Math23k [383], GSM8K [384], MathQA [385], MGSM [386], MultiArith [387], AS-"
"Div [388], MAWPS [389], SVAMP [390]","Div [388], MAWPS [389], SVAMP [390]"
"HumanEval [141], DS-1000 [391], MBPP [392], APPS [382], CodeContests [142]","HumanEval [141], DS-1000 [391], MBPP [392], APPS [382], CodeContests [142]"
"ANLI [393], MNLI-m [394], MNLI-mm [394],QNLI [362], WNLI [357], OCNLI [311], CMNLI [311],","ANLI [393], MNLI-m [394], MNLI-mm [394],QNLI [362], WNLI [357], OCNLI [311], CMNLI [311],"
"ANLI R1 [393], ANLI R2 [393], ANLI R3 [393], HANS [395], OCNLI-FC [312], LogiQA [396], Strate-","ANLI R1 [393], ANLI R2 [393], ANLI R3 [393], HANS [395], OCNLI-FC [312], LogiQA [396], Strate-"
"MLQA [397], XNLI [398], PAWS-X [399], XSum [400], XCOPA [401], XWinograd [402], TyDiQA-","MLQA [397], XNLI [398], PAWS-X [399], XSum [400], XCOPA [401], XWinograd [402], TyDiQA-"
"TruthfulQA [405], MultiFC [406], Fact Checking on Fever [407]","TruthfulQA [405], MultiFC [406], Fact Checking on Fever [407]"
Biases and Ethics in AI,Biases and Ethics in AI
"ETHOS [408], StereoSet [409], BBQ [410], Winobias [411], CrowS-Pairs [412]","ETHOS [408], StereoSet [409], BBQ [410], Winobias [411], CrowS-Pairs [412]"
"RealToxicityPrompts [413], CivilComments toxicity classification [414]","RealToxicityPrompts [413], CivilComments toxicity classification [414]"
"WMT [415], WMT20 [416], WMT20-enzh [416], EPRSTMT [312], CCPM [417]","WMT [415], WMT20 [416], WMT20-enzh [416], EPRSTMT [312], CCPM [417]"
"AminoProbe [148], BioLAMA [148], Chemical Reactions [148], Galaxy Clusters [148], Mineral","AminoProbe [148], BioLAMA [148], Chemical Reactions [148], Galaxy Clusters [148], Mineral"
"Wizard of Wikipedia [418], Empathetic Dialogues [419], DPC-generated [96] dialogues, ConvAI2 [420],","Wizard of Wikipedia [418], Empathetic Dialogues [419], DPC-generated [96] dialogues, ConvAI2 [420],"
"TNEWS-FC [312], YNAT [315], KLUE-TC [315], CSL [311], CSL-FC [312], IFLYTEK [422]","TNEWS-FC [312], YNAT [315], KLUE-TC [315], CSL [311], CSL-FC [312], IFLYTEK [422]"
It is a great starting point for models beginning to explore ad-,It is a great starting point for models beginning to explore ad-
"dataset, ARC-Challenge includes complex, grade-school level","dataset, ARC-Challenge includes complex, grade-school level"
"questions that demand reasoning beyond simple retrieval, test-","questions that demand reasoning beyond simple retrieval, test-"
ing the true comprehension capabilities of models.,ing the true comprehension capabilities of models.
RACE [347]: The RACE dataset is a reading comprehension,RACE [347]: The RACE dataset is a reading comprehension
"dataset collected from English examinations in China, which","dataset collected from English examinations in China, which"
benchmarks AI models for understanding and answering ques-,benchmarks AI models for understanding and answering ques-
"tions on long and complex passages, simulating the challenge","tions on long and complex passages, simulating the challenge"
RACE-Middle [347]: Another subset of the RACE [347],RACE-Middle [347]: Another subset of the RACE [347]
"dataset, RACE-Middle, contains middle school-level English","dataset, RACE-Middle, contains middle school-level English"
exam questions.,exam questions.
It offers a slightly less challenging but academ-,It offers a slightly less challenging but academ-
ically oriented evaluation of a model’s comprehension skills.,ically oriented evaluation of a model’s comprehension skills.
"RACE-High [347]: A subset of the RACE [347] dataset,","RACE-High [347]: A subset of the RACE [347] dataset,"
RACE-High consists of high school-level English exam ques-,RACE-High consists of high school-level English exam ques-
tions.,tions.
It is designed to evaluate the comprehension ability of,It is designed to evaluate the comprehension ability of
models in a more academic and challenging context.,models in a more academic and challenging context.
QuAC [348]: This dataset simulates an information-seeking,QuAC [348]: This dataset simulates an information-seeking
dialog between students and teachers using hidden Wikipedia,dialog between students and teachers using hidden Wikipedia
text.,text.
It introduces unique challenges not found in machine com-,It introduces unique challenges not found in machine com-
"prehension datasets, making it a valuable resource for advanc-","prehension datasets, making it a valuable resource for advanc-"
HellaSwag [355]: A dataset that challenges models to pick the,HellaSwag [355]: A dataset that challenges models to pick the
best ending to a context uses Adversarial Filtering to create a,best ending to a context uses Adversarial Filtering to create a
"‘Goldilocks’ zone of complexity, where generated text is absurd","‘Goldilocks’ zone of complexity, where generated text is absurd"
to humans but often misclassified by models.,to humans but often misclassified by models.
COPA [401]: This dataset evaluates a model’s progress in,COPA [401]: This dataset evaluates a model’s progress in
open-domain commonsense causal reasoning.,open-domain commonsense causal reasoning.
Each question,Each question
"comprises a premise and two alternatives, and the model must","comprises a premise and two alternatives, and the model must"
"select the more plausible alternative, testing a model’s ability to","select the more plausible alternative, testing a model’s ability to"
understand and reason about cause and effect.,understand and reason about cause and effect.
WSC [357]: The Winograd Schema Challenge (WSC) is a,WSC [357]: The Winograd Schema Challenge (WSC) is a
Page 30:,Page 30:
Table 10: An illustration of training datasets and evaluation tasks employed by pre-trained LLMs.,Table 10: An illustration of training datasets and evaluation tasks employed by pre-trained LLMs.
"Here, “QA” is question-answering, “Clf” is classification, “NLI”","Here, “QA” is question-answering, “Clf” is classification, “NLI”"
"is natural language inference, “MT” is machine translation, “RC” is reading comprehension, “CR” is commonsense reasoning, “MR” is mathematical reasoning,","is natural language inference, “MT” is machine translation, “RC” is reading comprehension, “CR” is commonsense reasoning, “MR” is mathematical reasoning,"
"Common Crawl, WebText, Books Cor-","Common Crawl, WebText, Books Cor-"
54 million public repositories from Github,54 million public repositories from Github
"Chinese text corpora, Baidu Search, Web","Chinese text corpora, Baidu Search, Web"
"text, QA-long, QA-short, Poetry and Cou-","text, QA-long, QA-short, Poetry and Cou-"
"plet Domain-specific data from medical,","plet Domain-specific data from medical,"
"law, and financial area Baidu knowledge","law, and financial area Baidu knowledge"
graph with more than 50 million facts,graph with more than 50 million facts
"Wikipedia, OWT, Books, C4, Pile [301],","Wikipedia, OWT, Books, C4, Pile [301],"
"Korean blogs, Community sites, News,","Korean blogs, Community sites, News,"
"KiN Korean Wikipedia, Wikipedia (En-","KiN Korean Wikipedia, Wikipedia (En-"
"glish and Japanese), Modu-Corpus: Mes-","glish and Japanese), Modu-Corpus: Mes-"
"senger, News, Spoken and written lan-","senger, News, Spoken and written lan-"
"Common Crawl, SogouT, Sogou News,","Common Crawl, SogouT, Sogou News,"
"subsets of MassiveWeb Books, C4, News,","subsets of MassiveWeb Books, C4, News,"
GitHub and Wikipedia samples from Mas-,GitHub and Wikipedia samples from Mas-
Same as ERNIE 3.0 and ERNIE 3.0 ad-,Same as ERNIE 3.0 and ERNIE 3.0 ad-
"versarial dataset, ERNIE 3.0 controllable","versarial dataset, ERNIE 3.0 controllable"
"RoBERTa [299], Pile [301], PushShift.io","RoBERTa [299], Pile [301], PushShift.io"
"arXiv, PMC, Semantic Scholar, Wikipedia,","arXiv, PMC, Semantic Scholar, Wikipedia,"
"books, RefSeq Genome, OEIS, LIPID","books, RefSeq Genome, OEIS, LIPID"
"itories Khan Problems, GSM8K, OneS-","itories Khan Problems, GSM8K, OneS-"
"Filtered Webpages, Social media conversa-","Filtered Webpages, Social media conversa-"
"tions Wikipedia, Forums, Books, News","tions Wikipedia, Forums, Books, News"
"Infiniset : Public documents, Dialogs, Ut-","Infiniset : Public documents, Dialogs, Ut-"
Two snapshots of Common Crawl and,Two snapshots of Common Crawl and
"[242], BookCorpus2, NIH ExPorter, Pile,","[242], BookCorpus2, NIH ExPorter, Pile,"
"webpages, books, Wikipedia, news, arti-","webpages, books, Wikipedia, news, arti-"
"cles, source code, social media conversa-","cles, source code, social media conversa-"
"WuDaoCorpora, CLUE, Pile, C4, Python","WuDaoCorpora, CLUE, Pile, C4, Python"
"Web documents, Code, Books, Maths,","Web documents, Code, Books, Maths,"
Page 31:,Page 31:
Table 11: An illustration of training datasets and evaluation benchmarks used in fine-tuned LLMs.,Table 11: An illustration of training datasets and evaluation benchmarks used in fine-tuned LLMs.
“SNI” is a short of Super-NaturalInsturctions.,“SNI” is a short of Super-NaturalInsturctions.
reading comprehension task in which a system must resolve,reading comprehension task in which a system must resolve
"references in a text, often requiring world knowledge and rea-","references in a text, often requiring world knowledge and rea-"
CSQA [358]: The CommonsenseQA is a question-answering,CSQA [358]: The CommonsenseQA is a question-answering
dataset that requires commonsense knowledge to evaluate the,dataset that requires commonsense knowledge to evaluate the
ability of AI models to understand and answer questions.,ability of AI models to understand and answer questions.
"BoolQ [363]: A dataset derived from Google search queries,","BoolQ [363]: A dataset derived from Google search queries,"
BoolQ challenges models to answer binary (yes/no) questions.,BoolQ challenges models to answer binary (yes/no) questions.
The questions are naturally occurring and are paired with a,The questions are naturally occurring and are paired with a
paragraph from a Wikipedia article containing the answer.,paragraph from a Wikipedia article containing the answer.
It,It
is a test of reading comprehension and reasoning.,is a test of reading comprehension and reasoning.
SQUADv2 [364]: The Stanford Question Answering Dataset,SQUADv2 [364]: The Stanford Question Answering Dataset
(SQuAD) [362] is a collection of questions posed by crowd,(SQuAD) [362] is a collection of questions posed by crowd
"workers on a set of Wikipedia articles, where the answer to ev-","workers on a set of Wikipedia articles, where the answer to ev-"
ery question is a segment of text from the corresponding reading,ery question is a segment of text from the corresponding reading
passage.,passage.
SQuADv2 combines the original SQuAD1.1 dataset,SQuADv2 combines the original SQuAD1.1 dataset
"with over 50,000 unanswerable questions.","with over 50,000 unanswerable questions."
The aim is to evalu-,The aim is to evalu-
ate a model’s ability to understand and answer questions based,ate a model’s ability to understand and answer questions based
on a given context and to determine when a question is unan-,on a given context and to determine when a question is unan-
"DROP [365]: DROP, or Discrete Reasoning Over the con-","DROP [365]: DROP, or Discrete Reasoning Over the con-"
"tent of Paragraphs, is designed to test a model’s ability to un-","tent of Paragraphs, is designed to test a model’s ability to un-"
derstand a wide variety of reading phenomena.,derstand a wide variety of reading phenomena.
It encourages,It encourages
comprehensive and reliable evaluation of reading comprehen-,comprehensive and reliable evaluation of reading comprehen-
The Recognizing Textual Entailment (RTE),The Recognizing Textual Entailment (RTE)
datasets come from a series of annual competitions on textual,datasets come from a series of annual competitions on textual
"entailment, predicting whether a given sentence logically fol-","entailment, predicting whether a given sentence logically fol-"
lows from another and evaluating a model’s understanding of,lows from another and evaluating a model’s understanding of
logical relationships in a text.,logical relationships in a text.
"WebQA [367]: A dataset for open-domain question answering,","WebQA [367]: A dataset for open-domain question answering,"
WebQA offers a large collection of web-based question-answer,WebQA offers a large collection of web-based question-answer
pairs.,pairs.
It is designed to assess the ability of AI models to under-,It is designed to assess the ability of AI models to under-
stand and answer questions based on web content.,stand and answer questions based on web content.
CMRC2018 [369]: This dataset is a test of Chinese language,CMRC2018 [369]: This dataset is a test of Chinese language
models’ ability to reason comprehensively and is designed with,models’ ability to reason comprehensively and is designed with
a challenging span-extraction format that pushes the boundaries,a challenging span-extraction format that pushes the boundaries
MATH [382]: This dataset is a platform for evaluating the,MATH [382]: This dataset is a platform for evaluating the
mathematical problem-solving abilities of AI models.,mathematical problem-solving abilities of AI models.
It con-,It con-
"tains a diverse set of math problems, ranging from arithmetic","tains a diverse set of math problems, ranging from arithmetic"
"to calculus, and is designed to test the model’s ability to under-","to calculus, and is designed to test the model’s ability to under-"
stand and solve complex mathematical problems.,stand and solve complex mathematical problems.
Math23k [383]: This one challenges a model’s ability to un-,Math23k [383]: This one challenges a model’s ability to un-
derstand and solve mathematical word problems.,derstand and solve mathematical word problems.
It contains,It contains
"23,000 Chinese arithmetic word problems that require models","23,000 Chinese arithmetic word problems that require models"
to perform reasoning and computation based on the problem,to perform reasoning and computation based on the problem
GSM8K [384]: A dataset of diverse grade school math word,GSM8K [384]: A dataset of diverse grade school math word
"problems, testing a model’s ability to perform multi-step math-","problems, testing a model’s ability to perform multi-step math-"
5.2.9.,5.2.9.
Problem Solving and Logical Reasoning,Problem Solving and Logical Reasoning
ANLI [393]: A large-scale dataset designed to test the robust-,ANLI [393]: A large-scale dataset designed to test the robust-
ness of machine learning models in Natural Language Inference,ness of machine learning models in Natural Language Inference
"(NLI) is created through an iterative, adversarial process where","(NLI) is created through an iterative, adversarial process where"
humans try to generate examples that models cannot correctly,humans try to generate examples that models cannot correctly
HumanEval [141]: A dataset for evaluating the problem-,HumanEval [141]: A dataset for evaluating the problem-
"solving ability of AI models, which includes a diverse set of","solving ability of AI models, which includes a diverse set of"
"tasks that require various cognitive abilities, making it a com-","tasks that require various cognitive abilities, making it a com-"
prehensive tool for assessing general intelligence in AI.,prehensive tool for assessing general intelligence in AI.
StrategyQA [349]: A question-answering dataset that re-,StrategyQA [349]: A question-answering dataset that re-
quires reasoning over multiple pieces of evidence to evaluate,quires reasoning over multiple pieces of evidence to evaluate
"the strategic reasoning ability of AI models, pushing the bound-","the strategic reasoning ability of AI models, pushing the bound-"
aries of what machines can understand and answer.,aries of what machines can understand and answer.
"XNLI [398]: A cross-lingual benchmark, XNLI extends the","XNLI [398]: A cross-lingual benchmark, XNLI extends the"
"MultiNLI [429] corpus to 15 languages, including low-resource","MultiNLI [429] corpus to 15 languages, including low-resource"
ones like Urdu.,ones like Urdu.
It tests models on cross-lingual sentence under-,It tests models on cross-lingual sentence under-
"standing, with 112,500 annotated pairs across three categories:","standing, with 112,500 annotated pairs across three categories:"
"PAWS-X [399]: PAWS-X, or Cross-lingual Paraphrase Adver-","PAWS-X [399]: PAWS-X, or Cross-lingual Paraphrase Adver-"
"saries from Word Scrambling, is a multilingual version of the","saries from Word Scrambling, is a multilingual version of the"
Page 32:,Page 32:
PAWS [430] dataset for paraphrase identification.,PAWS [430] dataset for paraphrase identification.
It includes,It includes
examples in seven languages and is designed to evaluate the,examples in seven languages and is designed to evaluate the
performance of cross-lingual paraphrase identification models.,performance of cross-lingual paraphrase identification models.
Truthful-QA [405]: A unique benchmark that measures a,Truthful-QA [405]: A unique benchmark that measures a
language model’s truthfulness when generating answers.,language model’s truthfulness when generating answers.
The,The
"dataset includes questions across various categories like health,","dataset includes questions across various categories like health,"
"law, and politics, some designed to test the model against com-","law, and politics, some designed to test the model against com-"
5.2.12.,5.2.12.
Biases and Ethics in AI,Biases and Ethics in AI
ETHOS [408]: ETHOS is a hate speech detection dataset,ETHOS [408]: ETHOS is a hate speech detection dataset
built from YouTube and Reddit comments.,built from YouTube and Reddit comments.
It is a tool in the,It is a tool in the
"fight against online hate speech, offering binary and multi-label","fight against online hate speech, offering binary and multi-label"
variants for robust content moderation.,variants for robust content moderation.
StereoSet [409]: StereoSet is a comprehensive dataset de-,StereoSet [409]: StereoSet is a comprehensive dataset de-
signed to measure and evaluate the presence of stereotypical,signed to measure and evaluate the presence of stereotypical
biases in language models.,biases in language models.
It focuses on four key domains:,It focuses on four key domains:
"gender, profession, race, and religion.","gender, profession, race, and religion."
Contrasting stereotypi-,Contrasting stereotypi-
cal bias against language modeling ability provides a valuable,cal bias against language modeling ability provides a valuable
tool for understanding and mitigating biases in large language,tool for understanding and mitigating biases in large language
Applying Large Language Models (LLMs) to a variety of,Applying Large Language Models (LLMs) to a variety of
downstream tasks has become a popular trend in both AI-,downstream tasks has become a popular trend in both AI-
"related research communities and industries, with many emerg-","related research communities and industries, with many emerg-"
ing uses being discovered and explored daily.,ing uses being discovered and explored daily.
"LLMs, which are","LLMs, which are"
"capable of understanding and generating human-like text, have","capable of understanding and generating human-like text, have"
found meaningful applications across a variety of fields.,found meaningful applications across a variety of fields.
This,This
"section provides an overview of LLM applications in medicine,","section provides an overview of LLM applications in medicine,"
"education, science, mathematics, law, finance, robotics, and","education, science, mathematics, law, finance, robotics, and"
coding.,coding.
"While each of these domains pose different challenges,","While each of these domains pose different challenges,"
LLMs open up opportunities to make significant contributions,LLMs open up opportunities to make significant contributions
to these domains through their generalizability.,to these domains through their generalizability.
LLMs are being widely considered as,LLMs are being widely considered as
general-purpose tools for a wide variety of tasks [431].,general-purpose tools for a wide variety of tasks [431].
This,This
"is due to their inherent ability to understand, generate, and","is due to their inherent ability to understand, generate, and"
manipulate human-like text in a contextually relevant man-,manipulate human-like text in a contextually relevant man-
ner.,ner.
This allows them to perform tasks ranging from simple,This allows them to perform tasks ranging from simple
language translation and question-answering to more complex,language translation and question-answering to more complex
"tasks like summarization, text generation, and even program-","tasks like summarization, text generation, and even program-"
ming help [432].,ming help [432].
The utility of LLMs is further enhanced by,The utility of LLMs is further enhanced by
their ability to adapt to the specific style and tone of the text,their ability to adapt to the specific style and tone of the text
"they are processing, making the outputs more user-friendly and","they are processing, making the outputs more user-friendly and"
context-aware.,context-aware.
"In everyday applications, LLMs can be used","In everyday applications, LLMs can be used"
"as personal assistants, helping users draft emails or schedule","as personal assistants, helping users draft emails or schedule"
appointments [433]; they can also be deployed in customer ser-,appointments [433]; they can also be deployed in customer ser-
vice to handle common questions or applied to generate content,vice to handle common questions or applied to generate content
for digital platforms like websites by creating human-like text,for digital platforms like websites by creating human-like text
based on given prompts [434].,based on given prompts [434].
"Moreover, LLMs play a cru-","Moreover, LLMs play a cru-"
"cial role in data analysis, where they can filter large volumes of","cial role in data analysis, where they can filter large volumes of"
"text data, summarize key points, and find patterns that would","text data, summarize key points, and find patterns that would"
take humans much longer to identify [435].,take humans much longer to identify [435].
Despite their wide-,Despite their wide-
"ranging applications, it is essential to remember that LLMs,","ranging applications, it is essential to remember that LLMs,"
"similar to any AI system, are only as good as the data they have","similar to any AI system, are only as good as the data they have"
Medicine: The application of LLMs in the field of medicine is,Medicine: The application of LLMs in the field of medicine is
reshaping healthcare delivery and research.,reshaping healthcare delivery and research.
"For example, LLMs","For example, LLMs"
are increasingly used in clinical decision support systems to,are increasingly used in clinical decision support systems to
provide physicians with evidence-based treatment recommen-,provide physicians with evidence-based treatment recommen-
"dations [436, 437, 438].","dations [436, 437, 438]."
By analyzing patient data and medical,By analyzing patient data and medical
"literature, they can help identify potential diagnoses, suggest","literature, they can help identify potential diagnoses, suggest"
"appropriate tests, and recommend optimal treatment strategies.","appropriate tests, and recommend optimal treatment strategies."
"Moreover, LLMs can also enhance patient interactions with","Moreover, LLMs can also enhance patient interactions with"
"healthcare systems; e.g., they can be used in chatbot applica-","healthcare systems; e.g., they can be used in chatbot applica-"
"tions [439, 440, 441] to answer patient queries about symptoms","tions [439, 440, 441] to answer patient queries about symptoms"
"or medications, schedule appointments, and even provide es-","or medications, schedule appointments, and even provide es-"
sential health advice.,sential health advice.
"For medical research, LLMs are used to","For medical research, LLMs are used to"
extract and filter information from a considerable amount of,extract and filter information from a considerable amount of
"medical literature, identify relevant studies, summarize find-","medical literature, identify relevant studies, summarize find-"
"ings, and even predict future research trends [442, 443, 444].","ings, and even predict future research trends [442, 443, 444]."
"For medical education, LLMs can help create training mate-","For medical education, LLMs can help create training mate-"
"rials, generate exam questions, provide detailed explanations","rials, generate exam questions, provide detailed explanations"
"of complex medical topics, and offer personalized feedback to","of complex medical topics, and offer personalized feedback to"
"students [445, 446, 447, 448].","students [445, 446, 447, 448]."
They can also simulate patient,They can also simulate patient
"interactions, enabling students to practice and improve their","interactions, enabling students to practice and improve their"
clinical skills.,clinical skills.
"At a broader level, LLMs can assist in public","At a broader level, LLMs can assist in public"
health initiatives by analyzing media data to detect disease out-,health initiatives by analyzing media data to detect disease out-
"breaks, monitor public sentiment towards health policies, and","breaks, monitor public sentiment towards health policies, and"
disseminate health information in a clear and understandable,disseminate health information in a clear and understandable
manner [449].,manner [449].
LLMs can be employed to support public health,LLMs can be employed to support public health
"initiatives, addressing related issues such as data privacy, the","initiatives, addressing related issues such as data privacy, the"
"necessity for explainability, and the potential risk of propagat-","necessity for explainability, and the potential risk of propagat-"
Education: The integration of LLMs into the educational sec-,Education: The integration of LLMs into the educational sec-
"tor offers opportunities to enhance learning experiences, teacher","tor offers opportunities to enhance learning experiences, teacher"
"support, and educational content development.","support, and educational content development."
"For students, by","For students, by"
"analyzing their learning styles, performance, and preferences,","analyzing their learning styles, performance, and preferences,"
LLMs can provide customized study materials and practice,LLMs can provide customized study materials and practice
questions to develop personalized learning experiences [452].,questions to develop personalized learning experiences [452].
"For teachers, LLMs can help to create lesson plans and grade","For teachers, LLMs can help to create lesson plans and grade"
assignments and generate diverse and inclusive educational,assignments and generate diverse and inclusive educational
"content, significantly saving more time for teaching and student","content, significantly saving more time for teaching and student"
"interaction [453, 454].","interaction [453, 454]."
"In language learning, LLMs serve as","In language learning, LLMs serve as"
advanced conversational partners capable of simulating conver-,advanced conversational partners capable of simulating conver-
"sations in multiple languages, correcting grammar, enhancing","sations in multiple languages, correcting grammar, enhancing"
"vocabulary, and aiding pronunciation for the needs of fluency","vocabulary, and aiding pronunciation for the needs of fluency"
in practice [455].,in practice [455].
"Furthermore, LLMs improve accessibility","Furthermore, LLMs improve accessibility"
in education by providing support for students with disabili-,in education by providing support for students with disabili-
ties.,ties.
They can generate real-time transcriptions for the hear-,They can generate real-time transcriptions for the hear-
"ing impaired, offer reading assistance for the visually impaired,","ing impaired, offer reading assistance for the visually impaired,"
and simplify complex texts for those with learning disabili-,and simplify complex texts for those with learning disabili-
ties [451].,ties [451].
"As LLMs continue to evolve, their applications in","As LLMs continue to evolve, their applications in"
education can benefit more students and teachers from different,education can benefit more students and teachers from different
"Science: Similar to medical applications, LLMs can expedite","Science: Similar to medical applications, LLMs can expedite"
the research process by quickly analyzing and summarizing sci-,the research process by quickly analyzing and summarizing sci-
entific literature.,entific literature.
By briefing comprehensible and accessible re-,By briefing comprehensible and accessible re-
"search summaries, LLMs can assist researchers in staying up-","search summaries, LLMs can assist researchers in staying up-"
"to-date with the latest findings, even in fields outside their area","to-date with the latest findings, even in fields outside their area"
"of expertise [456, 457].","of expertise [456, 457]."
"In addition, LLMs can aid scientists","In addition, LLMs can aid scientists"
Page 33:,Page 33:
Table 12: Performance comparison of top performing LLMs across various NLU and NLG tasks.,Table 12: Performance comparison of top performing LLMs across various NLU and NLG tasks.
"Here, “N-Shots” indicate the number of example prompts provided","Here, “N-Shots” indicate the number of example prompts provided"
"to the model during the evaluation, representing its capability in few-shot or zero-shot learning settings, “f” represents the fine-tuned version, and “B” represents the","to the model during the evaluation, representing its capability in few-shot or zero-shot learning settings, “f” represents the fine-tuned version, and “B” represents the"
in formulating new hypotheses and research questions since,in formulating new hypotheses and research questions since
their ability to process large-scale datasets allows them to un-,their ability to process large-scale datasets allows them to un-
veil insights that might not be immediately apparent to human,veil insights that might not be immediately apparent to human
researchers [458].,researchers [458].
"Moreover, for scientific writing, LLMs can","Moreover, for scientific writing, LLMs can"
"help researchers draft documents, suggest improvements, and","help researchers draft documents, suggest improvements, and"
"ensure adherence to specific formatting guidelines [459, 460].","ensure adherence to specific formatting guidelines [459, 460]."
This not only saves time but also improves the clarity of scien-,This not only saves time but also improves the clarity of scien-
"tific communication, enabling interdisciplinary teams to work","tific communication, enabling interdisciplinary teams to work"
Maths: In addition to providing mathematical research and,Maths: In addition to providing mathematical research and
"education support, LLMs can assist in solving mathematical","education support, LLMs can assist in solving mathematical"
problems by giving step-by-step explanations and guiding users,problems by giving step-by-step explanations and guiding users
through complex proofs and calculations.,through complex proofs and calculations.
They can help iden-,They can help iden-
"tify errors in reasoning or computation and suggest corrections,","tify errors in reasoning or computation and suggest corrections,"
serving as an invaluable tool for both learning and verification,serving as an invaluable tool for both learning and verification
"purposes [461, 462].","purposes [461, 462]."
LLMs can be employed to check the valid-,LLMs can be employed to check the valid-
"ity of mathematical proofs, offering a preliminary filter before","ity of mathematical proofs, offering a preliminary filter before"
human review.,human review.
While they are not a substitute for the meticu-,While they are not a substitute for the meticu-
"lous work of mathematicians, they can help simplify the process","lous work of mathematicians, they can help simplify the process"
"of proof verification [463, 464].","of proof verification [463, 464]."
"Moreover, LLMs enhance ac-","Moreover, LLMs enhance ac-"
cessibility to mathematics by translating complex concepts and,cessibility to mathematics by translating complex concepts and
"findings into understandable language for non-specialists [465],","findings into understandable language for non-specialists [465],"
where the gap between theoretical mathematics and applied,where the gap between theoretical mathematics and applied
"contexts such as physics, engineering, and economics can be","contexts such as physics, engineering, and economics can be"
Law: LLMs can assist with the thematic analysis of legal doc-,Law: LLMs can assist with the thematic analysis of legal doc-
"uments, including generating initial coding for datasets, iden-","uments, including generating initial coding for datasets, iden-"
"tifying themes, and classifying data according to these themes.","tifying themes, and classifying data according to these themes."
This collaborative effort between legal experts and LLMs has,This collaborative effort between legal experts and LLMs has
proved to be effective in analyzing legal texts such as court,proved to be effective in analyzing legal texts such as court
"opinions on theft, improving both the efficiency and quality of","opinions on theft, improving both the efficiency and quality of"
the research [466].,the research [466].
"Additionally, LLMs have been evaluated for","Additionally, LLMs have been evaluated for"
"their ability to generate explanations of legal terms, focusing","their ability to generate explanations of legal terms, focusing"
on improving factual accuracy and relevance by incorporating,on improving factual accuracy and relevance by incorporating
sentences from case law.,sentences from case law.
By feeding relevant case law into the,By feeding relevant case law into the
"LLM, the augmented models can generate higher-quality expla-","LLM, the augmented models can generate higher-quality expla-"
nations with less factually incorrect information [467].,nations with less factually incorrect information [467].
More-,More-
"over, LLMs can be trained with specialized domain knowledge","over, LLMs can be trained with specialized domain knowledge"
to perform legal reasoning tasks [468] and answer legal ques-,to perform legal reasoning tasks [468] and answer legal ques-
"Finance: LLMs like BloombergGPT [151], trained on exten-","Finance: LLMs like BloombergGPT [151], trained on exten-"
"sive proprietary financial datasets, exhibit superior performance","sive proprietary financial datasets, exhibit superior performance"
on financial tasks.,on financial tasks.
This indicates the value of domain-specific,This indicates the value of domain-specific
training in creating LLMs that can more accurately understand,training in creating LLMs that can more accurately understand
and process industry-specific language and concepts.,and process industry-specific language and concepts.
The intro-,The intro-
duction of FinGPT [470] as an open-source model offers trans-,duction of FinGPT [470] as an open-source model offers trans-
parent and accessible resources to develop novel applications,parent and accessible resources to develop novel applications
"such as robo-advising, algorithmic trading, and low-code so-","such as robo-advising, algorithmic trading, and low-code so-"
"lutions, ultimately expanding the capabilities of financial ser-","lutions, ultimately expanding the capabilities of financial ser-"
vices.,vices.
Both BloombergGPT and FinGPT show the adaptabil-,Both BloombergGPT and FinGPT show the adaptabil-
"ity of LLMs to the financial domain, with the former showing","ity of LLMs to the financial domain, with the former showing"
the power of custom datasets and the latter emphasizing a data-,the power of custom datasets and the latter emphasizing a data-
centric approach and low-rank adaptation techniques for cus-,centric approach and low-rank adaptation techniques for cus-
tomization.,tomization.
"Moreover, LLMs demonstrate an ability to break","Moreover, LLMs demonstrate an ability to break"
"down complex financial tasks into actionable plans, enabling","down complex financial tasks into actionable plans, enabling"
end-to-end solutions that were previously unfeasible with a sin-,end-to-end solutions that were previously unfeasible with a sin-
"Robotics: In robotics research, LLMs have promising appli-","Robotics: In robotics research, LLMs have promising appli-"
"cations, such as enhancing human-robot interaction [28, 472,","cations, such as enhancing human-robot interaction [28, 472,"
"473, 474], task planning [237], motion planning [246], nav-","473, 474], task planning [237], motion planning [246], nav-"
"igation [246, 475], object manipulation [236], personalized","igation [246, 475], object manipulation [236], personalized"
"robots [476], etc.","robots [476], etc."
LLMs enable robots to understand the en-,LLMs enable robots to understand the en-
vironment effectively and generate plans to complete tasks col-,vironment effectively and generate plans to complete tasks col-
"laboratively [240, 26].","laboratively [240, 26]."
They can facilitate continuous learning,They can facilitate continuous learning
by allowing robots to access and integrate information from a,by allowing robots to access and integrate information from a
"wide range of sources, helping robots acquire new skills, adapt","wide range of sources, helping robots acquire new skills, adapt"
"to changes, and refine their paths [224, 233, 234].","to changes, and refine their paths [224, 233, 234]."
7.,7.
Challenges and Future Directions,Challenges and Future Directions
LLMs such as GPT-4 and its predecessors have significantly,LLMs such as GPT-4 and its predecessors have significantly
advanced natural language processing.,advanced natural language processing.
"Nevertheless, they also","Nevertheless, they also"
bring along a set of challenges.,bring along a set of challenges.
"The computational cost, ad-","The computational cost, ad-"
"versarial robustness, and interpretability are among the tech-","versarial robustness, and interpretability are among the tech-"
nical challenges that are intrinsic to these models.,nical challenges that are intrinsic to these models.
"more, as these models are scaled up to handle more complex","more, as these models are scaled up to handle more complex"
Page 34:,Page 34:
"tasks or to operate in more complex or dynamic environments,","tasks or to operate in more complex or dynamic environments,"
"new challenges in scalability, privacy, and real-time processing","new challenges in scalability, privacy, and real-time processing"
emerge.,emerge.
"On the frontier of foundational research, integrating","On the frontier of foundational research, integrating"
multi-modality and the effectiveness of transfer learning are be-,multi-modality and the effectiveness of transfer learning are be-
ing keenly explored.,ing keenly explored.
"Additionally, the continuous learning as-","Additionally, the continuous learning as-"
"pect of these models, which aims to have models that can adapt","pect of these models, which aims to have models that can adapt"
"to new information over time, presents a fresh set of challenges.","to new information over time, presents a fresh set of challenges."
These challenges not only underscore the technical intricacies,These challenges not only underscore the technical intricacies
involved but also highlight the broader impact and the future,involved but also highlight the broader impact and the future
trajectory of LLMs in real-world applications.,trajectory of LLMs in real-world applications.
The following,The following
"sections delve into these challenges, shedding light on the on-","sections delve into these challenges, shedding light on the on-"
going and potential efforts to address them.,going and potential efforts to address them.
Computational Cost: Training LLMs require extensive compu-,Computational Cost: Training LLMs require extensive compu-
"tational resources, which increases production costs and raises","tational resources, which increases production costs and raises"
environmental concerns due to substantial energy consump-,environmental concerns due to substantial energy consump-
tion during large-scale training.,tion during large-scale training.
Improved performance occurs,Improved performance occurs
"as computational resources increase, but the rate of improve-","as computational resources increase, but the rate of improve-"
ment gradually decreases when both the model and dataset,ment gradually decreases when both the model and dataset
"size remain fixed, following the power law of diminishing re-","size remain fixed, following the power law of diminishing re-"
Bias and Fairness: LLMs can inherit and amplify societal bi-,Bias and Fairness: LLMs can inherit and amplify societal bi-
ases in their training data.,ases in their training data.
These biases can manifest in the,These biases can manifest in the
"model’s outputs, leading to potential ethical and fairness is-","model’s outputs, leading to potential ethical and fairness is-"
Overfitting: Although LLMs possess substantial learning ca-,Overfitting: Although LLMs possess substantial learning ca-
"pabilities, they are susceptible to overfitting noisy and peculiar","pabilities, they are susceptible to overfitting noisy and peculiar"
patterns within their extensive training data.,patterns within their extensive training data.
"Consequently, this","Consequently, this"
may cause them to generate illogical responses [479].,may cause them to generate illogical responses [479].
The de-,The de-
bate about Memorization vs. Generalization in LLMs is about,bate about Memorization vs. Generalization in LLMs is about
finding the right balance.,finding the right balance.
Memorization allows the model to,Memorization allows the model to
"remember specific details from its training data, ensuring it can","remember specific details from its training data, ensuring it can"
provide accurate answers to precise questions.,provide accurate answers to precise questions.
"However, gen-","However, gen-"
eralization enables the model to make inferences and produce,eralization enables the model to make inferences and produce
"responses for inputs it has not seen before, which is essential","responses for inputs it has not seen before, which is essential"
for handling various real-world tasks.,for handling various real-world tasks.
Striking the right bal-,Striking the right bal-
ance is the challenge: too much memorization can lead to over-,ance is the challenge: too much memorization can lead to over-
"fitting, making the model inflexible and struggling with new","fitting, making the model inflexible and struggling with new"
Economic and Research Inequality: The high cost of train-,Economic and Research Inequality: The high cost of train-
ing and deploying LLMs may make their development concen-,ing and deploying LLMs may make their development concen-
"trated within well-funded organizations, potentially worsening","trated within well-funded organizations, potentially worsening"
economic and research inequalities in AI [481].,economic and research inequalities in AI [481].
"Reasoning and Planning: Some reasoning and planning tasks,","Reasoning and Planning: Some reasoning and planning tasks,"
"even as seemingly simple as common-sense planning, which","even as seemingly simple as common-sense planning, which"
"humans find easy, remain well beyond the current capabilities","humans find easy, remain well beyond the current capabilities"
of LLMs evaluated using an assessment framework.,of LLMs evaluated using an assessment framework.
This is not,This is not
"entirely unexpected, considering that LLMs primarily generate","entirely unexpected, considering that LLMs primarily generate"
text completions based on likelihood and offer no solid guaran-,text completions based on likelihood and offer no solid guaran-
tees in terms of reasoning abilities [482].,tees in terms of reasoning abilities [482].
"Hallucinations: LLMs exhibit “hallucinations"", where they","Hallucinations: LLMs exhibit “hallucinations"", where they"
"generate responses that, while sounding plausible, are incorrect","generate responses that, while sounding plausible, are incorrect"
or do not align with the provided information [483].,or do not align with the provided information [483].
Hallucina-,Hallucina-
tions can be categorized into three categories.,tions can be categorized into three categories.
"• Input-conflicting hallucination, wherein LLMs produce","• Input-conflicting hallucination, wherein LLMs produce"
content that diverges from the input given by users.,content that diverges from the input given by users.
"• Context-conflicting hallucination, where LLMs generate","• Context-conflicting hallucination, where LLMs generate"
content that contradicts information they have generated,content that contradicts information they have generated
• Fact-conflicting hallucination involves LLM’s generation,• Fact-conflicting hallucination involves LLM’s generation
of content that does not align with established world,of content that does not align with established world
"Prompt Engineering: Prompts serve as inputs to LLMs, and","Prompt Engineering: Prompts serve as inputs to LLMs, and"
their syntax and semantics play a crucial role in determining,their syntax and semantics play a crucial role in determining
the model’s output.,the model’s output.
"The prompt variations, sometimes counter-","The prompt variations, sometimes counter-"
"intuitive to humans, can result in significant changes in model","intuitive to humans, can result in significant changes in model"
"output and are addressed through prompt engineering, which","output and are addressed through prompt engineering, which"
involves designing natural language queries to guide LLMs,involves designing natural language queries to guide LLMs
Limited Knowledge: Information acquired during pretraining,Limited Knowledge: Information acquired during pretraining
is limited and may become obsolete after some time.,is limited and may become obsolete after some time.
training the model using updated data is costly.,training the model using updated data is costly.
To generate,To generate
"factually accurate responses, people use a retrieval augmen-","factually accurate responses, people use a retrieval augmen-"
"However, pre-trained models are not","However, pre-trained models are not"
"trained with retrieval augmentation generation (RAG) [6, 21];","trained with retrieval augmentation generation (RAG) [6, 21];"
"hence, adapting the training pipeline is necessary [193, 25].","hence, adapting the training pipeline is necessary [193, 25]."
Safety and Controllability: Using LLMs comes with the risk,Safety and Controllability: Using LLMs comes with the risk
"of generating harmful, misleading, or inappropriate content,","of generating harmful, misleading, or inappropriate content,"
whether by accident or when given specific prompts.,whether by accident or when given specific prompts.
Ensuring,Ensuring
these models are safely utilized is a significant concern [485].,these models are safely utilized is a significant concern [485].
Security and Privacy: LLMs are prone to leaking personal,Security and Privacy: LLMs are prone to leaking personal
"information and generating false, unethical, misaligned re-","information and generating false, unethical, misaligned re-"
sponses.,sponses.
"Researchers have explored various security attacks,","Researchers have explored various security attacks,"
"i.e., backdoor attacks, jailbreaking, prompt injection, and data","i.e., backdoor attacks, jailbreaking, prompt injection, and data"
"poisoning, that lead to breaking LLMs security.","poisoning, that lead to breaking LLMs security."
developing better defense mechanisms is essential to ensure,developing better defense mechanisms is essential to ensure
"LLMs are safe, reliable, and trustworthy for complex AI","LLMs are safe, reliable, and trustworthy for complex AI"
"Multi-modal learning, where LLMs are","Multi-modal learning, where LLMs are"
"trained on diverse data like text, images, and videos, aims to","trained on diverse data like text, images, and videos, aims to"
create models with richer understanding but faces challenges,create models with richer understanding but faces challenges
"in data alignment, fusion strategies, and higher computational","in data alignment, fusion strategies, and higher computational"
LLMs are often pre-trained on,LLMs are often pre-trained on
"large datasets and then fine-tuned on domain-specific data,","large datasets and then fine-tuned on domain-specific data,"
"However, they face issues like","However, they face issues like"
"domain adaptation and catastrophic forgetting, which hinder","domain adaptation and catastrophic forgetting, which hinder"
the retention of original knowledge when learning new tasks.,the retention of original knowledge when learning new tasks.
have shown great capabilities in various tasks but are vul-,have shown great capabilities in various tasks but are vul-
"nerable to adversarial attacks, where slight, deliberate input","nerable to adversarial attacks, where slight, deliberate input"
"BERT, adversarial fine-tuning can enhance robustness, al-","BERT, adversarial fine-tuning can enhance robustness, al-"
though it sometimes compromises generalization [487].,though it sometimes compromises generalization [487].
"LLMs integrate more into complex systems, examining their","LLMs integrate more into complex systems, examining their"
"security properties becomes crucial, given the emerging field","security properties becomes crucial, given the emerging field"
of adversarial attacks on LLMs within trustworthy ML [488].,of adversarial attacks on LLMs within trustworthy ML [488].
"This vulnerability is notable in safety-critical domains, ne-","This vulnerability is notable in safety-critical domains, ne-"
cessitating robust adversarial evaluation tools to ensure LLM,cessitating robust adversarial evaluation tools to ensure LLM
Interpretability and Explainability: The “black-box” nature,Interpretability and Explainability: The “black-box” nature
of LLMs poses challenges in understanding their decision-,of LLMs poses challenges in understanding their decision-
"making, which is crucial for broader acceptance and trust,","making, which is crucial for broader acceptance and trust,"
Page 35:,Page 35:
"capabilities, the lack of insight into their operation limits their","capabilities, the lack of insight into their operation limits their"
"effectiveness and trustworthiness [490, 491].","effectiveness and trustworthiness [490, 491]."
Efforts are being,Efforts are being
made to make LLMs more explainable to promote user trust,made to make LLMs more explainable to promote user trust
and to ensure responsible AI usage.,and to ensure responsible AI usage.
Understanding the logic,Understanding the logic
behind LLMs’ responses is essential for fostering trust and,behind LLMs’ responses is essential for fostering trust and
ensuring they align with human values and legal standards.,ensuring they align with human values and legal standards.
Privacy concerns in Large Language,Privacy concerns in Large Language
Models (LLMs) have escalated with their growth in complexity,Models (LLMs) have escalated with their growth in complexity
"and size, particularly around data sharing and potential misuse.","and size, particularly around data sharing and potential misuse."
"There is a risk of malicious content creation, filter bypass,","There is a risk of malicious content creation, filter bypass,"
"and data privacy issues, especially in e-commerce, where","and data privacy issues, especially in e-commerce, where"
protecting customer privacy is crucial.,protecting customer privacy is crucial.
If models are trained,If models are trained
"on private data, additional concerns arise if such models are","on private data, additional concerns arise if such models are"
made publicly available.,made publicly available.
LLMs tend to memorize phrases from,LLMs tend to memorize phrases from
"their training sets, which an adversary could exploit to extract","their training sets, which an adversary could exploit to extract"
"sensitive data, posing a threat to personal privacy [492, 493].","sensitive data, posing a threat to personal privacy [492, 493]."
Real-Time Processing: Real-time processing in Large Lan-,Real-Time Processing: Real-time processing in Large Lan-
"guage Models (LLMs) is pivotal for various applications,","guage Models (LLMs) is pivotal for various applications,"
especially with the rising popularity of mobile AI applications,especially with the rising popularity of mobile AI applications
and concerns regarding information security and privacy.,and concerns regarding information security and privacy.
"However, LLMs often have hundreds of layers and millions","However, LLMs often have hundreds of layers and millions"
"of parameters, which impede real-time processing due to the","of parameters, which impede real-time processing due to the"
high computational demands and limited weight storage on,high computational demands and limited weight storage on
"hardware platforms, particularly in edge computing environ-","hardware platforms, particularly in edge computing environ-"
While certain efforts like MobileBERT aim,While certain efforts like MobileBERT aim
"to reduce memory requirements, they still face substantial","to reduce memory requirements, they still face substantial"
"execution overhead due to the large number of model layers,","execution overhead due to the large number of model layers,"
leading to high inference latency.,leading to high inference latency.
shown considerable progress in understanding and generating,shown considerable progress in understanding and generating
"text, yet they often struggle with preserving context and","text, yet they often struggle with preserving context and"
"handling long-term dependencies, particularly in complex,","handling long-term dependencies, particularly in complex,"
multi-turn conversations or long documents.,multi-turn conversations or long documents.
This limitation,This limitation
can lead to incoherent or irrelevant responses.,can lead to incoherent or irrelevant responses.
Hardware Acceleration: The growth of LLMs presents signif-,Hardware Acceleration: The growth of LLMs presents signif-
icant hardware challenges due to the increasing computational,icant hardware challenges due to the increasing computational
and memory demands associated with training and deploying,and memory demands associated with training and deploying
these models.,these models.
GPUs have played a crucial role in meeting the,GPUs have played a crucial role in meeting the
"hardware requirements for training LLMs, with the networking","hardware requirements for training LLMs, with the networking"
industry also evolving to optimize hardware for training,industry also evolving to optimize hardware for training
workloads.,workloads.
"However, the growing size of LLMs, which has","However, the growing size of LLMs, which has"
"been outpacing hardware progress, makes model inference in-","been outpacing hardware progress, makes model inference in-"
creasingly costly.,creasingly costly.
Model quantization is a promising approach,Model quantization is a promising approach
to bridge the widening gap between LLM size and hardware,to bridge the widening gap between LLM size and hardware
like GPUs or TPUs can significantly reduce the computational,like GPUs or TPUs can significantly reduce the computational
"cost, making real-time applications more feasible, they may not","cost, making real-time applications more feasible, they may not"
"fully resolve all limitations, necessitating further advancements","fully resolve all limitations, necessitating further advancements"
Regulatory and Ethical Frameworks: The rapid advancements,Regulatory and Ethical Frameworks: The rapid advancements
in artificial intelligence have given rise to sophisticated Large,in artificial intelligence have given rise to sophisticated Large
Language Models (LLMs) like OpenAI’s GPT-4 [157] and,Language Models (LLMs) like OpenAI’s GPT-4 [157] and
Google’s Bard.,Google’s Bard.
These developments underscore the imperative,These developments underscore the imperative
for regulatory oversight to manage the ethical and social,for regulatory oversight to manage the ethical and social
challenges accompanying LLMs’ widespread use [496].,challenges accompanying LLMs’ widespread use [496].
For,For
"instance, LLMs can generate content that can be used posi-","instance, LLMs can generate content that can be used posi-"
"tively or negatively, emphasizing the need for proactive ethical","tively or negatively, emphasizing the need for proactive ethical"
frameworks and policy measures to guide their responsible,frameworks and policy measures to guide their responsible
use and assign accountability for their outputs [497].,use and assign accountability for their outputs [497].
Auditing,Auditing
is identified as a promising governance mechanism to ensure,is identified as a promising governance mechanism to ensure
"that AI systems, including LLMs, are designed and deployed","that AI systems, including LLMs, are designed and deployed"
"ethically, legally, and technically robust [498].","ethically, legally, and technically robust [498]."
This article has comprehensively reviewed the develop-,This article has comprehensively reviewed the develop-
It contributes to summarizing significant,It contributes to summarizing significant
findings of LLMs in the existing literature and provides a,findings of LLMs in the existing literature and provides a
"detailed analysis of the design aspects, including architec-","detailed analysis of the design aspects, including architec-"
"tures, datasets, and training pipelines.","tures, datasets, and training pipelines."
architectural components and training strategies employed by,architectural components and training strategies employed by
These aspects are presented as summaries,These aspects are presented as summaries
and discussions throughout the article.,and discussions throughout the article.
discussed the performance differences of LLMs in zero-shot,discussed the performance differences of LLMs in zero-shot
"and few-shot settings, explored the impact of fine-tuning, and","and few-shot settings, explored the impact of fine-tuning, and"
compared supervised and generalized models and encoder vs.,compared supervised and generalized models and encoder vs.
decoder vs. encoder-decoder architectures.,decoder vs. encoder-decoder architectures.
A comprehensive,A comprehensive
"review of multi-modal LLMs, retrieval augmented LLMs,","review of multi-modal LLMs, retrieval augmented LLMs,"
"LLMs-powered agents, efficient LLMs, datasets, evaluation,","LLMs-powered agents, efficient LLMs, datasets, evaluation,"
"applications, and challenges is also provided.","applications, and challenges is also provided."
This article is,This article is
"anticipated to serve as a valuable resource for researchers,","anticipated to serve as a valuable resource for researchers,"
offering insights into the recent advancements in LLMs and,offering insights into the recent advancements in LLMs and
providing fundamental concepts and details to develop better,providing fundamental concepts and details to develop better
The author/s would like to acknowl-,The author/s would like to acknowl-
edge the support received from Saudi Data and AI Authority,edge the support received from Saudi Data and AI Authority
(SDAIA) and King Fahd University of Petroleum and Miner-,(SDAIA) and King Fahd University of Petroleum and Miner-
als (KFUPM) under SDAIA-KFUPM Joint Research Center for,als (KFUPM) under SDAIA-KFUPM Joint Research Center for
Artificial Intelligence Grant No.,Artificial Intelligence Grant No.
JRC-AI-RFP-11.,JRC-AI-RFP-11.
Page 1:,Page 1:
"Provided proper attribution is provided, Google hereby grants permission to","Provided proper attribution is provided, Google hereby grants permission to"
reproduce the tables and figures in this paper solely for use in journalistic or,reproduce the tables and figures in this paper solely for use in journalistic or
Attention Is All You Need,Attention Is All You Need
The dominant sequence transduction models are based on complex recurrent or,The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks that include an encoder and a decoder.,convolutional neural networks that include an encoder and a decoder.
The best,The best
performing models also connect the encoder and decoder through an attention,performing models also connect the encoder and decoder through an attention
mechanism.,mechanism.
"We propose a new simple network architecture, the Transformer,","We propose a new simple network architecture, the Transformer,"
"based solely on attention mechanisms, dispensing with recurrence and convolutions","based solely on attention mechanisms, dispensing with recurrence and convolutions"
entirely.,entirely.
Experiments on two machine translation tasks show these models to,Experiments on two machine translation tasks show these models to
be superior in quality while being more parallelizable and requiring significantly,be superior in quality while being more parallelizable and requiring significantly
less time to train.,less time to train.
Our model achieves 28.4 BLEU on the WMT 2014 English-,Our model achieves 28.4 BLEU on the WMT 2014 English-
"to-German translation task, improving over the existing best results, including","to-German translation task, improving over the existing best results, including"
"ensembles, by over 2 BLEU.","ensembles, by over 2 BLEU."
"On the WMT 2014 English-to-French translation task,","On the WMT 2014 English-to-French translation task,"
our model establishes a new single-model state-of-the-art BLEU score of 41.8 after,our model establishes a new single-model state-of-the-art BLEU score of 41.8 after
"training for 3.5 days on eight GPUs, a small fraction of the training costs of the","training for 3.5 days on eight GPUs, a small fraction of the training costs of the"
best models from the literature.,best models from the literature.
We show that the Transformer generalizes well to,We show that the Transformer generalizes well to
other tasks by applying it successfully to English constituency parsing both with,other tasks by applying it successfully to English constituency parsing both with
large and limited training data.,large and limited training data.
∗Equal contribution.,∗Equal contribution.
Listing order is random.,Listing order is random.
Jakob proposed replacing RNNs with self-attention and started,Jakob proposed replacing RNNs with self-attention and started
the effort to evaluate this idea.,the effort to evaluate this idea.
"Ashish, with Illia, designed and implemented the first Transformer models and","Ashish, with Illia, designed and implemented the first Transformer models and"
has been crucially involved in every aspect of this work.,has been crucially involved in every aspect of this work.
"Noam proposed scaled dot-product attention, multi-head","Noam proposed scaled dot-product attention, multi-head"
attention and the parameter-free position representation and became the other person involved in nearly every,attention and the parameter-free position representation and became the other person involved in nearly every
detail.,detail.
"Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and","Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and"
tensor2tensor.,tensor2tensor.
"Llion also experimented with novel model variants, was responsible for our initial codebase, and","Llion also experimented with novel model variants, was responsible for our initial codebase, and"
efficient inference and visualizations.,efficient inference and visualizations.
Lukasz and Aidan spent countless long days designing various parts of and,Lukasz and Aidan spent countless long days designing various parts of and
"implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating","implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating"
†Work performed while at Google Brain.,†Work performed while at Google Brain.
‡Work performed while at Google Research.,‡Work performed while at Google Research.
"31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.","31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA."
arXiv:1706.03762v7  [cs.CL]  2 Aug 2023,arXiv:1706.03762v7  [cs.CL]  2 Aug 2023
Page 2:,Page 2:
"Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks","Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks"
"in particular, have been firmly established as state of the art approaches in sequence modeling and","in particular, have been firmly established as state of the art approaches in sequence modeling and"
"transduction problems such as language modeling and machine translation [35, 2, 5].","transduction problems such as language modeling and machine translation [35, 2, 5]."
Numerous,Numerous
efforts have since continued to push the boundaries of recurrent language models and encoder-decoder,efforts have since continued to push the boundaries of recurrent language models and encoder-decoder
Recurrent models typically factor computation along the symbol positions of the input and output,Recurrent models typically factor computation along the symbol positions of the input and output
sequences.,sequences.
"Aligning the positions to steps in computation time, they generate a sequence of hidden","Aligning the positions to steps in computation time, they generate a sequence of hidden"
"states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently","states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently"
"sequential nature precludes parallelization within training examples, which becomes critical at longer","sequential nature precludes parallelization within training examples, which becomes critical at longer"
"sequence lengths, as memory constraints limit batching across examples.","sequence lengths, as memory constraints limit batching across examples."
Recent work has achieved,Recent work has achieved
significant improvements in computational efficiency through factorization tricks [21] and conditional,significant improvements in computational efficiency through factorization tricks [21] and conditional
"computation [32], while also improving model performance in case of the latter.","computation [32], while also improving model performance in case of the latter."
The fundamental,The fundamental
"constraint of sequential computation, however, remains.","constraint of sequential computation, however, remains."
Attention mechanisms have become an integral part of compelling sequence modeling and transduc-,Attention mechanisms have become an integral part of compelling sequence modeling and transduc-
"tion models in various tasks, allowing modeling of dependencies without regard to their distance in","tion models in various tasks, allowing modeling of dependencies without regard to their distance in"
"the input or output sequences [2, 19].","the input or output sequences [2, 19]."
"In all but a few cases [27], however, such attention mechanisms","In all but a few cases [27], however, such attention mechanisms"
are used in conjunction with a recurrent network.,are used in conjunction with a recurrent network.
"In this work we propose the Transformer, a model architecture eschewing recurrence and instead","In this work we propose the Transformer, a model architecture eschewing recurrence and instead"
relying entirely on an attention mechanism to draw global dependencies between input and output.,relying entirely on an attention mechanism to draw global dependencies between input and output.
The Transformer allows for significantly more parallelization and can reach a new state of the art in,The Transformer allows for significantly more parallelization and can reach a new state of the art in
translation quality after being trained for as little as twelve hours on eight P100 GPUs.,translation quality after being trained for as little as twelve hours on eight P100 GPUs.
The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU,The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU
"[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building","[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building"
"block, computing hidden representations in parallel for all input and output positions.","block, computing hidden representations in parallel for all input and output positions."
"In these models,","In these models,"
the number of operations required to relate signals from two arbitrary input or output positions grows,the number of operations required to relate signals from two arbitrary input or output positions grows
"in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.","in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet."
This makes,This makes
it more difficult to learn dependencies between distant positions [12].,it more difficult to learn dependencies between distant positions [12].
In the Transformer this is,In the Transformer this is
"reduced to a constant number of operations, albeit at the cost of reduced effective resolution due","reduced to a constant number of operations, albeit at the cost of reduced effective resolution due"
"to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as","to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as"
"Self-attention, sometimes called intra-attention is an attention mechanism relating different positions","Self-attention, sometimes called intra-attention is an attention mechanism relating different positions"
of a single sequence in order to compute a representation of the sequence.,of a single sequence in order to compute a representation of the sequence.
Self-attention has been,Self-attention has been
"used successfully in a variety of tasks including reading comprehension, abstractive summarization,","used successfully in a variety of tasks including reading comprehension, abstractive summarization,"
"textual entailment and learning task-independent sentence representations [4, 27, 28, 22].","textual entailment and learning task-independent sentence representations [4, 27, 28, 22]."
End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-,End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-
aligned recurrence and have been shown to perform well on simple-language question answering and,aligned recurrence and have been shown to perform well on simple-language question answering and
"To the best of our knowledge, however, the Transformer is the first transduction model relying","To the best of our knowledge, however, the Transformer is the first transduction model relying"
entirely on self-attention to compute representations of its input and output without using sequence-,entirely on self-attention to compute representations of its input and output without using sequence-
aligned RNNs or convolution.,aligned RNNs or convolution.
"In the following sections, we will describe the Transformer, motivate","In the following sections, we will describe the Transformer, motivate"
"self-attention and discuss its advantages over models such as [17, 18] and [9].","self-attention and discuss its advantages over models such as [17, 18] and [9]."
"Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].","Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]."
"Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence","Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence"
"of continuous representations z = (z1, ..., zn).","of continuous representations z = (z1, ..., zn)."
"Given z, the decoder then generates an output","Given z, the decoder then generates an output"
"sequence (y1, ..., ym) of symbols one element at a time.","sequence (y1, ..., ym) of symbols one element at a time."
At each step the model is auto-regressive,At each step the model is auto-regressive
"[10], consuming the previously generated symbols as additional input when generating the next.","[10], consuming the previously generated symbols as additional input when generating the next."
Page 3:,Page 3:
Figure 1: The Transformer - model architecture.,Figure 1: The Transformer - model architecture.
"The Transformer follows this overall architecture using stacked self-attention and point-wise, fully","The Transformer follows this overall architecture using stacked self-attention and point-wise, fully"
"connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,","connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,"
The encoder is composed of a stack of N = 6 identical layers.,The encoder is composed of a stack of N = 6 identical layers.
Each layer has two,Each layer has two
sub-layers.,sub-layers.
"The first is a multi-head self-attention mechanism, and the second is a simple, position-","The first is a multi-head self-attention mechanism, and the second is a simple, position-"
wise fully connected feed-forward network.,wise fully connected feed-forward network.
We employ a residual connection [11] around each of,We employ a residual connection [11] around each of
"the two sub-layers, followed by layer normalization [1].","the two sub-layers, followed by layer normalization [1]."
"That is, the output of each sub-layer is","That is, the output of each sub-layer is"
"LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer","LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer"
itself.,itself.
"To facilitate these residual connections, all sub-layers in the model, as well as the embedding","To facilitate these residual connections, all sub-layers in the model, as well as the embedding"
"layers, produce outputs of dimension dmodel = 512.","layers, produce outputs of dimension dmodel = 512."
The decoder is also composed of a stack of N = 6 identical layers.,The decoder is also composed of a stack of N = 6 identical layers.
In addition to the two,In addition to the two
"sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head","sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head"
attention over the output of the encoder stack.,attention over the output of the encoder stack.
"Similar to the encoder, we employ residual connections","Similar to the encoder, we employ residual connections"
"around each of the sub-layers, followed by layer normalization.","around each of the sub-layers, followed by layer normalization."
We also modify the self-attention,We also modify the self-attention
sub-layer in the decoder stack to prevent positions from attending to subsequent positions.,sub-layer in the decoder stack to prevent positions from attending to subsequent positions.
This,This
"masking, combined with fact that the output embeddings are offset by one position, ensures that the","masking, combined with fact that the output embeddings are offset by one position, ensures that the"
predictions for position i can depend only on the known outputs at positions less than i.,predictions for position i can depend only on the known outputs at positions less than i.
"An attention function can be described as mapping a query and a set of key-value pairs to an output,","An attention function can be described as mapping a query and a set of key-value pairs to an output,"
"where the query, keys, values, and output are all vectors.","where the query, keys, values, and output are all vectors."
The output is computed as a weighted sum,The output is computed as a weighted sum
Page 4:,Page 4:
Figure 2: (left) Scaled Dot-Product Attention.,Figure 2: (left) Scaled Dot-Product Attention.
(right) Multi-Head Attention consists of several,(right) Multi-Head Attention consists of several
attention layers running in parallel.,attention layers running in parallel.
"of the values, where the weight assigned to each value is computed by a compatibility function of the","of the values, where the weight assigned to each value is computed by a compatibility function of the"
query with the corresponding key.,query with the corresponding key.
"We call our particular attention ""Scaled Dot-Product Attention"" (Figure 2).","We call our particular attention ""Scaled Dot-Product Attention"" (Figure 2)."
The input consists of,The input consists of
"queries and keys of dimension dk, and values of dimension dv.","queries and keys of dimension dk, and values of dimension dv."
We compute the dot products of the,We compute the dot products of the
"query with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the","query with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the"
"In practice, we compute the attention function on a set of queries simultaneously, packed together","In practice, we compute the attention function on a set of queries simultaneously, packed together"
into a matrix Q.,into a matrix Q.
The keys and values are also packed together into matrices K and V .,The keys and values are also packed together into matrices K and V .
We compute,We compute
the matrix of outputs as:,the matrix of outputs as:
"Attention(Q, K, V ) = softmax(QKT","Attention(Q, K, V ) = softmax(QKT"
"The two most commonly used attention functions are additive attention [2], and dot-product (multi-","The two most commonly used attention functions are additive attention [2], and dot-product (multi-"
plicative) attention.,plicative) attention.
"Dot-product attention is identical to our algorithm, except for the scaling factor","Dot-product attention is identical to our algorithm, except for the scaling factor"
√dk .,√dk .
Additive attention computes the compatibility function using a feed-forward network with,Additive attention computes the compatibility function using a feed-forward network with
a single hidden layer.,a single hidden layer.
"While the two are similar in theoretical complexity, dot-product attention is","While the two are similar in theoretical complexity, dot-product attention is"
"much faster and more space-efficient in practice, since it can be implemented using highly optimized","much faster and more space-efficient in practice, since it can be implemented using highly optimized"
"While for small values of dk the two mechanisms perform similarly, additive attention outperforms","While for small values of dk the two mechanisms perform similarly, additive attention outperforms"
dot product attention without scaling for larger values of dk [3].,dot product attention without scaling for larger values of dk [3].
We suspect that for large values of,We suspect that for large values of
"dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has","dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has"
extremely small gradients 4.,extremely small gradients 4.
"To counteract this effect, we scale the dot products by","To counteract this effect, we scale the dot products by"
"Instead of performing a single attention function with dmodel-dimensional keys, values and queries,","Instead of performing a single attention function with dmodel-dimensional keys, values and queries,"
"we found it beneficial to linearly project the queries, keys and values h times with different, learned","we found it beneficial to linearly project the queries, keys and values h times with different, learned"
"linear projections to dk, dk and dv dimensions, respectively.","linear projections to dk, dk and dv dimensions, respectively."
On each of these projected versions of,On each of these projected versions of
"queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional","queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional"
"4To illustrate why the dot products get large, assume that the components of q and k are independent random","4To illustrate why the dot products get large, assume that the components of q and k are independent random"
variables with mean 0 and variance 1.,variables with mean 0 and variance 1.
"Then their dot product, q · k = Pdk","Then their dot product, q · k = Pdk"
"i=1 qiki, has mean 0 and variance dk.","i=1 qiki, has mean 0 and variance dk."
Page 5:,Page 5:
output values.,output values.
"These are concatenated and once again projected, resulting in the final values, as","These are concatenated and once again projected, resulting in the final values, as"
Multi-head attention allows the model to jointly attend to information from different representation,Multi-head attention allows the model to jointly attend to information from different representation
subspaces at different positions.,subspaces at different positions.
"With a single attention head, averaging inhibits this.","With a single attention head, averaging inhibits this."
"MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O","MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O"
where headi = Attention(QW Q,where headi = Attention(QW Q
"i , V W V","i , V W V"
Where the projections are parameter matrices W Q,Where the projections are parameter matrices W Q
"In this work we employ h = 8 parallel attention layers, or heads.","In this work we employ h = 8 parallel attention layers, or heads."
For each of these we use,For each of these we use
dk = dv = dmodel/h = 64.,dk = dv = dmodel/h = 64.
"Due to the reduced dimension of each head, the total computational cost","Due to the reduced dimension of each head, the total computational cost"
is similar to that of single-head attention with full dimensionality.,is similar to that of single-head attention with full dimensionality.
Applications of Attention in our Model,Applications of Attention in our Model
The Transformer uses multi-head attention in three different ways:,The Transformer uses multi-head attention in three different ways:
"• In ""encoder-decoder attention"" layers, the queries come from the previous decoder layer,","• In ""encoder-decoder attention"" layers, the queries come from the previous decoder layer,"
and the memory keys and values come from the output of the encoder.,and the memory keys and values come from the output of the encoder.
This allows every,This allows every
position in the decoder to attend over all positions in the input sequence.,position in the decoder to attend over all positions in the input sequence.
This mimics the,This mimics the
typical encoder-decoder attention mechanisms in sequence-to-sequence models such as,typical encoder-decoder attention mechanisms in sequence-to-sequence models such as
• The encoder contains self-attention layers.,• The encoder contains self-attention layers.
"In a self-attention layer all of the keys, values","In a self-attention layer all of the keys, values"
"and queries come from the same place, in this case, the output of the previous layer in the","and queries come from the same place, in this case, the output of the previous layer in the"
encoder.,encoder.
Each position in the encoder can attend to all positions in the previous layer of the,Each position in the encoder can attend to all positions in the previous layer of the
"• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to","• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to"
all positions in the decoder up to and including that position.,all positions in the decoder up to and including that position.
We need to prevent leftward,We need to prevent leftward
information flow in the decoder to preserve the auto-regressive property.,information flow in the decoder to preserve the auto-regressive property.
We implement this,We implement this
inside of scaled dot-product attention by masking out (setting to −∞) all values in the input,inside of scaled dot-product attention by masking out (setting to −∞) all values in the input
of the softmax which correspond to illegal connections.,of the softmax which correspond to illegal connections.
See Figure 2.,See Figure 2.
"In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully","In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully"
"connected feed-forward network, which is applied to each position separately and identically.","connected feed-forward network, which is applied to each position separately and identically."
This,This
consists of two linear transformations with a ReLU activation in between.,consists of two linear transformations with a ReLU activation in between.
"FFN(x) = max(0, xW1 + b1)W2 + b2","FFN(x) = max(0, xW1 + b1)W2 + b2"
"While the linear transformations are the same across different positions, they use different parameters","While the linear transformations are the same across different positions, they use different parameters"
from layer to layer.,from layer to layer.
Another way of describing this is as two convolutions with kernel size 1.,Another way of describing this is as two convolutions with kernel size 1.
"The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality","The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality"
"Similarly to other sequence transduction models, we use learned embeddings to convert the input","Similarly to other sequence transduction models, we use learned embeddings to convert the input"
tokens and output tokens to vectors of dimension dmodel.,tokens and output tokens to vectors of dimension dmodel.
We also use the usual learned linear transfor-,We also use the usual learned linear transfor-
mation and softmax function to convert the decoder output to predicted next-token probabilities.,mation and softmax function to convert the decoder output to predicted next-token probabilities.
In,In
"our model, we share the same weight matrix between the two embedding layers and the pre-softmax","our model, we share the same weight matrix between the two embedding layers and the pre-softmax"
"linear transformation, similar to [30].","linear transformation, similar to [30]."
"In the embedding layers, we multiply those weights by √dmodel.","In the embedding layers, we multiply those weights by √dmodel."
Page 6:,Page 6:
"Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations","Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations"
for different layer types.,for different layer types.
"n is the sequence length, d is the representation dimension, k is the kernel","n is the sequence length, d is the representation dimension, k is the kernel"
size of convolutions and r the size of the neighborhood in restricted self-attention.,size of convolutions and r the size of the neighborhood in restricted self-attention.
O(k · n · d2),O(k · n · d2)
O(r · n · d),O(r · n · d)
"Since our model contains no recurrence and no convolution, in order for the model to make use of the","Since our model contains no recurrence and no convolution, in order for the model to make use of the"
"order of the sequence, we must inject some information about the relative or absolute position of the","order of the sequence, we must inject some information about the relative or absolute position of the"
tokens in the sequence.,tokens in the sequence.
"To this end, we add ""positional encodings"" to the input embeddings at the","To this end, we add ""positional encodings"" to the input embeddings at the"
bottoms of the encoder and decoder stacks.,bottoms of the encoder and decoder stacks.
The positional encodings have the same dimension dmodel,The positional encodings have the same dimension dmodel
"as the embeddings, so that the two can be summed.","as the embeddings, so that the two can be summed."
"There are many choices of positional encodings,","There are many choices of positional encodings,"
"In this work, we use sine and cosine functions of different frequencies:","In this work, we use sine and cosine functions of different frequencies:"
where pos is the position and i is the dimension.,where pos is the position and i is the dimension.
"That is, each dimension of the positional encoding","That is, each dimension of the positional encoding"
corresponds to a sinusoid.,corresponds to a sinusoid.
The wavelengths form a geometric progression from 2π to 10000 · 2π.,The wavelengths form a geometric progression from 2π to 10000 · 2π.
We,We
chose this function because we hypothesized it would allow the model to easily learn to attend by,chose this function because we hypothesized it would allow the model to easily learn to attend by
"relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of","relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of"
"We also experimented with using learned positional embeddings [9] instead, and found that the two","We also experimented with using learned positional embeddings [9] instead, and found that the two"
versions produced nearly identical results (see Table 3 row (E)).,versions produced nearly identical results (see Table 3 row (E)).
We chose the sinusoidal version,We chose the sinusoidal version
because it may allow the model to extrapolate to sequence lengths longer than the ones encountered,because it may allow the model to extrapolate to sequence lengths longer than the ones encountered
In this section we compare various aspects of self-attention layers to the recurrent and convolu-,In this section we compare various aspects of self-attention layers to the recurrent and convolu-
tional layers commonly used for mapping one variable-length sequence of symbol representations,tional layers commonly used for mapping one variable-length sequence of symbol representations
"(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden","(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden"
layer in a typical sequence transduction encoder or decoder.,layer in a typical sequence transduction encoder or decoder.
Motivating our use of self-attention we,Motivating our use of self-attention we
One is the total computational complexity per layer.,One is the total computational complexity per layer.
Another is the amount of computation that can,Another is the amount of computation that can
"be parallelized, as measured by the minimum number of sequential operations required.","be parallelized, as measured by the minimum number of sequential operations required."
The third is the path length between long-range dependencies in the network.,The third is the path length between long-range dependencies in the network.
Learning long-range,Learning long-range
dependencies is a key challenge in many sequence transduction tasks.,dependencies is a key challenge in many sequence transduction tasks.
One key factor affecting the,One key factor affecting the
ability to learn such dependencies is the length of the paths forward and backward signals have to,ability to learn such dependencies is the length of the paths forward and backward signals have to
traverse in the network.,traverse in the network.
The shorter these paths between any combination of positions in the input,The shorter these paths between any combination of positions in the input
"and output sequences, the easier it is to learn long-range dependencies [12].","and output sequences, the easier it is to learn long-range dependencies [12]."
Hence we also compare,Hence we also compare
the maximum path length between any two input and output positions in networks composed of the,the maximum path length between any two input and output positions in networks composed of the
"As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially","As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially"
"executed operations, whereas a recurrent layer requires O(n) sequential operations.","executed operations, whereas a recurrent layer requires O(n) sequential operations."
In terms of,In terms of
"computational complexity, self-attention layers are faster than recurrent layers when the sequence","computational complexity, self-attention layers are faster than recurrent layers when the sequence"
Page 7:,Page 7:
"length n is smaller than the representation dimensionality d, which is most often the case with","length n is smaller than the representation dimensionality d, which is most often the case with"
"sentence representations used by state-of-the-art models in machine translations, such as word-piece","sentence representations used by state-of-the-art models in machine translations, such as word-piece"
[38] and byte-pair [31] representations.,[38] and byte-pair [31] representations.
To improve computational performance for tasks involving,To improve computational performance for tasks involving
"very long sequences, self-attention could be restricted to considering only a neighborhood of size r in","very long sequences, self-attention could be restricted to considering only a neighborhood of size r in"
the input sequence centered around the respective output position.,the input sequence centered around the respective output position.
This would increase the maximum,This would increase the maximum
path length to O(n/r).,path length to O(n/r).
We plan to investigate this approach further in future work.,We plan to investigate this approach further in future work.
A single convolutional layer with kernel width k < n does not connect all pairs of input and output,A single convolutional layer with kernel width k < n does not connect all pairs of input and output
positions.,positions.
"Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,","Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,"
"or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths","or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths"
between any two positions in the network.,between any two positions in the network.
Convolutional layers are generally more expensive than,Convolutional layers are generally more expensive than
"recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity","recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity"
"considerably, to O(k · n · d + n · d2).","considerably, to O(k · n · d + n · d2)."
"Even with k = n, however, the complexity of a separable","Even with k = n, however, the complexity of a separable"
"convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,","convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,"
the approach we take in our model.,the approach we take in our model.
"As side benefit, self-attention could yield more interpretable models.","As side benefit, self-attention could yield more interpretable models."
We inspect attention distributions,We inspect attention distributions
from our models and present and discuss examples in the appendix.,from our models and present and discuss examples in the appendix.
Not only do individual attention,Not only do individual attention
"heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic","heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic"
and semantic structure of the sentences.,and semantic structure of the sentences.
This section describes the training regime for our models.,This section describes the training regime for our models.
We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million,We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million
sentence pairs.,sentence pairs.
"Sentences were encoded using byte-pair encoding [3], which has a shared source-","Sentences were encoded using byte-pair encoding [3], which has a shared source-"
target vocabulary of about 37000 tokens.,target vocabulary of about 37000 tokens.
"For English-French, we used the significantly larger WMT","For English-French, we used the significantly larger WMT"
2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece,2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece
vocabulary [38].,vocabulary [38].
Sentence pairs were batched together by approximate sequence length.,Sentence pairs were batched together by approximate sequence length.
Each training,Each training
batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000,batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000
We trained our models on one machine with 8 NVIDIA P100 GPUs.,We trained our models on one machine with 8 NVIDIA P100 GPUs.
For our base models using,For our base models using
"the hyperparameters described throughout the paper, each training step took about 0.4 seconds.","the hyperparameters described throughout the paper, each training step took about 0.4 seconds."
We,We
"trained the base models for a total of 100,000 steps or 12 hours.","trained the base models for a total of 100,000 steps or 12 hours."
"For our big models,(described on the","For our big models,(described on the"
"bottom line of table 3), step time was 1.0 seconds.","bottom line of table 3), step time was 1.0 seconds."
"The big models were trained for 300,000 steps","The big models were trained for 300,000 steps"
"We used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9.","We used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9."
We varied the learning,We varied the learning
"rate over the course of training, according to the formula:","rate over the course of training, according to the formula:"
"model · min(step_num−0.5, step_num · warmup_steps−1.5)","model · min(step_num−0.5, step_num · warmup_steps−1.5)"
"This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,","This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,"
and decreasing it thereafter proportionally to the inverse square root of the step number.,and decreasing it thereafter proportionally to the inverse square root of the step number.
We used,We used
We employ three types of regularization during training:,We employ three types of regularization during training:
Page 8:,Page 8:
Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the,Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the
English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.,English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
Deep-Att + PosUnk Ensemble [39],Deep-Att + PosUnk Ensemble [39]
GNMT + RL Ensemble [38],GNMT + RL Ensemble [38]
"We apply dropout [33] to the output of each sub-layer, before it is added to the","We apply dropout [33] to the output of each sub-layer, before it is added to the"
sub-layer input and normalized.,sub-layer input and normalized.
"In addition, we apply dropout to the sums of the embeddings and the","In addition, we apply dropout to the sums of the embeddings and the"
positional encodings in both the encoder and decoder stacks.,positional encodings in both the encoder and decoder stacks.
"For the base model, we use a rate of","For the base model, we use a rate of"
"During training, we employed label smoothing of value ϵls = 0.1 [36].","During training, we employed label smoothing of value ϵls = 0.1 [36]."
This,This
"hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.","hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score."
"On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)","On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)"
in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0,in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0
"BLEU, establishing a new state-of-the-art BLEU score of 28.4.","BLEU, establishing a new state-of-the-art BLEU score of 28.4."
The configuration of this model is,The configuration of this model is
listed in the bottom line of Table 3.,listed in the bottom line of Table 3.
Training took 3.5 days on 8 P100 GPUs.,Training took 3.5 days on 8 P100 GPUs.
Even our base model,Even our base model
"surpasses all previously published models and ensembles, at a fraction of the training cost of any of","surpasses all previously published models and ensembles, at a fraction of the training cost of any of"
"On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,","On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,"
"outperforming all of the previously published single models, at less than 1/4 the training cost of the","outperforming all of the previously published single models, at less than 1/4 the training cost of the"
previous state-of-the-art model.,previous state-of-the-art model.
The Transformer (big) model trained for English-to-French used,The Transformer (big) model trained for English-to-French used
"dropout rate Pdrop = 0.1, instead of 0.3.","dropout rate Pdrop = 0.1, instead of 0.3."
"For the base models, we used a single model obtained by averaging the last 5 checkpoints, which","For the base models, we used a single model obtained by averaging the last 5 checkpoints, which"
were written at 10-minute intervals.,were written at 10-minute intervals.
"For the big models, we averaged the last 20 checkpoints.","For the big models, we averaged the last 20 checkpoints."
We,We
used beam search with a beam size of 4 and length penalty α = 0.6 [38].,used beam search with a beam size of 4 and length penalty α = 0.6 [38].
These hyperparameters,These hyperparameters
were chosen after experimentation on the development set.,were chosen after experimentation on the development set.
We set the maximum output length during,We set the maximum output length during
"inference to input length + 50, but terminate early when possible [38].","inference to input length + 50, but terminate early when possible [38]."
Table 2 summarizes our results and compares our translation quality and training costs to other model,Table 2 summarizes our results and compares our translation quality and training costs to other model
architectures from the literature.,architectures from the literature.
We estimate the number of floating point operations used to train a,We estimate the number of floating point operations used to train a
"model by multiplying the training time, the number of GPUs used, and an estimate of the sustained","model by multiplying the training time, the number of GPUs used, and an estimate of the sustained"
single-precision floating-point capacity of each GPU 5.,single-precision floating-point capacity of each GPU 5.
"To evaluate the importance of different components of the Transformer, we varied our base model","To evaluate the importance of different components of the Transformer, we varied our base model"
"in different ways, measuring the change in performance on English-to-German translation on the","in different ways, measuring the change in performance on English-to-German translation on the"
"5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.","5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively."
Page 9:,Page 9:
Table 3: Variations on the Transformer architecture.,Table 3: Variations on the Transformer architecture.
Unlisted values are identical to those of the base,Unlisted values are identical to those of the base
model.,model.
"All metrics are on the English-to-German translation development set, newstest2013.","All metrics are on the English-to-German translation development set, newstest2013."
Listed,Listed
"perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to","perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to"
positional embedding instead of sinusoids,positional embedding instead of sinusoids
"development set, newstest2013.","development set, newstest2013."
"We used beam search as described in the previous section, but no","We used beam search as described in the previous section, but no"
checkpoint averaging.,checkpoint averaging.
We present these results in Table 3.,We present these results in Table 3.
"In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,","In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,"
"keeping the amount of computation constant, as described in Section 3.2.2.","keeping the amount of computation constant, as described in Section 3.2.2."
While single-head,While single-head
"attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.","attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads."
"In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality.","In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality."
This,This
suggests that determining compatibility is not easy and that a more sophisticated compatibility,suggests that determining compatibility is not easy and that a more sophisticated compatibility
function than dot product may be beneficial.,function than dot product may be beneficial.
"We further observe in rows (C) and (D) that, as expected,","We further observe in rows (C) and (D) that, as expected,"
"bigger models are better, and dropout is very helpful in avoiding over-fitting.","bigger models are better, and dropout is very helpful in avoiding over-fitting."
In row (E) we replace our,In row (E) we replace our
"sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical","sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical"
results to the base model.,results to the base model.
To evaluate if the Transformer can generalize to other tasks we performed experiments on English,To evaluate if the Transformer can generalize to other tasks we performed experiments on English
constituency parsing.,constituency parsing.
This task presents specific challenges: the output is subject to strong structural,This task presents specific challenges: the output is subject to strong structural
constraints and is significantly longer than the input.,constraints and is significantly longer than the input.
"Furthermore, RNN sequence-to-sequence","Furthermore, RNN sequence-to-sequence"
models have not been able to attain state-of-the-art results in small-data regimes [37].,models have not been able to attain state-of-the-art results in small-data regimes [37].
We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the,We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the
"Penn Treebank [25], about 40K training sentences.","Penn Treebank [25], about 40K training sentences."
"We also trained it in a semi-supervised setting,","We also trained it in a semi-supervised setting,"
using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences,using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences
[37].,[37].
We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens,We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens
"We performed only a small number of experiments to select the dropout, both attention and residual","We performed only a small number of experiments to select the dropout, both attention and residual"
"(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters","(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters"
remained unchanged from the English-to-German base translation model.,remained unchanged from the English-to-German base translation model.
"During inference, we","During inference, we"
Page 10:,Page 10:
Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23,Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23
Vinyals & Kaiser el al.,Vinyals & Kaiser el al.
(2014) [37],(2014) [37]
Petrov et al.,Petrov et al.
(2006) [29],(2006) [29]
Zhu et al.,Zhu et al.
(2013) [40],(2013) [40]
Dyer et al.,Dyer et al.
(2016) [8],(2016) [8]
Zhu et al.,Zhu et al.
(2013) [40],(2013) [40]
Huang & Harper (2009) [14],Huang & Harper (2009) [14]
McClosky et al.,McClosky et al.
(2006) [26],(2006) [26]
Vinyals & Kaiser el al.,Vinyals & Kaiser el al.
(2014) [37],(2014) [37]
Luong et al.,Luong et al.
(2015) [23],(2015) [23]
Dyer et al.,Dyer et al.
(2016) [8],(2016) [8]
increased the maximum output length to input length + 300.,increased the maximum output length to input length + 300.
We used a beam size of 21 and α = 0.3,We used a beam size of 21 and α = 0.3
for both WSJ only and the semi-supervised setting.,for both WSJ only and the semi-supervised setting.
Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur-,Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur-
"prisingly well, yielding better results than all previously reported models with the exception of the","prisingly well, yielding better results than all previously reported models with the exception of the"
Recurrent Neural Network Grammar [8].,Recurrent Neural Network Grammar [8].
"In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-","In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-"
Parser [29] even when training only on the WSJ training set of 40K sentences.,Parser [29] even when training only on the WSJ training set of 40K sentences.
"In this work, we presented the Transformer, the first sequence transduction model based entirely on","In this work, we presented the Transformer, the first sequence transduction model based entirely on"
"attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with","attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with"
"For translation tasks, the Transformer can be trained significantly faster than architectures based","For translation tasks, the Transformer can be trained significantly faster than architectures based"
on recurrent or convolutional layers.,on recurrent or convolutional layers.
On both WMT 2014 English-to-German and WMT 2014,On both WMT 2014 English-to-German and WMT 2014
"English-to-French translation tasks, we achieve a new state of the art.","English-to-French translation tasks, we achieve a new state of the art."
In the former task our best,In the former task our best
model outperforms even all previously reported ensembles.,model outperforms even all previously reported ensembles.
We are excited about the future of attention-based models and plan to apply them to other tasks.,We are excited about the future of attention-based models and plan to apply them to other tasks.
We,We
plan to extend the Transformer to problems involving input and output modalities other than text and,plan to extend the Transformer to problems involving input and output modalities other than text and
"to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs","to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs"
"such as images, audio and video.","such as images, audio and video."
Making generation less sequential is another research goals of ours.,Making generation less sequential is another research goals of ours.
The code we used to train and evaluate our models is available at https://github.com/,The code we used to train and evaluate our models is available at https://github.com/
We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful,We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful
